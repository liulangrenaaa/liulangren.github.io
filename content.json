{"pages":[{"title":"About Me","text":"code change the world how to contact me: email: sh_def@163.com where: PuTong ShangHai 如果不是特别注明，博客代码都是基于 当时 linux-stable 版本.","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"docker与cgroup如何联系起来的","text":"一直说Docker 是基于linux三大技术 Cgroups, Namespace, OverlayFs 技术。Cgroups, Namespace 着重的是 docker 的运行时，OverlayFs 着重的是 docker Image.但是Docker 与 Cgroups, Namespace, OverlayFs 是怎么联系起来的呢，却很少有人说的明白，我是一个好奇心很重的人，我会用几篇文章来记录分析一下他们怎么练习起来的。 这篇文章主要实操看一看 Docker 与 Cgroup 的联系。 首先去 /sys/fs/cgroup/ 目录看一下当前支持的子系统controller，各个子系统controller下面一般情况下都是空的，表明系统中没有额外的cgroup，基本都只有root_cgroup.虽然4.5版本之后 cgroup v2 就进入mainline了，但是 现在（2020.12.01）docker 基本还是在使用 cgroup v1, 应该也算是一个历史原因了吧，不知道何时 docker能迁移到 cgroup v2 上，期待ing 1234567891011121314151617181920212223242526272829303132333435tencent_clould@ubuntu: /sys/fs/cgroup# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdma unifiedcpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemdtencent_clould@ubuntu: /sys/fs/cgroup# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)freezer on /sys/fs/cgroup/freezer type cgroup (rw,relatime,freezer)tencent_clould@ubuntu: /sys/fs/cgroup# ls memorycgroup.clone_children memory.kmem.failcnt memory.limit_in_bytes memory.usage_in_bytescgroup.event_control memory.kmem.limit_in_bytes memory.max_usage_in_bytes memory.use_hierarchycgroup.procs memory.kmem.max_usage_in_bytes memory.move_charge_at_immigrate notify_on_releasecgroup.sane_behavior memory.kmem.slabinfo memory.numa_stat release_agentdocker memory.kmem.tcp.failcnt memory.oom_control system.sliceinit.scope memory.kmem.tcp.limit_in_bytes memory.pressure_level tasksmachine.slice memory.kmem.tcp.max_usage_in_bytes memory.soft_limit_in_bytes usermemory.failcnt memory.kmem.tcp.usage_in_bytes memory.stat user.slicememory.force_empty memory.kmem.usage_in_bytes memory.swappinesstencent_clould@ubuntu: /sys/fs/cgroup# cat memory/tasks234....17286921731590 基于 ubuntu image新建一个docker，限制memory 为 4MB 123tencent_clould@ubuntu: /sys/fs/cgroup# sudo docker run -t -i -m 4MB ubuntu /bin/bashWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.root@b3c618bb4df9:/# 警告可以忽略，可以看到docker的 id是 b3c618bb4df9。 可以发现各个子系统目录都会新建一个docker目录，且docker目录下有一个目录就是上面的docker container 的id。 1234567891011121314151617181920212223242526272829tencent_clould@ubuntu: /sys/fs/cgroup/memory# lscgroup.clone_children memory.kmem.slabinfo memory.soft_limit_in_bytescgroup.event_control memory.kmem.tcp.failcnt memory.statcgroup.procs memory.kmem.tcp.limit_in_bytes memory.swappinesscgroup.sane_behavior memory.kmem.tcp.max_usage_in_bytes memory.usage_in_bytesdocker memory.kmem.tcp.usage_in_bytes memory.use_hierarchyinit.scope memory.kmem.usage_in_bytes notify_on_releasemachine.slice memory.limit_in_bytes release_agentmemory.failcnt memory.max_usage_in_bytes system.slicememory.force_empty memory.move_charge_at_immigrate tasksmemory.kmem.failcnt memory.numa_stat usermemory.kmem.limit_in_bytes memory.oom_control user.slicememory.kmem.max_usage_in_bytes memory.pressure_leveltencent_clould@ubuntu: /sys/fs/cgroup/memory# ls docker1f3a24f9105950f6f463da0effa2465a518039c3bec76de71c6e20626dcfb286 memory.kmem.usage_in_bytesb3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df memory.limit_in_bytescgroup.clone_children memory.max_usage_in_bytescgroup.event_control memory.move_charge_at_immigratecgroup.procs memory.numa_statmemory.failcnt memory.oom_controlmemory.force_empty memory.pressure_levelmemory.kmem.failcnt memory.soft_limit_in_bytesmemory.kmem.limit_in_bytes memory.statmemory.kmem.max_usage_in_bytes memory.swappinessmemory.kmem.slabinfo memory.usage_in_bytesmemory.kmem.tcp.failcnt memory.use_hierarchymemory.kmem.tcp.limit_in_bytes notify_on_releasememory.kmem.tcp.max_usage_in_bytes tasksmemory.kmem.tcp.usage_in_bytes 其中 docker 目录下除了 container id的目录其他目录都是无效的，docker这个目录只是限制所有container id的一个顶层目录而已。通过 docker/tasks 为空就可以看出来。可以看到 memory/docker/b3c618bb4df9/ 目录下的最大内存限制，就是创建这个容器时设置的最大内存使用示4MB的限制。 12345678910111213tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/memory/docker#tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat memory.limit_in_bytes4194304tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 1733194 可以看到这个 1733194 进程就是启动 容器时的 /bin/bash 进程 由于其他的 cgroup子系统我们没有对刚刚启动的 container 进行限制，所以也理论上从 cgroup上看到的也是没有限制的，可以看看 cpu controller. 123456789tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cd -/sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602dftencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_period_us100000tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_quota_us-1 这里就不具体介绍 各个子系统目录下文件含义了。","link":"/2020/12/12/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E4%B8%8Ecgroup%E5%A6%82%E4%BD%95%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"docker与cgroup如何联系起来的","text":"这篇文章主要实操看一看 Docker 与 Namespace 的联系。 首先去 /sys/fs/cgroup/ 目录看一下当前支持的子系统controller，各个子系统controller下面一般情况下都是空的，表明系统中没有额外的cgroup，基本都只有root_cgroup.虽然4.5版本之后 cgroup v2 就进入mainline了，但是 现在（2020.12.01）docker 基本还是在使用 cgroup v1, 应该也算是一个历史原因了吧，不知道何时 docker能迁移到 cgroup v2 上，期待ing 1234567891011121314151617181920212223242526272829303132333435tencent_clould@ubuntu: /sys/fs/cgroup# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdma unifiedcpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemdtencent_clould@ubuntu: /sys/fs/cgroup# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)freezer on /sys/fs/cgroup/freezer type cgroup (rw,relatime,freezer)tencent_clould@ubuntu: /sys/fs/cgroup# ls memorycgroup.clone_children memory.kmem.failcnt memory.limit_in_bytes memory.usage_in_bytescgroup.event_control memory.kmem.limit_in_bytes memory.max_usage_in_bytes memory.use_hierarchycgroup.procs memory.kmem.max_usage_in_bytes memory.move_charge_at_immigrate notify_on_releasecgroup.sane_behavior memory.kmem.slabinfo memory.numa_stat release_agentdocker memory.kmem.tcp.failcnt memory.oom_control system.sliceinit.scope memory.kmem.tcp.limit_in_bytes memory.pressure_level tasksmachine.slice memory.kmem.tcp.max_usage_in_bytes memory.soft_limit_in_bytes usermemory.failcnt memory.kmem.tcp.usage_in_bytes memory.stat user.slicememory.force_empty memory.kmem.usage_in_bytes memory.swappinesstencent_clould@ubuntu: /sys/fs/cgroup# cat memory/tasks234....17286921731590 基于 ubuntu image新建一个docker，限制memory 为 4MB 123tencent_clould@ubuntu: /sys/fs/cgroup# sudo docker run -t -i -m 4MB ubuntu /bin/bashWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.root@b3c618bb4df9:/# 警告可以忽略，可以看到docker的 id是 b3c618bb4df9。 可以发现各个子系统目录都会新建一个docker目录，且docker目录下有一个目录就是上面的docker container 的id。 1234567891011121314151617181920212223242526272829tencent_clould@ubuntu: /sys/fs/cgroup/memory# lscgroup.clone_children memory.kmem.slabinfo memory.soft_limit_in_bytescgroup.event_control memory.kmem.tcp.failcnt memory.statcgroup.procs memory.kmem.tcp.limit_in_bytes memory.swappinesscgroup.sane_behavior memory.kmem.tcp.max_usage_in_bytes memory.usage_in_bytesdocker memory.kmem.tcp.usage_in_bytes memory.use_hierarchyinit.scope memory.kmem.usage_in_bytes notify_on_releasemachine.slice memory.limit_in_bytes release_agentmemory.failcnt memory.max_usage_in_bytes system.slicememory.force_empty memory.move_charge_at_immigrate tasksmemory.kmem.failcnt memory.numa_stat usermemory.kmem.limit_in_bytes memory.oom_control user.slicememory.kmem.max_usage_in_bytes memory.pressure_leveltencent_clould@ubuntu: /sys/fs/cgroup/memory# ls docker1f3a24f9105950f6f463da0effa2465a518039c3bec76de71c6e20626dcfb286 memory.kmem.usage_in_bytesb3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df memory.limit_in_bytescgroup.clone_children memory.max_usage_in_bytescgroup.event_control memory.move_charge_at_immigratecgroup.procs memory.numa_statmemory.failcnt memory.oom_controlmemory.force_empty memory.pressure_levelmemory.kmem.failcnt memory.soft_limit_in_bytesmemory.kmem.limit_in_bytes memory.statmemory.kmem.max_usage_in_bytes memory.swappinessmemory.kmem.slabinfo memory.usage_in_bytesmemory.kmem.tcp.failcnt memory.use_hierarchymemory.kmem.tcp.limit_in_bytes notify_on_releasememory.kmem.tcp.max_usage_in_bytes tasksmemory.kmem.tcp.usage_in_bytes 其中 docker 目录下除了 container id的目录其他目录都是无效的，docker这个目录只是限制所有container id的一个顶层目录而已。通过 docker/tasks 为空就可以看出来。可以看到 memory/docker/b3c618bb4df9/ 目录下的最大内存限制，就是创建这个容器时设置的最大内存使用示4MB的限制。 12345678910111213tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/memory/docker#tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat memory.limit_in_bytes4194304tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 1733194 可以看到这个 1733194 进程就是启动 容器时的 /bin/bash 进程 由于其他的 cgroup子系统我们没有对刚刚启动的 container 进行限制，所以也理论上从 cgroup上看到的也是没有限制的，可以看看 cpu controller. 123456789tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cd -/sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602dftencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_period_us100000tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_quota_us-1 这里就不具体介绍 各个子系统目录下文件含义了。","link":"/2020/12/12/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E4%B8%8Enamespace%E5%A6%82%E4%BD%95%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"docker如何使用","text":"这是一个简单概括docker 安装，拉取image,docker 命令使用的 记录。 在ubuntu平台上安装 docker 1sudo apt install docker 拉取ubuntu的 image 1sudo docker pull ubuntu 基于ubuntu的 image 创建并运行一个docker，限制4MB内存，开启 bash 1sudo docker run -t -i -m 4MB ubuntu /bin/bash 查看当前有哪些运行的docker 123tencent_clould@ubuntu: ~/workspace# sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f3a24f91059 ubuntu &quot;/bin/bash&quot; 47 hours ago Up 47 hours hhhh 查看当前有哪些的docker(运行中 + 非运行中) 123tencent_clould@ubuntu: ~/workspace# sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f3a24f91059 ubuntu &quot;/bin/bash&quot; 47 hours ago Up 47 hours hhhhh 给 hhh 的容器重命名为 my_ubuntu 1sudo docker rename hhh my_ubuntu 停止 my_ubuntu 容器 1sudo docker stop my_ubuntu 重启开启 my_ubuntu 容器 1sudo docker start my_ubuntu 进入运行中的 my_ubuntu 容器的 bash 1sudo docker exec -ti my_ubuntu /bin/bash 其他常用命令想起来再记录 这些命令还是比较长的，可以缩写到 bashrc 的alias里面，比较方便 12345678tencent_clould@ubuntu: ~/workspace# cat ~/.zshrc | grep &quot;alias d&quot;alias dn=&quot;sudo docker run -t -i -m 128MB ubuntu /bin/bash&quot;alias ds=&quot;sudo docker start my_ubuntu&quot;alias dk=&quot;sudo docker stop my_ubuntu&quot;alias di=&quot;sudo docker exec -ti my_ubuntu /bin/bash&quot;alias dps=&quot;sudo docker ps&quot;alias dpsa=&quot;sudo docker ps -a&quot;alias drma=&quot;sudo docker container prune&quot;","link":"/2020/12/01/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/"},{"title":"x86 平台常用寄存器和函数调用分析","text":"后续填坑。。","link":"/2021/01/23/crash%E4%B8%93%E9%A2%98/x86%20%E5%B9%B3%E5%8F%B0%E5%B8%B8%E7%94%A8%E5%AF%84%E5%AD%98%E5%99%A8%E5%92%8C%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90/"},{"title":"最简单的空指针oops","text":"只是做一个记录，为了演示最简单的空指针case, 写了一个demo, 可以参考github 代码 用 crash 分析insmod 出错之后已经生成了相关 dump文件。下面直接使用 crash 工具分析： 12345678910111213141516171819202122232425262728293031stable_kernel@kernel: /var/crash/202101211201# sudo crash vmlinux dumpcrash 7.2.9++GNU gdb (GDB) 7.6Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [704MB]: patching 137170 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101211201 [PARTIAL DUMP] CPUS: 4 DATE: Thu Jan 21 12:00:56 CST 2021 UPTIME: 00:12:53LOAD AVERAGE: 0.16, 0.11, 0.10 TASKS: 454 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #5 SMP Wed Jan 20 20:41:47 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Oops: 0002 [#1] SMP NOPTI&quot; (check log for details) PID: 3605 COMMAND: &quot;krace_thread&quot; TASK: ffffa1b006754b40 [THREAD_INFO: ffffa1b006754b40] CPU: 2 STATE: TASK_RUNNING (PANIC)crash&gt; 可以看到 发生问题的 kernel 版本是 5.11.0-rc4+，编译时间是 #5 SMP Wed Jan 20 20:41:47 CST 2021，内存大小是 2G，出问题时刻的负载是0.16, 0.11, 0.10 PANIC 原因是&quot;Oops: 0002 [#1] SMP NOPTI&quot; (check log for details)，CPU:2 上的TASK（krace_thread-3605）: ffffa1b006754b40发生了 oops，具体原因需要看 日志来得到。 bt 查看出问题的taskcrash 运行之后默认的task是出问题的task，可以通过 set 查看 123456crash&gt; set PID: 3605COMMAND: &quot;krace_thread&quot; TASK: ffffa1b006754b40 [THREAD_INFO: ffffa1b006754b40] CPU: 2 STATE: TASK_RUNNING (PANIC) bt 可以查看当前追踪的task的 backtrace 12345678910111213crash&gt; btPID: 3605 TASK: ffffa1b006754b40 CPU: 2 COMMAND: &quot;krace_thread&quot; #0 [ffffbbbf004ebc40] machine_kexec at ffffffffad04d87c #1 [ffffbbbf004ebc88] __crash_kexec at ffffffffad1283b8 #2 [ffffbbbf004ebd50] crash_kexec at ffffffffad1290d0 #3 [ffffbbbf004ebd60] oops_end at ffffffffad021d75 #4 [ffffbbbf004ebd80] no_context at ffffffffad0570e0 #5 [ffffbbbf004ebdf0] __bad_area_nosemaphore at ffffffffad0572c7 #6 [ffffbbbf004ebe38] exc_page_fault at ffffffffadd16b67 #7 [ffffbbbf004ebe60] asm_exc_page_fault at ffffffffade00ace #8 [ffffbbbf004ebee8] create_oops at ffffffffc0371027 [01_null_pointer] #9 [ffffbbbf004ebf10] kthread at ffffffffad0930da#10 [ffffbbbf004ebf50] ret_from_fork at ffffffffad001ae2 bt -c 1： 可以查看 cpu:1 上当前运行的线程的backtracebt -a ： 可以查看 当前所有 cpu上运行的线程的backtrace 这个case 十分显然，是 create_oops 这里出现了问题。 dis 查看bug地址dis 是 disassemble 反汇编的缩写，可以 查看出问题 text 地址内容 某个函数 symbol 符号内容 某个函数 symbol 符号 + 偏移的内容 某个符号 或者 text 与 代码行显示在一起（如果是module 中crash需要加载 module.ko） 123456789101112131415161718crash&gt; dis ffffffffc03710270xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0crash&gt;crash&gt; dis create_oops0xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;0xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt;0xffffffffc0371016 &lt;create_oops+22&gt;: mov $0x1388,%edi0xffffffffc037101b &lt;create_oops+27&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;0xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x00xffffffffc0371032 &lt;create_oops+50&gt;: callq 0xffffffffadcc96da &lt;printk&gt;0xffffffffc0371037 &lt;create_oops+55&gt;: xor %eax,%eax0xffffffffc0371039 &lt;create_oops+57&gt;: retqcrash&gt; dis create_oops+390xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0crash&gt; 直接可以看出问题是，将立即数$0x0 赋值到 地址0x0中，所以直接 oops了 10xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0 但是在哪一行呢，这就需要加载 ko文件了 12345678910111213crash&gt; lsmod MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 (not loaded) [CONFIG_KALLSYMS]crash&gt;crash&gt;crash&gt; mod -s 01_null_pointer /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.ko MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.kocrash&gt;crash&gt; lsmod MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.kocrash&gt; 加载 ko文件之后，直接 dis -l 反汇编 出问题的函数 123456789101112131415161718crash&gt; dis -l create_oops/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 120xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 130xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 140xffffffffc0371016 &lt;create_oops+22&gt;: mov $0x1388,%edi0xffffffffc037101b &lt;create_oops+27&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 160xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 170xffffffffc0371032 &lt;create_oops+50&gt;: callq 0xffffffffadcc96da &lt;printk&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 180xffffffffc0371037 &lt;create_oops+55&gt;: xor %eax,%eax0xffffffffc0371039 &lt;create_oops+57&gt;: retq 直接定位到 12301_null_pointer.c: 160xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0 代码中看看 1*(int *)0 = 0; 问题很快解决了。 试着查看x86 如何调用函数传参的123/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 120xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt; 对应代码是，0x1388 就是十六进制的 5000 1msleep(5000); 是不是第一个整形参数是存在 edi 寄存器中的 123/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 130xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt; 对应的代码是 1printk(&quot;create_oops start\\n&quot;); 0xffffffffc037203c 是啥呢？可以使用 rd 命令读取一下，原来是字符串的起始的地址 1234crash&gt; rd 0xffffffffc037203c 4ffffffffc037203c: 6f5f657461657263 726174732073706f create_oops starffffffffc037204c: 7461657263000a74 652073706f6f5f65 t..create_oops ecrash&gt; 是不是 第一个地址型参数是存放在 rdi 中的呢？ 后面会用不同个数参数，不同类型参数的函数 crash，来实验一下这个是不是对～ 找到一篇讲解x86-64寄存器和函数调用的文章，上面说的猜想就是扯淡。。","link":"/2021/01/21/crash%E4%B8%93%E9%A2%98/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A9%BA%E6%8C%87%E9%92%88oops/"},{"title":"page cache如何产生的","text":"WHY Page cache?CPU如果要访问外部磁盘上的文件，需要首先将这些文件的内容拷贝到内存中，由于硬件的限制，从磁盘到内存的数据传输速度是很慢的，如果现在物理内存有空余，干嘛不用这些空闲内存来缓存一些磁盘的文件内容呢，这部分用作缓存磁盘文件的内存就叫做page cache。 用户进程启动read()系统调用后，内核会首先查看page cache里有没有用户要读取的文件内容，如果有（cache hit），那就直接读取，没有的话（cache miss）再启动I/O操作从磁盘上读取，然后放到page cache中，下次再访问这部分内容的时候，就又可以cache hit，不用忍受磁盘的龟速了（相比内存慢几个数量级）。 由此可见 page_cache 会对 磁盘性能，应用性能有极大提高 但是相对于磁盘，内存的容量还是很有限的，所以没必要缓存整个文件，只需要当文件的某部分内容真正被访问到时，再将这部分内容调入内存缓存起来就可以了，这种方式叫做demand paging（按需调页），把对需求的满足延迟到最后一刻，很懒很实用。 Page cache 组成filePage cache 是文件部分或者全部在内核中缓存的部分，首先需要了解 文件在磁盘和linux中的表现: 在磁盘等存储介质上，文件都是分块存储在磁盘上的， 磁盘inode 是文件唯一标识。 linux系统中为了表示文件，也有文件系统inode，一般会跟文件系统相关，是 从物理磁盘 inode 读取到内存之后的形态 linux系统中虚拟文件系统VFS以实现多文件系统支持，vfs inode是VFS层文件内存数据结构，大多数是所有 文件系统 inode 公共成员 ext4 fs for example: 12345678910111213/* * fourth extended file system inode data in memory */struct ext4_inode_info { __le32 i_data[15]; /* unconverted */ __u32 i_dtime; ext4_fsblk_t i_file_acl; struct rw_semaphore xattr_sem; struct list_head i_orphan; /* unlinked but open inodes */ struct rw_semaphore i_data_sem; struct inode vfs_inode; struct jbd2_inode *jinode; ext4_inode_info 就是对应文件系统inode, vfs_inode 就是 vfs层的inode. 123456789101112131415/* * Structure of an inode on the disk */struct ext4_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size_lo; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Inode Change time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks_lo; /* Blocks count */ ..... ext4_inode 就是对应 物理磁盘的inode. 大多数成员是 记录和物理磁盘 和 物理文件真实 相关的信息 address_space实际情况中，一个文件可能有 100M - 10G这么大，kernel会给文件在内存中分配很多page cache,这些pagecache是如何管理起来的呢，这就引出了 第二个主要结构 address_space – 地址空间。 首先看 address_space 定义 12345678910111213141516171819202122232425262728293031323334353637383940/** * struct address_space - Contents of a cacheable, mappable object. * @host: Owner, either the inode or the block_device. * @i_pages: Cached pages. * @gfp_mask: Memory allocation flags to use for allocating pages. * @i_mmap_writable: Number of VM_SHARED mappings. * @nr_thps: Number of THPs in the pagecache (non-shmem only). * @i_mmap: Tree of private and shared mappings. * @i_mmap_rwsem: Protects @i_mmap and @i_mmap_writable. * @nrpages: Number of page entries, protected by the i_pages lock. * @nrexceptional: Shadow or DAX entries, protected by the i_pages lock. * @writeback_index: Writeback starts here. * @a_ops: Methods. * @flags: Error bits and flags (AS_*). * @wb_err: The most recent error which has occurred. * @private_lock: For use by the owner of the address_space. * @private_list: For use by the owner of the address_space. * @private_data: For use by the owner of the address_space. */struct address_space { struct inode *host; //一般就是 inode 与 文件关联 struct xarray i_pages; // xarray 管理着这个地址空间里面所有的 page,之前kernel版本是 radix tree gfp_t gfp_mask; atomic_t i_mmap_writable;#ifdef CONFIG_READ_ONLY_THP_FOR_FS /* number of thp, only for non-shmem files */ atomic_t nr_thps;#endif struct rb_root_cached i_mmap; // i_mmap 红黑树的根节点，会将 page 按照 某种？ 序列组织起来，便于查找 struct rw_semaphore i_mmap_rwsem; unsigned long nrpages; unsigned long nrexceptional; pgoff_t writeback_index; const struct address_space_operations *a_ops; unsigned long flags; errseq_t wb_err; spinlock_t private_lock; struct list_head private_list; void *private_data;} __attribute__((aligned(sizeof(long)))) __randomize_layout; 这个定义没有 inode 那么长，但是很核心 这样 inode 与 many pages 通过 address_space 的 host 与 i_pages 成员 相互连接起来。 其中 inode 既可以是 磁盘文件的 inode，也可以是 内存文件系统 的 inode（proc sys等）还可以是 swap 文件的 inode. a_ops 同样也是一个基类指针，定义了抽象的文件系统交互接口，由具体文件系统负责实现。例如如果文件是存储在ext4文件系统之上，那么该结构便被初始化为 ext4_aops （见fs/ext4/inode.c）。 如何查找 一个文件的page_cache?inode –&gt; address_space: container_of 通过inode 找到 地址空间address_space –&gt; i_pages: 成员变量访问 address_space 是Linux内核中的一个关键抽象，它是页缓存和外部设备中文件系统的桥梁。 上层应用读取数据会进入到该结构内的page cache，上层应用对文件的写入内容也会缓存于该结构内的page cache。 $$ 这里配图 dentry这和 pagecache 关系没有那密切。 假如需要查找 /etc/apt/aaa 这个文件，linux系统会如何去查找呢？文件目录这些信息就涉及到 dentry 的信息了，dentry也是实现Linux文件系统目录层次结构的关键. dentry 从另外一个层面描述文件：文件名. 更准确地说，是保存文件名和文件inode号 与 inode 一样， dentry 除了VFS层dentry结构，每种具体文件系统也有自身的内存dentry和 磁盘dentry结构 ext4 fs for example: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647struct dentry { /* RCU lookup touched fields */ unsigned int d_flags; /* protected by d_lock */ seqcount_spinlock_t d_seq; /* per dentry seqlock */ struct hlist_bl_node d_hash; /* lookup hash list */ struct dentry *d_parent; /* parent directory */ struct qstr d_name; struct inode *d_i