{"pages":[{"title":"About Me","text":"code change the world how to contact me: email: sh_def@163.com where: PuTong ShangHai 如果不是特别注明，博客代码都是基于 当时 linux-stable 版本.","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"docker与cgroup如何联系起来的","text":"一直说Docker 是基于linux三大技术 Cgroups, Namespace, OverlayFs 技术。Cgroups, Namespace 着重的是 docker 的运行时，OverlayFs 着重的是 docker Image.但是Docker 与 Cgroups, Namespace, OverlayFs 是怎么联系起来的呢，却很少有人说的明白，我是一个好奇心很重的人，我会用几篇文章来记录分析一下他们怎么练习起来的。 这篇文章主要实操看一看 Docker 与 Cgroup 的联系。 首先去 /sys/fs/cgroup/ 目录看一下当前支持的子系统controller，各个子系统controller下面一般情况下都是空的，表明系统中没有额外的cgroup，基本都只有root_cgroup.虽然4.5版本之后 cgroup v2 就进入mainline了，但是 现在（2020.12.01）docker 基本还是在使用 cgroup v1, 应该也算是一个历史原因了吧，不知道何时 docker能迁移到 cgroup v2 上，期待ing 1234567891011121314151617181920212223242526272829303132333435tencent_clould@ubuntu: /sys/fs/cgroup# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdma unifiedcpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemdtencent_clould@ubuntu: /sys/fs/cgroup# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)freezer on /sys/fs/cgroup/freezer type cgroup (rw,relatime,freezer)tencent_clould@ubuntu: /sys/fs/cgroup# ls memorycgroup.clone_children memory.kmem.failcnt memory.limit_in_bytes memory.usage_in_bytescgroup.event_control memory.kmem.limit_in_bytes memory.max_usage_in_bytes memory.use_hierarchycgroup.procs memory.kmem.max_usage_in_bytes memory.move_charge_at_immigrate notify_on_releasecgroup.sane_behavior memory.kmem.slabinfo memory.numa_stat release_agentdocker memory.kmem.tcp.failcnt memory.oom_control system.sliceinit.scope memory.kmem.tcp.limit_in_bytes memory.pressure_level tasksmachine.slice memory.kmem.tcp.max_usage_in_bytes memory.soft_limit_in_bytes usermemory.failcnt memory.kmem.tcp.usage_in_bytes memory.stat user.slicememory.force_empty memory.kmem.usage_in_bytes memory.swappinesstencent_clould@ubuntu: /sys/fs/cgroup# cat memory/tasks234....17286921731590 基于 ubuntu image新建一个docker，限制memory 为 4MB 123tencent_clould@ubuntu: /sys/fs/cgroup# sudo docker run -t -i -m 4MB ubuntu /bin/bashWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.root@b3c618bb4df9:/# 警告可以忽略，可以看到docker的 id是 b3c618bb4df9。 可以发现各个子系统目录都会新建一个docker目录，且docker目录下有一个目录就是上面的docker container 的id。 1234567891011121314151617181920212223242526272829tencent_clould@ubuntu: /sys/fs/cgroup/memory# lscgroup.clone_children memory.kmem.slabinfo memory.soft_limit_in_bytescgroup.event_control memory.kmem.tcp.failcnt memory.statcgroup.procs memory.kmem.tcp.limit_in_bytes memory.swappinesscgroup.sane_behavior memory.kmem.tcp.max_usage_in_bytes memory.usage_in_bytesdocker memory.kmem.tcp.usage_in_bytes memory.use_hierarchyinit.scope memory.kmem.usage_in_bytes notify_on_releasemachine.slice memory.limit_in_bytes release_agentmemory.failcnt memory.max_usage_in_bytes system.slicememory.force_empty memory.move_charge_at_immigrate tasksmemory.kmem.failcnt memory.numa_stat usermemory.kmem.limit_in_bytes memory.oom_control user.slicememory.kmem.max_usage_in_bytes memory.pressure_leveltencent_clould@ubuntu: /sys/fs/cgroup/memory# ls docker1f3a24f9105950f6f463da0effa2465a518039c3bec76de71c6e20626dcfb286 memory.kmem.usage_in_bytesb3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df memory.limit_in_bytescgroup.clone_children memory.max_usage_in_bytescgroup.event_control memory.move_charge_at_immigratecgroup.procs memory.numa_statmemory.failcnt memory.oom_controlmemory.force_empty memory.pressure_levelmemory.kmem.failcnt memory.soft_limit_in_bytesmemory.kmem.limit_in_bytes memory.statmemory.kmem.max_usage_in_bytes memory.swappinessmemory.kmem.slabinfo memory.usage_in_bytesmemory.kmem.tcp.failcnt memory.use_hierarchymemory.kmem.tcp.limit_in_bytes notify_on_releasememory.kmem.tcp.max_usage_in_bytes tasksmemory.kmem.tcp.usage_in_bytes 其中 docker 目录下除了 container id的目录其他目录都是无效的，docker这个目录只是限制所有container id的一个顶层目录而已。通过 docker/tasks 为空就可以看出来。可以看到 memory/docker/b3c618bb4df9/ 目录下的最大内存限制，就是创建这个容器时设置的最大内存使用示4MB的限制。 12345678910111213tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/memory/docker#tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat memory.limit_in_bytes4194304tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 1733194 可以看到这个 1733194 进程就是启动 容器时的 /bin/bash 进程 由于其他的 cgroup子系统我们没有对刚刚启动的 container 进行限制，所以也理论上从 cgroup上看到的也是没有限制的，可以看看 cpu controller. 123456789tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cd -/sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602dftencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_period_us100000tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_quota_us-1 这里就不具体介绍 各个子系统目录下文件含义了。","link":"/2020/12/12/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E4%B8%8Ecgroup%E5%A6%82%E4%BD%95%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"docker与cgroup如何联系起来的","text":"这篇文章主要实操看一看 Docker 与 Namespace 的联系。 首先去 /sys/fs/cgroup/ 目录看一下当前支持的子系统controller，各个子系统controller下面一般情况下都是空的，表明系统中没有额外的cgroup，基本都只有root_cgroup.虽然4.5版本之后 cgroup v2 就进入mainline了，但是 现在（2020.12.01）docker 基本还是在使用 cgroup v1, 应该也算是一个历史原因了吧，不知道何时 docker能迁移到 cgroup v2 上，期待ing 1234567891011121314151617181920212223242526272829303132333435tencent_clould@ubuntu: /sys/fs/cgroup# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdma unifiedcpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemdtencent_clould@ubuntu: /sys/fs/cgroup# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)freezer on /sys/fs/cgroup/freezer type cgroup (rw,relatime,freezer)tencent_clould@ubuntu: /sys/fs/cgroup# ls memorycgroup.clone_children memory.kmem.failcnt memory.limit_in_bytes memory.usage_in_bytescgroup.event_control memory.kmem.limit_in_bytes memory.max_usage_in_bytes memory.use_hierarchycgroup.procs memory.kmem.max_usage_in_bytes memory.move_charge_at_immigrate notify_on_releasecgroup.sane_behavior memory.kmem.slabinfo memory.numa_stat release_agentdocker memory.kmem.tcp.failcnt memory.oom_control system.sliceinit.scope memory.kmem.tcp.limit_in_bytes memory.pressure_level tasksmachine.slice memory.kmem.tcp.max_usage_in_bytes memory.soft_limit_in_bytes usermemory.failcnt memory.kmem.tcp.usage_in_bytes memory.stat user.slicememory.force_empty memory.kmem.usage_in_bytes memory.swappinesstencent_clould@ubuntu: /sys/fs/cgroup# cat memory/tasks234....17286921731590 基于 ubuntu image新建一个docker，限制memory 为 4MB 123tencent_clould@ubuntu: /sys/fs/cgroup# sudo docker run -t -i -m 4MB ubuntu /bin/bashWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.root@b3c618bb4df9:/# 警告可以忽略，可以看到docker的 id是 b3c618bb4df9。 可以发现各个子系统目录都会新建一个docker目录，且docker目录下有一个目录就是上面的docker container 的id。 1234567891011121314151617181920212223242526272829tencent_clould@ubuntu: /sys/fs/cgroup/memory# lscgroup.clone_children memory.kmem.slabinfo memory.soft_limit_in_bytescgroup.event_control memory.kmem.tcp.failcnt memory.statcgroup.procs memory.kmem.tcp.limit_in_bytes memory.swappinesscgroup.sane_behavior memory.kmem.tcp.max_usage_in_bytes memory.usage_in_bytesdocker memory.kmem.tcp.usage_in_bytes memory.use_hierarchyinit.scope memory.kmem.usage_in_bytes notify_on_releasemachine.slice memory.limit_in_bytes release_agentmemory.failcnt memory.max_usage_in_bytes system.slicememory.force_empty memory.move_charge_at_immigrate tasksmemory.kmem.failcnt memory.numa_stat usermemory.kmem.limit_in_bytes memory.oom_control user.slicememory.kmem.max_usage_in_bytes memory.pressure_leveltencent_clould@ubuntu: /sys/fs/cgroup/memory# ls docker1f3a24f9105950f6f463da0effa2465a518039c3bec76de71c6e20626dcfb286 memory.kmem.usage_in_bytesb3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df memory.limit_in_bytescgroup.clone_children memory.max_usage_in_bytescgroup.event_control memory.move_charge_at_immigratecgroup.procs memory.numa_statmemory.failcnt memory.oom_controlmemory.force_empty memory.pressure_levelmemory.kmem.failcnt memory.soft_limit_in_bytesmemory.kmem.limit_in_bytes memory.statmemory.kmem.max_usage_in_bytes memory.swappinessmemory.kmem.slabinfo memory.usage_in_bytesmemory.kmem.tcp.failcnt memory.use_hierarchymemory.kmem.tcp.limit_in_bytes notify_on_releasememory.kmem.tcp.max_usage_in_bytes tasksmemory.kmem.tcp.usage_in_bytes 其中 docker 目录下除了 container id的目录其他目录都是无效的，docker这个目录只是限制所有container id的一个顶层目录而已。通过 docker/tasks 为空就可以看出来。可以看到 memory/docker/b3c618bb4df9/ 目录下的最大内存限制，就是创建这个容器时设置的最大内存使用示4MB的限制。 12345678910111213tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/memory/docker#tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat memory.limit_in_bytes4194304tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 1733194 可以看到这个 1733194 进程就是启动 容器时的 /bin/bash 进程 由于其他的 cgroup子系统我们没有对刚刚启动的 container 进行限制，所以也理论上从 cgroup上看到的也是没有限制的，可以看看 cpu controller. 123456789tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cd -/sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602dftencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_period_us100000tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_quota_us-1 这里就不具体介绍 各个子系统目录下文件含义了。","link":"/2020/12/12/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E4%B8%8Enamespace%E5%A6%82%E4%BD%95%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"docker如何使用","text":"这是一个简单概括docker 安装，拉取image,docker 命令使用的 记录。 在ubuntu平台上安装 docker 1sudo apt install docker 拉取ubuntu的 image 1sudo docker pull ubuntu 基于ubuntu的 image 创建并运行一个docker，限制4MB内存，开启 bash 1sudo docker run -t -i -m 4MB ubuntu /bin/bash 查看当前有哪些运行的docker 123tencent_clould@ubuntu: ~/workspace# sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f3a24f91059 ubuntu &quot;/bin/bash&quot; 47 hours ago Up 47 hours hhhh 查看当前有哪些的docker(运行中 + 非运行中) 123tencent_clould@ubuntu: ~/workspace# sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f3a24f91059 ubuntu &quot;/bin/bash&quot; 47 hours ago Up 47 hours hhhhh 给 hhh 的容器重命名为 my_ubuntu 1sudo docker rename hhh my_ubuntu 停止 my_ubuntu 容器 1sudo docker stop my_ubuntu 重启开启 my_ubuntu 容器 1sudo docker start my_ubuntu 进入运行中的 my_ubuntu 容器的 bash 1sudo docker exec -ti my_ubuntu /bin/bash 其他常用命令想起来再记录 这些命令还是比较长的，可以缩写到 bashrc 的alias里面，比较方便 12345678tencent_clould@ubuntu: ~/workspace# cat ~/.zshrc | grep &quot;alias d&quot;alias dn=&quot;sudo docker run -t -i -m 128MB ubuntu /bin/bash&quot;alias ds=&quot;sudo docker start my_ubuntu&quot;alias dk=&quot;sudo docker stop my_ubuntu&quot;alias di=&quot;sudo docker exec -ti my_ubuntu /bin/bash&quot;alias dps=&quot;sudo docker ps&quot;alias dpsa=&quot;sudo docker ps -a&quot;alias drma=&quot;sudo docker container prune&quot;","link":"/2020/12/01/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/"},{"title":"x86 平台常用寄存器和函数调用分析","text":"后续填坑。。","link":"/2021/01/23/crash%E4%B8%93%E9%A2%98/x86%20%E5%B9%B3%E5%8F%B0%E5%B8%B8%E7%94%A8%E5%AF%84%E5%AD%98%E5%99%A8%E5%92%8C%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90/"},{"title":"最简单的空指针oops","text":"只是做一个记录，为了演示最简单的空指针case, 写了一个demo, 可以参考github 代码 用 crash 分析insmod 出错之后已经生成了相关 dump文件。下面直接使用 crash 工具分析： 12345678910111213141516171819202122232425262728293031stable_kernel@kernel: /var/crash/202101211201# sudo crash vmlinux dumpcrash 7.2.9++GNU gdb (GDB) 7.6Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [704MB]: patching 137170 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101211201 [PARTIAL DUMP] CPUS: 4 DATE: Thu Jan 21 12:00:56 CST 2021 UPTIME: 00:12:53LOAD AVERAGE: 0.16, 0.11, 0.10 TASKS: 454 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #5 SMP Wed Jan 20 20:41:47 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Oops: 0002 [#1] SMP NOPTI&quot; (check log for details) PID: 3605 COMMAND: &quot;krace_thread&quot; TASK: ffffa1b006754b40 [THREAD_INFO: ffffa1b006754b40] CPU: 2 STATE: TASK_RUNNING (PANIC)crash&gt; 可以看到 发生问题的 kernel 版本是 5.11.0-rc4+，编译时间是 #5 SMP Wed Jan 20 20:41:47 CST 2021，内存大小是 2G，出问题时刻的负载是0.16, 0.11, 0.10 PANIC 原因是&quot;Oops: 0002 [#1] SMP NOPTI&quot; (check log for details)，CPU:2 上的TASK（krace_thread-3605）: ffffa1b006754b40发生了 oops，具体原因需要看 日志来得到。 bt 查看出问题的taskcrash 运行之后默认的task是出问题的task，可以通过 set 查看 123456crash&gt; set PID: 3605COMMAND: &quot;krace_thread&quot; TASK: ffffa1b006754b40 [THREAD_INFO: ffffa1b006754b40] CPU: 2 STATE: TASK_RUNNING (PANIC) bt 可以查看当前追踪的task的 backtrace 12345678910111213crash&gt; btPID: 3605 TASK: ffffa1b006754b40 CPU: 2 COMMAND: &quot;krace_thread&quot; #0 [ffffbbbf004ebc40] machine_kexec at ffffffffad04d87c #1 [ffffbbbf004ebc88] __crash_kexec at ffffffffad1283b8 #2 [ffffbbbf004ebd50] crash_kexec at ffffffffad1290d0 #3 [ffffbbbf004ebd60] oops_end at ffffffffad021d75 #4 [ffffbbbf004ebd80] no_context at ffffffffad0570e0 #5 [ffffbbbf004ebdf0] __bad_area_nosemaphore at ffffffffad0572c7 #6 [ffffbbbf004ebe38] exc_page_fault at ffffffffadd16b67 #7 [ffffbbbf004ebe60] asm_exc_page_fault at ffffffffade00ace #8 [ffffbbbf004ebee8] create_oops at ffffffffc0371027 [01_null_pointer] #9 [ffffbbbf004ebf10] kthread at ffffffffad0930da#10 [ffffbbbf004ebf50] ret_from_fork at ffffffffad001ae2 bt -c 1： 可以查看 cpu:1 上当前运行的线程的backtracebt -a ： 可以查看 当前所有 cpu上运行的线程的backtrace 这个case 十分显然，是 create_oops 这里出现了问题。 dis 查看bug地址dis 是 disassemble 反汇编的缩写，可以 查看出问题 text 地址内容 某个函数 symbol 符号内容 某个函数 symbol 符号 + 偏移的内容 某个符号 或者 text 与 代码行显示在一起（如果是module 中crash需要加载 module.ko） 123456789101112131415161718crash&gt; dis ffffffffc03710270xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0crash&gt;crash&gt; dis create_oops0xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;0xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt;0xffffffffc0371016 &lt;create_oops+22&gt;: mov $0x1388,%edi0xffffffffc037101b &lt;create_oops+27&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;0xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x00xffffffffc0371032 &lt;create_oops+50&gt;: callq 0xffffffffadcc96da &lt;printk&gt;0xffffffffc0371037 &lt;create_oops+55&gt;: xor %eax,%eax0xffffffffc0371039 &lt;create_oops+57&gt;: retqcrash&gt; dis create_oops+390xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0crash&gt; 直接可以看出问题是，将立即数$0x0 赋值到 地址0x0中，所以直接 oops了 10xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0 但是在哪一行呢，这就需要加载 ko文件了 12345678910111213crash&gt; lsmod MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 (not loaded) [CONFIG_KALLSYMS]crash&gt;crash&gt;crash&gt; mod -s 01_null_pointer /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.ko MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.kocrash&gt;crash&gt; lsmod MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.kocrash&gt; 加载 ko文件之后，直接 dis -l 反汇编 出问题的函数 123456789101112131415161718crash&gt; dis -l create_oops/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 120xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 130xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 140xffffffffc0371016 &lt;create_oops+22&gt;: mov $0x1388,%edi0xffffffffc037101b &lt;create_oops+27&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 160xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 170xffffffffc0371032 &lt;create_oops+50&gt;: callq 0xffffffffadcc96da &lt;printk&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 180xffffffffc0371037 &lt;create_oops+55&gt;: xor %eax,%eax0xffffffffc0371039 &lt;create_oops+57&gt;: retq 直接定位到 12301_null_pointer.c: 160xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0 代码中看看 1*(int *)0 = 0; 问题很快解决了。 试着查看x86 如何调用函数传参的123/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 120xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt; 对应代码是，0x1388 就是十六进制的 5000 1msleep(5000); 是不是第一个整形参数是存在 edi 寄存器中的 123/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 130xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt; 对应的代码是 1printk(&quot;create_oops start\\n&quot;); 0xffffffffc037203c 是啥呢？可以使用 rd 命令读取一下，原来是字符串的起始的地址 1234crash&gt; rd 0xffffffffc037203c 4ffffffffc037203c: 6f5f657461657263 726174732073706f create_oops starffffffffc037204c: 7461657263000a74 652073706f6f5f65 t..create_oops ecrash&gt; 是不是 第一个地址型参数是存放在 rdi 中的呢？ 后面会用不同个数参数，不同类型参数的函数 crash，来实验一下这个是不是对～ 找到一篇讲解x86-64寄存器和函数调用的文章，上面说的猜想就是扯淡。。","link":"/2021/01/21/crash%E4%B8%93%E9%A2%98/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A9%BA%E6%8C%87%E9%92%88oops/"},{"title":"page cache如何产生的","text":"WHY Page cache?CPU如果要访问外部磁盘上的文件，需要首先将这些文件的内容拷贝到内存中，由于硬件的限制，从磁盘到内存的数据传输速度是很慢的，如果现在物理内存有空余，干嘛不用这些空闲内存来缓存一些磁盘的文件内容呢，这部分用作缓存磁盘文件的内存就叫做page cache。 用户进程启动read()系统调用后，内核会首先查看page cache里有没有用户要读取的文件内容，如果有（cache hit），那就直接读取，没有的话（cache miss）再启动I/O操作从磁盘上读取，然后放到page cache中，下次再访问这部分内容的时候，就又可以cache hit，不用忍受磁盘的龟速了（相比内存慢几个数量级）。 由此可见 page_cache 会对 磁盘性能，应用性能有极大提高 但是相对于磁盘，内存的容量还是很有限的，所以没必要缓存整个文件，只需要当文件的某部分内容真正被访问到时，再将这部分内容调入内存缓存起来就可以了，这种方式叫做demand paging（按需调页），把对需求的满足延迟到最后一刻，很懒很实用。 Page cache 组成filePage cache 是文件部分或者全部在内核中缓存的部分，首先需要了解 文件在磁盘和linux中的表现: 在磁盘等存储介质上，文件都是分块存储在磁盘上的， 磁盘inode 是文件唯一标识。 linux系统中为了表示文件，也有文件系统inode，一般会跟文件系统相关，是 从物理磁盘 inode 读取到内存之后的形态 linux系统中虚拟文件系统VFS以实现多文件系统支持，vfs inode是VFS层文件内存数据结构，大多数是所有 文件系统 inode 公共成员 ext4 fs for example: 12345678910111213/* * fourth extended file system inode data in memory */struct ext4_inode_info { __le32 i_data[15]; /* unconverted */ __u32 i_dtime; ext4_fsblk_t i_file_acl; struct rw_semaphore xattr_sem; struct list_head i_orphan; /* unlinked but open inodes */ struct rw_semaphore i_data_sem; struct inode vfs_inode; struct jbd2_inode *jinode; ext4_inode_info 就是对应文件系统inode, vfs_inode 就是 vfs层的inode. 123456789101112131415/* * Structure of an inode on the disk */struct ext4_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size_lo; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Inode Change time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks_lo; /* Blocks count */ ..... ext4_inode 就是对应 物理磁盘的inode. 大多数成员是 记录和物理磁盘 和 物理文件真实 相关的信息 address_space实际情况中，一个文件可能有 100M - 10G这么大，kernel会给文件在内存中分配很多page cache,这些pagecache是如何管理起来的呢，这就引出了 第二个主要结构 address_space – 地址空间。 首先看 address_space 定义 12345678910111213141516171819202122232425262728293031323334353637383940/** * struct address_space - Contents of a cacheable, mappable object. * @host: Owner, either the inode or the block_device. * @i_pages: Cached pages. * @gfp_mask: Memory allocation flags to use for allocating pages. * @i_mmap_writable: Number of VM_SHARED mappings. * @nr_thps: Number of THPs in the pagecache (non-shmem only). * @i_mmap: Tree of private and shared mappings. * @i_mmap_rwsem: Protects @i_mmap and @i_mmap_writable. * @nrpages: Number of page entries, protected by the i_pages lock. * @nrexceptional: Shadow or DAX entries, protected by the i_pages lock. * @writeback_index: Writeback starts here. * @a_ops: Methods. * @flags: Error bits and flags (AS_*). * @wb_err: The most recent error which has occurred. * @private_lock: For use by the owner of the address_space. * @private_list: For use by the owner of the address_space. * @private_data: For use by the owner of the address_space. */struct address_space { struct inode *host; //一般就是 inode 与 文件关联 struct xarray i_pages; // xarray 管理着这个地址空间里面所有的 page,之前kernel版本是 radix tree gfp_t gfp_mask; atomic_t i_mmap_writable;#ifdef CONFIG_READ_ONLY_THP_FOR_FS /* number of thp, only for non-shmem files */ atomic_t nr_thps;#endif struct rb_root_cached i_mmap; // i_mmap 红黑树的根节点，会将 page 按照 某种？ 序列组织起来，便于查找 struct rw_semaphore i_mmap_rwsem; unsigned long nrpages; unsigned long nrexceptional; pgoff_t writeback_index; const struct address_space_operations *a_ops; unsigned long flags; errseq_t wb_err; spinlock_t private_lock; struct list_head private_list; void *private_data;} __attribute__((aligned(sizeof(long)))) __randomize_layout; 这个定义没有 inode 那么长，但是很核心 这样 inode 与 many pages 通过 address_space 的 host 与 i_pages 成员 相互连接起来。 其中 inode 既可以是 磁盘文件的 inode，也可以是 内存文件系统 的 inode（proc sys等）还可以是 swap 文件的 inode. a_ops 同样也是一个基类指针，定义了抽象的文件系统交互接口，由具体文件系统负责实现。例如如果文件是存储在ext4文件系统之上，那么该结构便被初始化为 ext4_aops （见fs/ext4/inode.c）。 如何查找 一个文件的page_cache?inode –&gt; address_space: container_of 通过inode 找到 地址空间address_space –&gt; i_pages: 成员变量访问 address_space 是Linux内核中的一个关键抽象，它是页缓存和外部设备中文件系统的桥梁。 上层应用读取数据会进入到该结构内的page cache，上层应用对文件的写入内容也会缓存于该结构内的page cache。 $$ 这里配图 dentry这和 pagecache 关系没有那密切。 假如需要查找 /etc/apt/aaa 这个文件，linux系统会如何去查找呢？文件目录这些信息就涉及到 dentry 的信息了，dentry也是实现Linux文件系统目录层次结构的关键. dentry 从另外一个层面描述文件：文件名. 更准确地说，是保存文件名和文件inode号 与 inode 一样， dentry 除了VFS层dentry结构，每种具体文件系统也有自身的内存dentry和 磁盘dentry结构 ext4 fs for example: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647struct dentry { /* RCU lookup touched fields */ unsigned int d_flags; /* protected by d_lock */ seqcount_spinlock_t d_seq; /* per dentry seqlock */ struct hlist_bl_node d_hash; /* lookup hash list */ struct dentry *d_parent; /* parent directory */ struct qstr d_name; struct inode *d_inode; /* Where the name belongs to - NULL is * negative */ unsigned char d_iname[DNAME_INLINE_LEN]; /* small names */ /* Ref lookup also touches following */ struct lockref d_lockref; /* per-dentry lock and refcount */ const struct dentry_operations *d_op; struct super_block *d_sb; /* The root of the dentry tree */ unsigned long d_time; /* used by d_revalidate */ void *d_fsdata; /* fs-specific data */ union { struct list_head d_lru; /* LRU list */ wait_queue_head_t *d_wait; /* in-lookup ones only */ }; struct list_head d_child; /* child of parent list */ struct list_head d_subdirs; /* our children */ /* * d_alias and d_rcu can share memory */ union { struct hlist_node d_alias; /* inode alias list */ struct hlist_bl_node d_in_lookup_hash; /* only for in-lookup ones */ struct rcu_head d_rcu; } d_u;} __randomize_layout;/* * The new version of the directory entry. Since EXT4 structures are * stored in intel byte order, and the name_len field could never be * bigger than 255 chars, it's safe to reclaim the extra byte for the * file_type field. */struct ext4_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; /* See file type macros EXT4_FT_* below */ char name[EXT4_NAME_LEN]; /* File name */}; 在ext4中内存与磁盘dentry结构 ext4_dir_entry_2 看起来保持了一致。磁盘文件系统dentry也必须被持久化存储在磁盘上。","link":"/2020/09/07/page_cache/page%20cache%E4%BA%A7%E7%94%9F/"},{"title":"我的服务器配置","text":"服务器配置我之前一直是使用 笔记本windows+ubuntu虚拟机 的开发配置，但是这样有几个限制： ubuntu 虚拟机比较耗费资源，一到夏天笔记本风扇狂转，影响体验 只在本地使用没有问题，但是有时候还想在公司或者出差的时候使用 基于以上种种限制，我就搞了腾讯云的服务器，1H2G1M3Y 价格好像是 300￥，还是比较便宜的。 但是直到前些天才真正用起来，主要用于编译和测试，但是一直有几个缺点： 云服务器基本都是基于 KVM的，然后他还禁止了租户继续使用KVM进行虚拟化 云服务器禁止了 邮件服务的端口，导致无法收发邮件 云服务器在低配置（1核心2G）下，费用较低，一旦配置升高，价格急剧上升，性价比不高 1核2G用来编译效率太低 云服务器带宽资源比较昂贵，在本地连接云服务器的时候经常会需要连接好久才能连接上 考虑最近AMD cpu很强势，便配置了一个AMD 4650G的ITX机器，用作我自己的服务器。 物理服务器安装我直接选择了 ubuntu20.04.1 版本，通过UltraISO 工具将iso写入SD卡中，然后开启U盘启动，安装ubuntu。还需要通过 主板 BIOS开启 KVM，通过CPU设置开启 SVM:如果不开启KVM直接用qemu开启 ubuntu虚拟机，效率比开启KVM虚拟机要慢 50倍左右，这也是我不想用 云服务器的原因。换用清华源 物理服务器安装qemu-ubuntu虚拟机可以参考文章:qemu起ubuntu server 在云服务器上按着文章成功配置了qemu虚拟机，但是在服务器上一直是失败的，原因就是起了服务器之后一直无法登陆。所以最后我用了桌面版本ubuntu 20.04.1，而不是 ubuntu server。 下载 ubuntu iso 12wget https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04.1/ubuntu-20.04.1-desktop-amd64.isomv ubuntu-20.04.1-desktop-amd64.iso ubuntu.iso 创建磁盘 1qemu-img create -q -f qcow2 stable_ubuntu.img 32G 安装虚拟机这一步尽量不要用 ssh做，直接在物理机器上搞比较快，第一次安装需要 -cdrom指定iso文件位置，后面就不需要了。在安装的之后需要指定 用户名密码。 12安装sudo qemu-system-x86_64 ubuntu.img -m 1024 -cdrom ubuntu.iso --enable-kvm 启动虚拟机启动虚拟机分为两种:a. 直接启动原来内核的虚拟机(可以直接联网，下载软件)也可以ssh链接： ssh -v rlk@127.0.0.1 -p 2222 12345678910sudo qemu-system-x86_64 \\ -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\ -smp 4 \\ -m 4096 \\ --enable-kvm \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ --nographic \\ -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\ -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share b. 用新kernel代替 stable_ubuntu.img中 的 kernel(替代内核) 123456789101112sudo qemu-system-x86_64 \\ -kernel /home/ubuntu/workspace/share/stable/bzImage \\ -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\ -append &quot;root=/dev/sda5 console=ttyS0&quot; \\ -smp 4 \\ -m 4096 \\ --enable-kvm \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ --nographic \\ -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\ -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share 至于开机启动都是 /etc/rc.local 里面配置https://gitee.com/shshshsh/cloud_server_shell test 相关a. ltp（linux kernel test project）：linux内核测试 project 编译安装，运行https://github.com/linux-test-project/ltp b. mmetst 可以测试 linux kernel 不同版本之间性能差异https://github.com/gormanm/mmtestshttps://lwn.net/Articles/820823/https://lwn.net/Articles/463339/https://blog.csdn.net/Linux_Everything/article/details/106485101 c. lkp （linux kernel performance）:是 intel发起的一个kernel performance test项目https://github.com/fengguang/lkp-https://01.org/lkphttps://01.org/blogs/jdu1/2017/lkp-tests-linux-kernel-performance-test-and-analysis-toolhttps://github.com/sammcj/kernel-cihttp://hejq.me/2014/10/30/lkp-tests-notes/https://libraries.io/github/openthos/lkp-analysis 学习kvmhttps://www.linux-kvm.org/page/Kvmtoolshttps://blog.csdn.net/leoufung/article/details/48781119git://git.kernel.org/pub/scm/linux/kernel/git/will/kvmtool.git 远程访问本地服务器由于本地服务器其实是没有固定ip的，所以需要一台固定ip的服务器做中转，这样才能从任何地方远程访问到我的物理服务器，正好有一个腾讯云服务器，就充当了这个角色。 我使用的是 nps 这个开源代理工具，相比于其他工具来说，配置简单，直接在网页上就可以配置参考超好用轻量级NPS内网穿透 NPS问题 稳定性：这个我其实已经使用了超过1个月了，机器都没重启过，服务都还正常，对于个人使用来说完全足够了 带宽由于我的腾讯云服务器带宽了 1Mbps,其实峰值带宽也就 128Kb/s。 我使用服务器主要场景是 1231. 有编译任务2. ssh 到服务器3. vscode ssh 到服务器看代码，写代码 其实 1 和 2对 带宽要求都很低，但是 vscode ssh 对于带宽要求还是比较高的，从腾讯云后台监控数据上看，我的服务器 每次都是 vscode ssh 到腾讯云的时候或者服务器的时候带宽都用满了，导致有时候ssh需要连 几min 才能连接上。 vscode ssh 在连接上之后对于带宽使用率其实很低，在连接瞬间有较高要求。 我无奈之下就想给 腾讯云的服务搞成按流量计费的，一通操作下来没想到腾讯云居然不支持活动时候买的服务器改网络配置。。 那就只能升级到2M带宽 或者更高了，升级了一下到 2Mbps,然后再用 vscode shh连接 就很丝滑了，然后我就想能不能通过调整vscode ssh配置来达到这样目的的，发现好像还真有个 timeout 时间，默认 15s，我改到了30s，也可能如果不升级带宽,仅仅将配置改到30s vscode经常超时的问题就好了呢？ 等到5月份之后不续费再继续看看情况。。。","link":"/2020/12/19/qemu/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AEqemu/"},{"title":"mmap_sem可扩展问题","text":"在 Linux 中，同进程的不同线程虽然对应不同的 task_struct ，但都共享同一个 mm_struct ，即 task_struct::mm 指向同一个变量，其描述了该进程的整个虚拟地址空间。 mm_struct 包含的主要成员如下： 1234567struct mm_struct { struct vm_area_struct *mmap; /* list of VMAs */ struct rb_root mm_rb; pgd_t * pgd; struct rw_semaphore mmap_sem;...}; 其中， mmap 指向了 vm_area_struct 双向链表，每个 vm_area_struct (VMA) 描述了虚拟地址空间的一个区间，比如 text / data / bss / heap / stack 各对应一个 VMA ，mmap 每次调用会产生 VMA(如果可和之前合并的话则不会产生新 VMA)。同时为了加速 VMA 查找，vm_area_struct 之间亦通过红黑树串起来，通过 mm_rb 指向。 接下来就是大家都很熟悉的页表，在 x86_64 下，分页采用 PGD - PUD - PMD - PTE 四级页表，在此通过 pgd 指向 PGD 页表项。 mmap_sem 是一把很大的锁 (信号量)，进程内大部分的内存操作都需要拿它： 所有对 VMA 的操作 (mmap、munmap)所有对页表的修改 (page fault 等)madvise (如 jemalloc 经常使用的 MADV_DONTNEED) Problem某业务进程包含若干个工作线程和一个数据加载线程，每隔一段时间数据加载线程会映射一份新的数据到内存供工作线程使用，并负责将工作线程不再使用的上一份数据释放。结果发现在释放期间，工作线程受到了影响，导致失败率上升。通过一波分析后发现是 mummap 大块内存 (20G+) 造成的。 我们来看内核中 munmap 的实现： 123456789101112SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len){ int ret; struct mm_struct *mm = current-&gt;mm; profile_munmap(addr); if (down_write_killable(&amp;mm-&gt;mmap_sem)) return -EINTR; ret = do_munmap(mm, addr, len); up_write(&amp;mm-&gt;mmap_sem); return ret;} 可以发现刚进来就把 mmap_sem 的写锁拿上了，然后执行 do_munmap ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384int do_munmap(struct mm_struct *mm, unsigned long start, size_t len){ unsigned long end; struct vm_area_struct *vma, *prev, *last; if ((offset_in_page(start)) || start &gt; TASK_SIZE || len &gt; TASK_SIZE-start) return -EINVAL; len = PAGE_ALIGN(len); if (len == 0) return -EINVAL; /* Find the first overlapping VMA */ vma = find_vma(mm, start); if (!vma) return 0; prev = vma-&gt;vm_prev; /* we have start &lt; vma-&gt;vm_end */ /* if it doesn't overlap, we have nothing.. */ end = start + len; if (vma-&gt;vm_start &gt;= end) return 0; /* * If we need to split any vma, do it now to save pain later. * * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially * unmapped vm_area_struct will remain in use: so lower split_vma * places tmp vma above, and higher split_vma places tmp vma below. */ if (start &gt; vma-&gt;vm_start) { int error; /* * Make sure that map_count on return from munmap() will * not exceed its limit; but let map_count go just above * its limit temporarily, to help free resources as expected. */ if (end &lt; vma-&gt;vm_end &amp;&amp; mm-&gt;map_count &gt;= sysctl_max_map_count) return -ENOMEM; error = __split_vma(mm, vma, start, 0); if (error) return error; prev = vma; } /* Does it split the last one? */ last = find_vma(mm, end); if (last &amp;&amp; end &gt; last-&gt;vm_start) { int error = __split_vma(mm, last, end, 1); if (error) return error; } vma = prev ? prev-&gt;vm_next : mm-&gt;mmap; /* * unlock any mlock()ed ranges before detaching vmas */ if (mm-&gt;locked_vm) { struct vm_area_struct *tmp = vma; while (tmp &amp;&amp; tmp-&gt;vm_start &lt; end) { if (tmp-&gt;vm_flags &amp; VM_LOCKED) { mm-&gt;locked_vm -= vma_pages(tmp); munlock_vma_pages_all(tmp); } tmp = tmp-&gt;vm_next; } } /* * Remove the vma's, and unmap the actual pages */ detach_vmas_to_be_unmapped(mm, vma, prev, end); unmap_region(mm, vma, prev, start, end); arch_unmap(mm, vma, start, end); /* Fix up all other VM information */ remove_vma_list(mm, vma); return 0;} 个函数做了很多事：首先通过 find_vma 找到 start 到 end 之间相应的 VMA ，必要时通过 __split_vma 对 VMA 进行切分。随后将 VMA 从链表和红黑树中移除，并对移除的区域调用 unmap_region =&gt; free_pgtables 清空该区域对应的页表项。 对于 mummap 大块内存 (20G+) 这样的操作，其中最耗时的应该是 free_pgtables ：20G 内存对应 20 * 1024 * 1024 / 4 = 5242880 个 4KB 页，要清空这 5242880 个页的页表项无疑是非常耗时的，实测需要好几秒。在此期间，进程的 mmap_sem 的写锁一直被拿住，导致其他线程的对内存的相关操作阻塞在 mmap_sem 锁上，从而导致了性能抖动。 为了解决这个问题，同事实现了分段 unmap ：在业务程序对 munmap 再进行一层封装，当遇到大块内存释放时，将内存切分成若干段，比如 128MB 一段，依次对每段调用 munmap ，并在调用后 sleep 若干毫秒，这样避免了长时间拿 mmap_sem ，卡住其他工作进程导致影响服务质量的问题。和 Yang Shi 在 Drop mmap_sem during unmapping large map 提出的内核态实现有异曲同工之妙。 总结从用户的角度来看，一个线程不应该受到另外一个线程的干扰。然而从这次的 case 来看，多个线程对进程级共享的 mmap_sem 的竞争是问题产生的主要原因，换句话说，mmap_sem 管太多，导致干啥都要拿它，从而限制了进程的可扩展性。 能否换用更细粒度的锁？Laurent DUFOUR 在 LPC2019 上提出用 VMA 层级的锁来取代 mmap_sem ，这样在上述的 case 中数据加载线程只会拿住 20G 内存对应的 VMA 锁，而不会影响到早已不使用这块内存的其他工作线程。当然，换用更细粒度的锁带来的就是内核实现上复杂度的提升，比如需要遵循一些规则来避免死锁。 参考：mmap_sem 的可扩展性LWN文章PDF文档","link":"/2020/09/14/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/mmap_sem%E5%8F%AF%E6%89%A9%E5%B1%95%E9%97%AE%E9%A2%98/"},{"title":"per-cpu变量引起的思考","text":"疑问 内核为什么要引入per-cpu变量？解决了什么问题？ per-cpu变量如何实现的？ 是否可以用数组实现per-cpu变量？ per-cpu变量访问有什么原则？ 如果一个per-cpu变量既可以在线程上下文中访问，又可以在中断上下文中访问，需要保护吗？ 接下来我们就带着以上的疑问继续往下看 内核为什么要引入per-cpu变量？解决了什么问题？per-cpu变量是Linux内核中的一种同步机制。 当系统中的所有CPU访问并共享变量V时，CPU0修改变量V的值。 CPU1也会同时修改变量V，这将导致变量V的值不正确。如果使用了原子锁，CPU0只能等待修改。 这种方式有两个缺点： （1）原子操作很耗时 （2）现在，所有CPU都具有L1高速缓存，因此许多CPU同时访问变量将导致高速缓存一致性问题。 当CPU修改共享变量V时，其他CPU上的相应缓存行必须无效，这会导致性能损失。 per-cpu变量提供了解决上述问题的有趣功能。 它将变量的副本分配给系统中的每个处理器。 在多处理器系统中，当处理器只能访问其所属变量的副本时，无需考虑与其他处理器的竞争，因此可以充分利用处理器的本地硬件缓存来提高性能。 。 per-cpu变量如何实现的？这里分 静态per-cpu变量 和 动态per-cpu变量首先看一下使用的 API 123456789101112// 静态定义 include/linux/Percpu-def.h #define DECLARE_PER_CPU(type, name) \\ DECLARE_PER_CPU_SECTION(type, name, &quot;&quot;)#define DEFINE_PER_CPU(type, name) \\ DEFINE_PER_CPU_SECTION(type, name, &quot;&quot;)// 动态定义 include/linux/percpu.h#define alloc_percpu(type) \\ (typeof(type) __percpu *)__alloc_percpu(sizeof(type), __alignof__(type))void free_percpu(void __percpu *__pdata);","link":"/2020/09/05/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/percpu%E5%8F%98%E9%87%8F%E5%BC%95%E8%B5%B7%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"softirq何时会被执行","text":"很多人可能都知道中断irq，但是对软中断softfirq却比较陌生，软中断这个概念是纯软件意义上的，与中断依赖于硬件行为不一样。在linux中，软中断主要用于执行irq中没有执行，但又不是很紧急的事情，现在linux内核中为每个CPU都分配了一个线程 [ksoftirqd/n]，用来执行软中断。 12345sh@ubuntu[root]:/sys/kernel/debug/tracing# ps -aux |grep softroot 10 0.0 0.0 0 0 ? S 9月19 0:19 [ksoftirqd/0]root 18 0.0 0.0 0 0 ? S 9月19 0:17 [ksoftirqd/1]root 24 0.0 0.0 0 0 ? S 9月19 0:18 [ksoftirqd/2]root 30 0.0 0.0 0 0 ? S 9月19 0:21 [ksoftirqd/3] 具体软中断分以下几种 12345678910111213enum{ HI_SOFTIRQ=0, TIMER_SOFTIRQ, NET_TX_SOFTIRQ, NET_RX_SOFTIRQ, BLOCK_SOFTIRQ, IRQ_POLL_SOFTIRQ, TASKLET_SOFTIRQ, SCHED_SOFTIRQ, HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the numbering. Sigh! */ RCU_SOFTIRQ, /* Preferable RCU should always be the last softirq */ NR_SOFTIRQS}; 通过 open_softirq 初始化 1234567static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;void open_softirq(int nr, void (*action)(struct softirq_action *)){ softirq_vec[nr].action = action;} 触发软中断的时候通过 raise_softirq 来触发 12345678void raise_softirq(unsigned int nr){ unsigned long flags; local_irq_save(flags); raise_softirq_irqoff(nr); local_irq_restore(flags);} 但tasklet机制直接使用了 raise_softirq_irqoff，搜索代码可以发现其他类型的softirq也基本都是用 raise_softirq_irqoff 触发。 123456789101112131415static void __tasklet_schedule_common(struct tasklet_struct *t, struct tasklet_head __percpu *headp, unsigned int softirq_nr){ struct tasklet_head *head; unsigned long flags; local_irq_save(flags); head = this_cpu_ptr(headp); t-&gt;next = NULL; *head-&gt;tail = t; head-&gt;tail = &amp;(t-&gt;next); raise_softirq_irqoff(softirq_nr); local_irq_restore(flags);} 来看下 raise_softirq_irqoff 实现， 123456789101112131415#define local_softirq_pending_ref irq_stat.__softirq_pending#define or_softirq_pending(x) (__this_cpu_or(local_softirq_pending_ref, (x)))void __raise_softirq_irqoff(unsigned int nr){ trace_softirq_raise(nr); // 软中断 raise 的 tracepoint 点 or_softirq_pending(1UL &lt;&lt; nr); // 设置当前cpu的软中断pending状态}inline void raise_softirq_irqoff(unsigned int nr){ __raise_softirq_irqoff(nr); if (!in_interrupt()) wakeup_softirqd(); //如果不是中断上下文，就需要唤醒 ksoftirqd来执行相关软中断，这也保证了在线程上下文中软中断可以得到较快执行} __raise_softirq_irqoff 仅仅是设置 __softirq_pending 标志位，这有两个作用 如果当前是中断irq上下文，在 irq_exit 之后，检查 local_softirq_pending，判断有软中断需要执行 如果当前是线程上下文，在 ksoftirq 线程中检查标志位，最后执行相关的软中断 如果当前在临界区上，在打开中断时，可以检测pending的软中断如何去执行 irq_exit的情况 1234567891011121314151617181920212223static inline void invoke_softirq(void){ //不考虑软中断强制线程化，简化代码 if (ksoftirqd_running(local_softirq_pending())) return; __do_softirq();}void irq_exit(void) {#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED local_irq_disable();#else lockdep_assert_irqs_disabled();#endif account_irq_exit_time(current); preempt_count_sub(HARDIRQ_OFFSET); if (!in_interrupt() &amp;&amp; local_softirq_pending()) invoke_softirq(); //在中断退出之后，如果有 pending的软中断，就需要执行 软中断 tick_irq_exit(); rcu_irq_exit(); trace_hardirq_exit(); /* must be last! */} ksoftirq线程 123456789101112131415static void run_ksoftirqd(unsigned int cpu){ local_irq_disable(); if (local_softirq_pending()) { /* * We can safely run softirq on inline stack, as we are not deep * in the task stack here. */ __do_softirq(); local_irq_enable(); cond_resched(); return; } local_irq_enable();} 开启中断的情况下 123456789101112131415161718void __local_bh_enable_ip(unsigned long ip, unsigned int cnt){ WARN_ON_ONCE(in_irq()); lockdep_assert_irqs_enabled(); preempt_count_sub(cnt - 1); if (unlikely(!in_interrupt() &amp;&amp; local_softirq_pending())) { // 如果不在中断中，且 softirq 有pending的位，就需要执行软中断 do_softirq(); } preempt_count_dec(); preempt_check_resched();}EXPORT_SYMBOL(__local_bh_enable_ip);static inline void local_bh_enable(void){ __local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);} __do_softirq不管是退出中断时执行软中断，还是在ksoftirqd中執行软中断，最终都会执行到 __do_softirq 这个函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#define MAX_SOFTIRQ_TIME msecs_to_jiffies(2) //2ms#define MAX_SOFTIRQ_RESTART 10asmlinkage __visible void __softirq_entry __do_softirq(void){ unsigned long end = jiffies + MAX_SOFTIRQ_TIME; unsigned long old_flags = current-&gt;flags; int max_restart = MAX_SOFTIRQ_RESTART; struct softirq_action *h; bool in_hardirq; __u32 pending; int softirq_bit; /* * Mask out PF_MEMALLOC as the current task context is borrowed for the * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC * again if the socket is related to swapping. */ current-&gt;flags &amp;= ~PF_MEMALLOC; pending = local_softirq_pending(); account_irq_enter_time(current); __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET); in_hardirq = lockdep_softirq_start();restart: /* Reset the pending bitmask before enabling irqs */ set_softirq_pending(0); local_irq_enable(); h = softirq_vec; while ((softirq_bit = ffs(pending))) { //循环执行 pending的软中断 unsigned int vec_nr; int prev_count; h += softirq_bit - 1; vec_nr = h - softirq_vec; prev_count = preempt_count(); kstat_incr_softirqs_this_cpu(vec_nr); trace_softirq_entry(vec_nr); //trace 软中断执行 h-&gt;action(h); trace_softirq_exit(vec_nr); //trace 软中断退出 if (unlikely(prev_count != preempt_count())) { pr_err(&quot;huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\n&quot;, vec_nr, softirq_to_name[vec_nr], h-&gt;action, prev_count, preempt_count()); preempt_count_set(prev_count); } h++; pending &gt;&gt;= softirq_bit; } if (__this_cpu_read(ksoftirqd) == current) rcu_softirq_qs(); local_irq_disable(); pending = local_softirq_pending(); if (pending) { if (time_before(jiffies, end) &amp;&amp; !need_resched() &amp;&amp; --max_restart) //如果又有pending的软中断了，看看是否执行超时了 2ms，且 restart 不能超过10次 goto restart; //未超时 wakeup_softirqd(); //超时 2ms，唤醒softirqd去执行软中断 } lockdep_softirq_end(in_hardirq); account_irq_exit_time(current); __local_bh_enable(SOFTIRQ_OFFSET); WARN_ON_ONCE(in_interrupt()); current_restore_flags(old_flags, PF_MEMALLOC);} do_softirqdo_softirq 和 其他代码路径下执行软中断不一样，最终执行代码的是 do_softirq_own_stack，后续分析 -=-！ 12345678910111213141516171819202122232425void do_softirq_own_stack(void){ struct irq_stack *irqstk; u32 *isp, *prev_esp; irqstk = __this_cpu_read(softirq_stack_ptr); /* build the stack frame on the softirq stack */ isp = (u32 *) ((char *)irqstk + sizeof(*irqstk)); /* Push the previous esp onto the stack */ prev_esp = (u32 *)irqstk; *prev_esp = current_stack_pointer; call_on_stack(__do_softirq, isp);}asmlinkage __visible void do_softirq(void){ unsigned long flags; local_irq_save(flags); if (local_softirq_pending() &amp;&amp; !ksoftirqd_running(pending)) do_softirq_own_stack(); local_irq_restore(flags);} 如何观察softirq在softirq处理过程中（非ksoftirqd线程），在该local cpu上的其他进程是无法进行调度的，不管进程有多高的优先级。因此这点势必对系统实时性造成影响。 通过 /proc/softirqs ，配合 watch 命令来观察 通过一些tracepoint来观察","link":"/2020/09/20/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/softirq%E4%BD%95%E6%97%B6%E4%BC%9A%E8%A2%AB%E6%89%A7%E8%A1%8C/"},{"title":"linux内核死锁检测","text":"死锁类型大概可以将linux内核中死锁分为以下两类 121. AA型死锁2. AB-BA型死锁 AA型死锁AA型死锁主要是两个代码路径竞争同一把锁造成的： 进程连续连续两次申请同一把锁就会导致在在第二次申请这把锁的时候死锁 或者是读写锁，同一代码路径先申请了读锁，然后在读锁没有释放的前提下又申请了写者锁，这就会导致在申请写者锁的时候发生死锁 或者在具有抢占关系的代码路径上申请同一把锁，比如进程申请了一把锁A，然后发生了中断或者软中断都会抢占进程，如果在中断或者软中断中又去申请这把锁就会导致死锁的发生 AB-BA型死锁AB-BA型死锁 主要是对两把锁加锁顺序不确定导致的死锁： 假设进程1，2执行过程中都需要持有AB两个锁，进程1线申请到了A锁，进程2申请到了B锁，此时进程1尝试去获取B锁，进程2尝试去获取A锁都无法成功，进程1等待进程2使用完B锁，进程2等待进程1使用完A锁，这就导致了ABBA的死锁情况 还有一种情形是和中断上下文中，进程1需要顺序持有A-B两把锁，进程2会持有B锁，中断会持有A锁。假设T时刻进程1已经持有了A锁，进程2已经持有了B锁，然后进程A去尝试获取B锁，此时中断打断了进程2，试图获取A锁，这样又完美的形成了AB-BA的死锁 想要避免AB-BA死锁的问题，最简单有效的一个办法就是定义获取锁的顺序，不要在不同代码路径下获取不同锁的顺序不一样，破坏死锁的条件。如果大家获取锁的顺序都是一样的，死锁概率会降低很多。 lockdep其实想要在内核成千上百个锁中定义好持有锁的顺序，并且coder严格按照要求来执行的可能性很小，内核同时也提供了死锁检测的工具lockdep。开启这个工具只需要开启下面这两个宏 12CONFIG_PROVE_LOCKING=yCONFIG_DEBUG_LOCK_ALLOC=y 在menuconfig之后会自动打开CONFIG_LOCKDEP这个宏，重新编译替换内核就可以检测kernel中的死锁问题了.. lockdep实现原理开个头，后面好好分析一下-=- 案列分析开个头，后面好好分析一下-=-","link":"/2020/09/11/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/%E6%AD%BB%E9%94%81%E9%97%AE%E9%A2%98/"},{"title":"kdump+crash定位稳定性问题","text":"kudmp 原理kdump实际上有两个内核，一个是正常运行的内核，一个我们称为捕获内核/第二内核。 首先需要通过crashkernel=xxx@xxxx来专门为kdump功能专门预留一部分内存，用来放置第二个内核和其他数据信息的，这一部分是给捕获内核使用的。 可以通过kexec系统调用，将第二个内核和initrd，vmcore信息，启动参数等传递给第一个内核。 当系统崩溃时，保存当前寄存器信息，检查是否加载了捕获内核，如果存在则跳转到捕获内核，捕获内核和正常内核一样启动，但是会提供/proc/vmcore接口能够导出内存镜像信息，可以使用dd工具，高级的makedumpfile提供压缩选项。 系统重启，之后就可以使用crash/gdb分析dump文件了 所以可以配置的就本就是 crashkernel的大小，makedumpfile 工具 kudmp 安装使用首先是编译 kernel 配置 12345CONFIG_KEXEC=yCONFIG_SYSFS=yCONFIG_DEBUG_INFO=YvCONFIG_CRASH_DUMP=yCONFIG_PROC_VMCORE=y 安装 12sudo apt install kdump-toolssudo apt install crash 如何使用？可以直接主动触发一个崩溃来看看效果 12sudo suecho c &gt; /proc/sysrq-trigger 然后就进入转存储过程，一般会转存储到 /var/crash/$date 目录下，此时应该包含 dump.$date 文件，但是还需要 vmlinux 文件才可以使用 crash 工具分析。vmlinux 是 编译生成的带符号表的可执行文件，只要将它copy到当前目录就可以一起分析了。 一般系统这样就可以搞起来了。 kdump 遇到的问题但是我这边是一直跟踪 mainline kernel, 上述过程都不是很顺利。遇到的问题： makefiledump 转存储不成功1234567[ 4.030842] kdump-tools[301]: Starting kdump-tools:[ 4.032464] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192137/dump-incomplete[ 24.045632] kdump-tools[380]: check_release: Can't get the kernel version.[ 24.069205] kdump-tools[380]: The kernel version is not supported.[ 24.071343] kdump-tools[380]: The makedumpfile operation may be incomplete.[ 24.077177] kdump-tools[380]: makedumpfile Failed.[ 24.079278] kdump-tools[308]: * kdump-tools: makedumpfile failed, falling back to 'cp' 我们发现是 运行 makedumpfile 过程中，由于 无法 get kernel version 导致无法压缩转存储成功。找到 makedumpfile 源码 添加log 复现。(重新编译 makedumpfile 花费了不少时间) 12345678910[ 4.030842] kdump-tools[301]: Starting kdump-tools:[ 4.032464] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192137/dump-incomplete[ 4.041333] kdump-tools[380]: create_dumpfile: create_dumpfile![ 24.045632] kdump-tools[380]: check_release: Can't get the kernel version.[ 24.056179] kdump-tools[380]: The kernel version 0:0:0.[ 24.063203] kdump-tools[380]: The kernel version:[0] old_version:[33947663] new_version:[85196807].[ 24.069205] kdump-tools[380]: The kernel version is not supported.[ 24.071343] kdump-tools[380]: The makedumpfile operation may be incomplete.[ 24.077177] kdump-tools[380]: makedumpfile Failed.[ 24.079278] kdump-tools[308]: * kdump-tools: makedumpfile failed, falling back to 'cp' 是这段代码原因 1234567891011121314151617181920#define OLDEST_VERSION KERNEL_VERSION(2, 6, 15) /* linux-2.6.15 */#define LATEST_VERSION KERNEL_VERSION(5, 9, 4) /* linux-5.9.4 */int32_t get_kernel_version(char *release){ version = KERNEL_VERSION(maj, min, rel); if ((version &lt; OLDEST_VERSION) || (LATEST_VERSION &lt; version)) { MSG(&quot;The kernel version is not supported.\\n&quot;); MSG(&quot;The makedumpfile operation may be incomplete.\\n&quot;); }}int check_release(void){ info-&gt;kernel_version = get_kernel_version(info-&gt;system_utsname.release); if (info-&gt;kernel_version == FALSE) { ERRMSG(&quot;Can't get the kernel version.\\n&quot;); return FALSE; }} 第一步先将 LATEST_VERSION 宏定义改掉 1#define LATEST_VERSION KERNEL_VERSION(5, 20, 7) /* linux-5.20.7 */ 还有一个是要看 为什么获得kernel version不对 123456[ 3.917016] kdump-tools[301]: Starting kdump-tools:[ 3.919053] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192153/dump-incomplete[ 3.930292] kdump-tools[392]: create_dumpfile: create_dumpfile![ 3.943676] kdump-tools[392]: check_release: info-&gt;system_utsname.release: 5.11.0-rc4+.[ 3.955855] kdump-tools[392]: check_release: info-&gt;release: .rc4+.Copying data : [100.0 %] \\ eta: 看代码应该是这段的问题 12345678910111213141516check_release(void){ unsigned long utsname; /* * Get the kernel version. */ if (SYMBOL(system_utsname) != NOT_FOUND_SYMBOL) { utsname = SYMBOL(system_utsname); } else if (SYMBOL(init_uts_ns) != NOT_FOUND_SYMBOL) { utsname = SYMBOL(init_uts_ns) + sizeof(int); //?????? } else { ERRMSG(&quot;Can't get the symbol of system_utsname.\\n&quot;); return FALSE; }} 去掉 + sizeof(int) 即可正常工作 12345[ 3.917016] kdump-tools[3https://github.com/crash-utility/crash01]: Starting kdump-tools:[ 3.919053] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192153/dump-incomplete[ 3.943676] kdump-tools[392]: check_release: info-&gt;system_utsname.release: 5.11.0-rc4+.[ 3.955855] kdump-tools[392]: check_release: info-&gt;release: 5.11.0-rc4+.Copying data : [100.0 %] \\ eta: 0s crash 版本较低不合适拿到转存储的dump文件之后，就可以用crash 来分析了 ubuntu 20.04 自带的 crash版本是 7.2.8 123456789101112131415161718192021222324252627stable_kernel@kernel: /var/crash/202101192317# sudo crash dump.202101192317 vmlinuxcrash 7.2.8Copyright (C) 2002-2020 Red Hat, Inc.Copyright (C) 2004, 2005, 2006, 2010 IBM CorporationCopyright (C) 1999-2006 Hewlett-Packard CoCopyright (C) 2005, 2006, 2011, 2012 Fujitsu LimitedCopyright (C) 2006, 2007 VA Linux Systems Japan K.K.Copyright (C) 2005, 2011 NEC CorporationCopyright (C) 1999, 2002, 2007 Silicon Graphics, Inc.Copyright (C) 1999, 2000, 2001, 2002 Mission Critical Linux, Inc.This program is free software, covered by the GNU General Public License,and you are welcome to change it and/or distribute copies of it undercertain conditions. Enter &quot;help copying&quot; to see the conditions.This program has absolutely no warranty. Enter &quot;help warranty&quot; for details.GNU gdb (GDB) 7.6Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [702MB]: patching 130299 gdb minimal_symbol valuesplease wait... (patching 130299 gdb minimal_symbol values) [1] 4172 segmentation fault sudo crash dump.202101192317 vmlinux 发现一直是有 segmentfault..怎么也没法解决，才想到可能是 crash 根据版本较低导致的。 找到 crash 源码 找到 7.2.8 的release版本， 发现最新的版本是 7.2.9就下载了7.2.9版本编译安装，其中还需要下载 gdb-7.6,建议直接国内源下载 但是还会出现如下错误 1234567891011121314151617181920stable_kernel@kernel: /var/crash/202101192317# sudo crash dump.202101192317 vmlinuxThis GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [702MB]: patching 130299 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101192317 [PARTIAL DUMP] CPUS: 4 DATE: Tue Jan 19 23:17:07 CST 2021 UPTIME: 00:03:31LOAD AVERAGE: 0.06, 0.06, 0.01 TASKS: 476 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #27 SMP Tue Jan 19 13:36:03 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Kernel panic - not syncing: sysrq triggered crash&quot; PID: 3655crash: cannot determine length of symbol: log_end 最后看了看 crash github 的 issue,地一个 open 的问题就是这个…issuse-74 12The crash-7.2.9 doesn't support the 5.10 kernel. It needs these two patchesa5531b2 and 71e159c, so please use the latest master if possible. 果然用最新 master 分支编译安装就 轻松秒杀。。（crash 工具编译安装很省心，没有过多依赖） 1234567891011121314151617181920212223242526stable_kernel@kernel: /var/crash/202101192317# sudo crash dump.202101192317 vmlinuxThis GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [702MB]: patching 130299 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101192317 [PARTIAL DUMP] CPUS: 4 DATE: Tue Jan 19 23:17:07 CST 2021 UPTIME: 00:03:31LOAD AVERAGE: 0.06, 0.06, 0.01 TASKS: 476 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #27 SMP Tue Jan 19 13:36:03 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Kernel panic - not syncing: sysrq triggered crash&quot; PID: 3655 COMMAND: &quot;bash&quot; TASK: ffff9aa984598d80 [THREAD_INFO: ffff9aa984598d80] CPU: 1 STATE: TASK_RUNNING (PANIC)crash&gt; exit 这里就不记录具体使用了，在后面具体案例再记录","link":"/2021/01/20/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/kdump+crash%E5%AE%9A%E4%BD%8D%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98/"},{"title":"IDE选择","text":"Vim 还是 Vscode 还是 Source Insight相信很多人一开始都是使用的是SI，还是比较方便的，也比较轻量，运行时占用内存也比较少，一般也能找到破解的方式。我再到公司之后发现我们公司都是使用linux服务器进行开发，使用SI不太方便，但还有方法：比如使用samba服务器，SI通过samba访问服务器代码，也可以建立完整工程但这样有两个致命缺点 121. samba 方式修改代码之后，文件被修改成为了可执行的了，每次提交之前都需要手动修改，或者写个脚本在每次提交之前执行2. samba需要在windows开发机 和 linux服务器之间大量同步，有时候同步很慢 基于上面两个原因，我后面开始使用Vim来进行日常开发。 Vim 使用过的人都知道学习曲线比较陡峭，有个著名的问题：我该怎么退出vim?在使用一段时间Vim之后，插件也越装越多，在远程的时候还是会卡顿，且Vim用起来也不是很方便 12341. Vim 查找替换十分繁琐2. Vim 对于散落在不同文件夹的文件，建立工程十分不友好，也可以做3. 在重构代码的时候，Vim 就不是一个很好的选择，需要经常查找定义等...... 过了一段时间之后，我再也不能忍受Vim了，然后看到了vscode，正巧推出了Remote-ssh插件，插件也都可以离线安装，所以我就切换到了 Vscode。 从我实际使用经验来看，Vscode 除了内存占用大一点之外几乎没有其他缺点，但是优点可以列一堆 12345671. 查找替换十分方便，相比于Vim SI，不要太方便2. 工作区简历十分方便3. Pretty!!4. 各种插件支持，新的语言，只要装一个插件就可以很好支持，且这些插件是社区在维护，基本一段时间就有新的版本（BugFix + New Feature）5. Remote-ssh: 运算，查找这些操作都是在 服务器进行，不会导致本地卡顿，但是对网络要求比较高6. 跟随社区，一直有新版本，新功能可以体验........ So, Vscode Yes!","link":"/2020/09/02/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/vim%20vscode%20si/"},{"title":"vscode 阅读内核源码","text":"相信大多数人使用vscode阅读内核源码跟我一开始的做法是一样的： 121. 下载vscode,安装Remote-ssh,C/C++解析等工具2. 建立 kernel workspace,添加kernel代码 其实这样从我实际的体验来说都要比 SI,Vim 要方便一点了，但是我们还是会遇到 找 12341. 不到清晰的定义，类似于多个slab slub slob的代码，通常都是乱跳转2. 硬件体系结构相关代码和kernel 通用代码之间跳转的时候乱跳转3. kernel 代码中有很多宏定义，不知道实际到底定义了哪些宏，如果看代码的时候需要对比.config文件，那效率会大大降低4. 看一个代码文件，也不知道当前工作会不会用到，可能看到代码，在你工作的环境根本就不会编译 你是不是也有这样的困惑呢？我们接下来一条一条的解决上面这几个问题，让你在看内核代码时畅通无阻 12341. 第一点和第二点可以通过在 settings.json中 设置 search.exclude files.exclude来避免, search.exclude 可以避免你搜索时搜索到他，files.exclude可以直接让你在文件列表视图上看不到他2. 主要原因是 Vscode 默认认为宏是未定义的。可以写一个简单脚本来处理 .config 文件，将其中 =y =m的 CONFIG 项目提取出来，最后在settings.json中 设置 DEFINE 的项，来避免这个问题3. 这个就需要使用 gcc的特性 和 cpp解析的插件了，还需要新版本内核提供的一个脚本 解析config文件脚本下面我重点来讲如何利用 内核编译产生的中间文件，来解析建立你的工程 首先需要下面这个脚本，超过4.19的版本内核应该有这个文件 12sh@ubuntu:~/workspace/linux$ find ./scripts/ -name gen_compile*./scripts/gen_compile_commands.py 先编译bzImage 123mkdir outcp .config ./out/.configmake -j4 bzImage O=./out 生成compile_commands.json 1234# 4.19 - 5.8 之前可以用./scripts/gen_compile_commands.py -d ./out/# 5.9 之后可以用./scripts/clang-tools/gen_compile_commands.py -d ./out/ 配置 .vscode/c_cpp_properties.json 12345678910{# 4.19 - 5.8 之前可以用 ..... &quot;compileCommands&quot;: &quot;${WorkspaceFolder}/out/compile_commands.json&quot; .....# 5.9 之后可以用 ..... &quot;compileCommands&quot;: &quot;${WorkspaceFolder}/compile_commands.json&quot; .....} 到此为止，基本上在浏览内核代码上不会有困难了 贴一下 .vscode/c_cpp_properties.json 配置 1234567891011121314151617181920212223242526{ &quot;env&quot;: { &quot;myDefaultIncludePath&quot;: [&quot;${workspaceFolder}&quot;, &quot;${workspaceFolder}/include&quot;], &quot;myCompilerPath&quot;: &quot;/usr/local/bin/gcc-9&quot; }, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;linux-kernel&quot;, &quot;intelliSenseMode&quot;: &quot;gcc-x64&quot;, &quot;includePath&quot;: [&quot;${myDefaultIncludePath}&quot;], &quot;defines&quot;: [ &quot;__KERNEL__&quot;, &quot;BAR=100&quot;], &quot;compilerPath&quot;: &quot;/usr/bin/clang&quot;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++17&quot;, &quot;compileCommands&quot;: &quot;${workspaceFolder}/out/compile_commands.json&quot;, &quot;browse&quot;: { &quot;path&quot;: [&quot;${workspaceFolder}&quot;], &quot;limitSymbolsToIncludedHeaders&quot;: true, &quot;databaseFilename&quot;: &quot;&quot; } } ], &quot;version&quot;: 4} 注意”intelliSenseMode”: “gcc-x64” 不同平台配置不一样，我是代码放在ubuntu20上，所以是gcc-x64","link":"/2020/09/03/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/vscode%20%E9%98%85%E8%AF%BB%E5%86%85%E6%A0%B8%E6%BA%90%E4%BB%A3%E7%A0%81/"},{"title":"那些经常忘记的shell命令","text":"总有那么几条经常使用，但有比较难记住的命令，做个记录。 ln 创建连接 ln 创建 软链接 12ln -s 实际文件/目录 软链接文件/目录ln -s /usr/sbin/trace-bpfcc /usr/bin/trace ln 创建 硬链接 12ln 实际文件 硬链接文件 //目录无法使用硬链接ln /usr/sbin/trace-bpfcc /usr/bin/trace tar 解压 tar.gz 文件 // extrat – 提取 1tar -zxvf file.tar.gz 压缩 文件、文件夹 // create – 创建 1tar -zcvf file.tar.gz ./file zip zip 解压 aaa.zip文件 1unzip aaa.zip zip 压缩文件夹 1zip -r aaa.zip ./aaa zip 压缩多个文件 1zip -r aaa.zip ./aaa ./bbb scp 本地 &lt;–&gt; 远程 互传文件 从 ubuntu@xxx:222 获取 /home/ubuntu/.vscode-server/data/Machine/settings.json 文件，且保存为 /tmp/settings.json 1scp -P 222 ubuntu@xxx:/home/ubuntu/.vscode-server/data/Machine/settings.json /tmp/settings.json 将本地的 elfutils-0.144.tar.bz2 文件传输到 ubuntu@xxxx:/home/ubuntu/ 目录下 1scp -P 222 ./elfutils-0.144.tar.bz2 ubuntu@xxxx:/home/ubuntu/ 可以参考scp 命令介绍 ubuntu 截图工具 安装 1sudo apt-get install flameshot 使用 1flameshot gui 123Inspiron-5548@ubuntu: ~/workspace/linux-stable# cat ~/.zshrc| tail -n 1alias shot=&quot;flameshot gui&quot;Inspiron-5548@ubuntu: ~/workspace/linux-stable# shot 编译linux kernel 前准备工作 1sudo apt install git fakeroot build-essential ncurses-dev xz-utils libssl-dev bc flex libelf-dev bison clang dd 快速生成大文件12345Inspiron-5548@130ubuntu: ~# dd if=/dev/zero of=haha bs=1M count=100记录了100+0 的读入记录了100+0 的写出104857600 bytes (105 MB, 100 MiB) copied, 0.294862 s, 356 MB/sInspiron-5548@ubuntu: ~# rsync为啥我需要 rsync来同步文件目录呢？ 我hexo_blog是部署在 tencent上的，其实本地公司、家里个人电脑也部署了一份，github gitee上也备份了。但是如果依赖手动去同步： 在哪个 linux机器上修改了之后及时发布到github,且及时备份到gitee 到新的机器上修改之前，需要先 git pull origin master 将本地，或者服务器上 blog更新到最新 开始修改，修改完成 –&gt; 1 然后我发现我这个场景 rsync 并不能很好解决。。如果是 一个服务器 和 备份服务器之间 数据同步 可以使用这样的方式。。突然想到，在新安装一台 linux主机 虚拟机 的时候，可以使用 rsync 来同步，减少 home 目录下文件搞来稿去的 事情。。 rsync 命令 samba 相关 1net Z: 192.168.1.103??? wine 使用 工作上可能需要使用通讯会议这样的软件，或者是临时使用source insight，但这些又都没有linux版本，要么使用wine来模拟windows api实现，要么使用windows虚拟机。 这里使用wine:安装 wine 12sudo apt install winesudo apt install winetricks 安装 软件 1sudo apt install winetricks 进去选择资源管理器，初始化一下~/.wine 目录，然后直接安装 使用 软件 1234Inspiron-5548@ubuntu: ~/workspace# tail ~/.zshrc -n 2alias shot=&quot;flameshot gui&quot;alias wemeet=&quot;wine /home/ubuntu/.wine/drive_c/Program\\ Files\\ \\(x86\\)/Tencent/WeMeet/wemeetapp.exe&quot;Inspiron-5548@ubuntu: ~/workspace# wemeet 查询常见函数 apropos 1234567891011121314Inspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# apropos pidacpid (8) - Advanced Configuration and Power Interface event daemonbiosnoop-bpfcc (8) - Trace block device I/O and print details incl. issuing PID.getpid (2) - get process identificationgetppid (2) - get process identificationgit (1) - the stupid content trackerpid_namespaces (7) - overview of Linux PID namespacespidfd_open (2) - obtain a file descriptor that refers to a processpidfd_send_signal (2) - send a signal to a process specified by a file descriptorpidof (8) - find the process ID of a running program.pidpersec-bpfcc (8) - Count new processes (via fork()). Uses Linux eBPF/bcc.pidpersec.bt (8) - Count new processes (via fork()). Uses bpftrace/eBPF.waitpid (2) - wait for process to change stateInspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# 查找常见软件包sudo apt-cache search 123456Inspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# sudo apt-cache search qemu | grep armqemu-efi-arm - UEFI firmware for 32-bit ARM virtual machinesqemu-system-arm - QEMU full system emulation binaries (arm)Inspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# sudo apt-cache search qemu | grep aarch64qemu-efi-aarch64 - UEFI firmware for 64-bit ARM virtual machinesInspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# make 相关指定 gcc version，和 忽略 warning. 1make CC=gcc-7 CFLAGS=&quot;-Wno-error&quot; sed 命令sed grep awk 被称为linux三剑客，足以说明其强大的字符文本处理能力。a. 删除 ‘str1 str2’ 的特定行(这个在处理 kernel config文件时候很有用) 12345ubuntu@~: $ cat .config | grep &quot;CONFIG_INLINE_SPIN_UNLOCK_IRQ=y&quot;CONFIG_INLINE_SPIN_UNLOCK_IRQ=yubuntu@~: $ubuntu@~: $ sed -i '/CONFIG_INLINE_SPIN_UNLOCK_IRQ=y/d' .configubuntu@~: $ cat .config | grep &quot;CONFIG_INLINE_SPIN_UNLOCK_IRQ=y&quot; b. l grep 命令a. 最常见的1234tencent_clould@1ubuntu: ~/workspace/hexo_blog# grep &quot;asdfasdf&quot; . -nr./source/file.md:224:asdfasdf_./source/file.md:239:asdfasdftencent_clould@ubuntu: ~/workspace/hexo_blog# b. -w 全词匹配 123tencent_clould@ubuntu: ~/workspace/hexo_blog# grep &quot;asdfasdf&quot; . -nr -w./source/file.md:239:asdfasdftencent_clould@ubuntu: ~/workspace/hexo_blog# c. -A 3 -B 3 -C 3显示前后几行：加上查找到的后三行 加上查找前三行 加上查找的前后三行 awk 命令1 inlinelinux 代码中会经常使用到 inline的技巧，使得少一次函数调用，代码原地展开，但是有时候 gcc 编译器会 ‘智能’的给我们 inline一些我们不想 inline的函数，所以有什么部分可以让编译器知道我们不想 inline 某些函数呢？ 添加 noinline 关键字 1234static void noinline create_oops(void){ *(int *)0 = 0;} mount重新挂载某一个目录，并改变大小 1mount -o 128M -o remount /tmp ubuntu shell 切换 wifi切换wifi12sudo nmcli connection up Susudo nmcli connection up ChinaNet-5mrv 连接wifi 1sudo nmcli dev wifi connect ChinaNet-5mrv password ejgaefpq ubuntu cmd 测试网络速度 123456789101112amd_server@130ubuntu: ~/workspace# sudo apt install speedtest-cliamd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace# speedtest-cliRetrieving speedtest.net configuration...Testing from China Telecom Shanghai (101.228.28.85)...Retrieving speedtest.net server list...Selecting best server based on ping...Hosted by China Telecom (Shanghai) [21.59 km]: 8.567 msTesting download speed................................................................................Download: 19.08 Mbit/sTesting upload speed......................................................................................................Upload: 22.78 Mbit/s vmbox 虚拟机？","link":"/2020/09/12/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/%E7%BB%8F%E5%B8%B8%E5%BF%98%E8%AE%B0%E7%9A%84shell%E5%91%BD%E4%BB%A4/"},{"title":"欢迎来到liulangren的读核之旅","text":"不忘初心我是博主 liulangren, 从2019年毕业开始，到目前一直在一家无人机(DJI)公司 从事linux 内核性能、稳定性相关的工作。 建立这个博客的初心是想记录在工作中的一些发现，读代码的一些心得。从毕业开始到现在一年多时间不长也不短，看了很多代码，也解决了很多系统中的问题，但是一直没有系统的去记录一下，导致现在回想起来一时竟然不知道如何总结描述过去这一年多得工作。linux 正所谓好记性不如烂笔头，之后在工作之余，一定多多记录总结。我的博客应该会包含以下几个部分 1231. linux 内核某些子系统的阅读分享2. linux 内核性能稳定性、性能调优实践案列分享3. 个人年度总结、月度总结等 回首过去回想大学生活，还是很精彩的。 大一，懵懵懂懂，充满了对大学生活的期待，从军训到刚入班级时的自我介绍，一切都是那么美好。上学期和大多数人一样，参加社团，努力学习公共课，泡图书馆，成绩还不错拿到了一等奖学金。大一下学期开始，开始思考之后的职业路线，慢慢接触到学院的一些实验室，也认识了很多厉害的学长。也是从大一下学期，在学校的某个神奇的实验室，参加了第一个校电子设计大赛，当时好像是做的一个888的光立方，当然最后也没拿到奖，不过融入了实验室的大佬，后面跟着他们开始做大学生电子设计大赛。 还记得那个炎热的暑假，大家暑假之后陆陆续续离校了，我和同队的小伙伴还继续坚守在实验室的一线。也是在这个暑假，苏北的我第一次在宿舍见到拇指大的蟑螂，还记得因为蟑螂在实验室睡了好几天…。 白天大家在实验室画板子，晚上熬夜写代码，那时候都是玩MCU,代码也是操作寄存器加上一些 if else，虽然很简单，但是配合几个电机传感器，确实让我着迷的很；有时候干活干累了，几个好友聚在一起吹一吹牛皮，探讨一下今年可能的赛题，几个人再去学校对面的香江不夜街点几个菜，好好犒劳一下自己。工作之后想起来，尤其怀念那种无忧无虑，没有KPI压力，有大把时间做自己喜欢的事情的生活。 这样从暑假开始到八月中，几乎每天都是早上十点多到实验室，晚上十一点回去，最后也取得了不错的成绩，在当时看最重要的是学会了 AD，STM32等一系列MCU、传感器的使用，可以自己单独干一些小项目了，实际上后来我也靠着画板子，写MCU裸机代码赚了一些钱，后面再说。 大二，说起大二我都有些模糊了，因为我的大二和大三是十分相似的：逃课，泡实验室，跟老师做项目，参加比赛。先说说逃课吧，其实从大一下学期接触到实验室开始，我就开始经常逃课，不过到了大二大三逃课更多，可能一周上个两节课这种吧，实验室的几个老师都知道我逃课，也经常说我不要逃课，但是并不会阻止我去实验室。其次，泡实验室，那时候H老师从一些当地企业接到了不少的外包项目，H老师自己只做硬件，然后MCU的简单代码 都是让我搞定的，在大二大三这两年应该给他做了四五个项目，H老师也很厚道，每个月按本科标准都会给一点劳务费，一直给到我大四毕业。期间我也自己从外面接到了一些项目，自己画原理图，画PCB，做板子，焊接，debug代码，简直一条龙服务啊，也赚到了一点钱补贴生活费。从大二下学期我就开始搬出去住了，主要是晚上经常11点会宿舍，怕打扰宿管阿姨。大二暑假和大一暑假并没有什么不同，也是留校画板子，写代码，不过开始从打酱油的角色变到了主力队员，比赛过程不表，参加过电赛的都知道很辛苦，结果当然也还可以。 大三，基本和大二类似，业余时间接触了linux，从安装虚拟机，使用cat ls等命令做起，C语言也重新学习了一遍，画板子，debug裸机代码也比较熟练了。大三上学期还参加了一个农业机械的比赛，是去的重庆的西南大学，在高铁上 和 路上学会了王者荣耀（挺后悔的，到现在王者荣耀应该打了2500场以上了，浪费了比较多的时间），也是第一次去西部，在去往重庆的高铁上一路山见识了祖国的大好河山，着实震撼（我自己出生在江苏，大学也在江苏，没咋见过大山大河）。大三下学期开始面临读研和工作的选择，由于我自己呆实验室时间比较长，对学术研究不太感兴趣，当时感觉也可以找到一个不错的工作，就没有准备考研（PS：由于大二大三基本没咋上课，早就没有保研名额了，我们学校电赛一等奖也不能保研…）。从春天开始着手准备，自己在实验室干了两年MCU裸机相关的事情，感觉技术壁垒不高（当时原子、野火、安富莱很火），可替代性太强。然后学习了UCOSII的代码，对RTOS有了一定了解，当时裸机开发实习那种就没考虑，其中过程应该比较艰辛的，忘得差不多了。一开始一个学长S给我内推了百度的一份实习，一直没面试，然后遇到著名的ODM企业（华勤）招实习生就去了，基本还是比较简单的，很快就拿到了实习offer（主要考察C语言）。又过了很久百度开始面试，和面试官说打算写一个RTOS，然后面试官问我栈相关问题，没回答上来，被无情嘲讽了一下，但是最后拿到了百度的 offer，华勤就被备胎了。。。当时拿到offer有点飘，有一门课就没复习，想着补考再说了（一点课都没去过。。）后面再说。大三暑假就去上海百度实习了，过程比较辛苦，还记得实习工资150/天，当时对 RTOS也还不太了解，去了之后发现是用乐鑫的ESP32跑RTOS做智能小度音响，不知道为啥一直没给我分配活。我就开始学习RTOS，每天看代码，看博客，写博客（之前是在CSDN上）准备秋招，学习了一个学长自己写的 一个RTOS–SMC_RTOS，真的是麻雀虽小，五脏俱全，学习了RTOS 调度原理，mutex、simapore 实现原理，对比UCOSII、UCOSIII、FREERTOS也看了看，写了一些博客，后面在秋招的帮助很大。期间我也还参加了电赛，当然只有比赛那几天回去了，也是辛苦，但是收获满满，其中暑假还给老师做了一个项目，每个周五从公司出发去虹桥火车站到学校，周日再从学校出发到上海，那个暑假坐高铁花了不少钱….期间虽然没给我安排啥活，但是从正好碰到了部门年中大会，居然在长白山举行，人生第一次坐飞机，第一次去东北，第一次泡温泉等等….现在回想起来，互联网公司真的有钱，之后还是想到互联网公司去发展（DJI年会就在附近一个宾馆随便举行了，也没有年中大会这种）很多公司秋招在大三暑假前就开始了，DJI我记得还没放暑假就开始了，但是真正面试却到了八月，当时正看SMC_RTOS、FREERTOS、UCOSII，和面试官吹了一波，就过了；暑假过程中也面了一些其他公司，华勤（自然被备胎了），CVTE(挂)，OVHM(简历没过)，这时候才意识到学校不是211带来的伤害，也比较晚了。到九月份拿到了DJI的正式电话、邮件 offer，开出得到薪资很高（至少在当时的我看来），然后十月份又面了海康拿到了offer没去。学校一发三方我就和DJI签了… 大四，和前两年相比轻松了些许，主要是之后工作已经确定，但是奇葩的专业大四还有很多课，比大三还多，大四下也补考了大三由于逃课落下的课。到了寒假，我就立刻去DJi实习了，工作也都是linux下的相关开发，去了之后发现 现实 和 想象中的 DJI还是有很大不一样的，没有外界的那种无人机的高大上，只有每天10115低头打代码，更可恶的是公司严格的保密制度，导致研发不能上外网，遇到问题基本手机百度…实习快结束的时候，去深圳出差了两周，第一次出差，算是第二次坐飞机，五星级酒店，体验还不错。。。 步入职场毕业之后回家呆了两周，然后就去公司正式入职了。 DJI 确实会给年轻人很多机会，刚正式入职就直接跟项目（也说明没有成熟的培训体系），一个team几个人开始一个模块重构，重构过程中，写了很多代码（BUG），对个人能力也得到了一些锻炼。。。也会经常出差… 重新出发后面会 尽力业余时间更新本博客","link":"/2020/09/01/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/%E4%B8%8D%E5%BF%98%E5%88%9D%E5%BF%83/"},{"title":"我应该如何看代码","text":"每次看一个模块源码都比较痛苦，在 kernel 的代码海洋里面像一只无头苍蝇一样，看来半天却不知所云，记录一下之后在看某个模块源码时 应该注意什么，比较快的熟悉，警醒自己！ 庞大的linux内核现在是 2021-01-11号，上个月linus发布了 Linux-5.10 LTS版本，往往每年的最后一个 kernel 版本就是作为LTS的版本，今年是12月13号。 linus 会在每个大版本发布之后都会 开启 一个merge window的时间，这个时间大家都可以尽量的合入之前早就开发好的已经在next tree的 patch，大概有一两周？今年这时候恰好是圣诞节，老外是基本周末都不会工作的，更何况是 圣诞+元旦 这种节日了，看了基本 2020-12-23 – 2021-01-04 都是没有daily next tree 版本更新的。 A total of 1,971 developers contributed to 5.10 — again, just short of the record set by 5.8. Of those developers, 252 (just under 13%) made their first contribution in 5.10; that is the lowest number seen since 5.6. The most active 5.10 developers were: Linux 5.10代码统计 新的模块应该如何涉及首先要高明白他是干什么的 结合注释去看，谷歌翻译，vscode在线、离线插件都很好用 结合 git log 看： git log -S’xxx’ file_name 根据Makefile看：如果这个目录下Makefile 都没直接包含这个文件.o需要使能之后才会编译到内核中的话，结合是否工作实际需要再看 谷歌、百度：中文 英文都可以作为关键次搜索 LWN 找到Archives，看看有没有介绍的文章 看看 Documents 目录下有没有介绍的文档 看这个模块 关键数据结构 和关键数据结构数据成员 如 struct_task struct_mmaddress_space等 新的子系统 知道模块子系统作用 找个方法实践一下，比如 docker 可以很好实践 cgroups 关键数据结构 导出到用户空间的接口，看他在 kernel中实现比如123456789101112tencent_clould@127ubuntu: /proc/sys/fs# lsaio-max-nr dir-notify-enable inode-nr leases-enable overflowgid pipe-user-pages-soft protected_symlinksaio-nr epoll inode-state mount-max overflowuid protected_fifos quotabinfmt_misc file-max inotify mqueue pipe-max-size protected_hardlinks suid_dumpabledentry-state file-nr lease-break-time nr_open pipe-user-pages-hard protected_regular veritytencent_clould@ubuntu: /proc/sys/fs#tencent_clould@ubuntu: /proc/sys/fs#tencent_clould@ubuntu: /proc/sys/fs# cat file-nr3200 0 9223372036854775807tencent_clould@ubuntu: /proc/sys/fs# cat file-max9223372036854775807tencent_clould@ubuntu: /proc/sys/fs# 12345678910111213141516171819202122static struct ctl_table fs_table[] = { { .procname = &quot;inode-nr&quot;, .data = &amp;inodes_stat, .maxlen = 2*sizeof(long), .mode = 0444, .proc_handler = proc_nr_inodes, }, { .procname = &quot;inode-state&quot;, .data = &amp;inodes_stat, .maxlen = 7*sizeof(long), .mode = 0444, .proc_handler = proc_nr_inodes, }, { .procname = &quot;file-nr&quot;, .data = &amp;files_stat, .maxlen = sizeof(files_stat), .mode = 0444, .proc_handler = proc_nr_files, }, 如何深入比如我知道 page_cache 的大概了，也知道他的作用，但是看代码的时候比较迷茫？ 看看 page_cache 的API 12345678amd_server@ubuntu: ~/workspace/linux-stable/mm# grep &quot;EXPORT_SYMBOL&quot; ./filemap.c -nr279:EXPORT_SYMBOL(delete_from_page_cache);377:EXPORT_SYMBOL(filemap_check_errors);437:EXPORT_SYMBOL(filemap_fdatawrite);444:EXPORT_SYMBOL(filemap_fdatawrite_range);459:EXPORT_SYMBOL(filemap_flush);502:EXPORT_SYMBOL(filemap_range_has_page);557:EXPORT_SYMBOL(filemap_fdatawait_range); 在.c 文件中搜索trace_ 看看，找到他的静态追踪点 tracepoints 1234567amd_server@130ubuntu: ~/workspace/linux-stable/mm# grep &quot;trace_&quot; ./filemap.c -nr236: trace_mm_filemap_delete_from_page_cache(page);354: trace_mm_filemap_delete_from_page_cache(pvec-&gt;pages[i]);683: trace_filemap_set_wb_err(mapping, eseq);724: trace_file_check_and_advance_wb_err(file, old);902: trace_mm_filemap_add_to_page_cache(page);amd_server@ubuntu: ~/workspace/linux-stable/mm# 直接使用 bpftrace -l 这个根据搜索你的模块，看看有那些比较重要的函数，这些重要函数往往是掌握这个模块的关键，也是经常出现问题需要debug 或者观测的地方，所以 kernel 才会为他设置一个静态追踪点 1234amd_server@ubuntu: ~/workspace/linux-stable/mm# sudo bpftrace -l | grep tracepoint: | grep page_cachetracepoint:filemap:mm_filemap_delete_from_page_cachetracepoint:filemap:mm_filemap_add_to_page_cacheamd_server@ubuntu: ~/workspace/linux-stable/mm# stat 统计数据入手类似于 slub 这种内存分配器，内核提供了很多统计数据，可以利用这些事件统计 来跟踪关键代码路径 如 12345678910111213enum stat_item { ALLOC_FASTPATH, /* Allocation from cpu slab */ ALLOC_SLOWPATH, /* Allocation by getting a new cpu slab */ FREE_FASTPATH, /* Free to cpu slab */ FREE_SLOWPATH, /* Freeing not to cpu slab */amd_server@ubuntu: ~/workspace/linux-stable/mm# grep ALLOC_FASTPATH . -nr./slub.c:2883: stat(s, ALLOC_FASTPATH);./slub.c:5390:STAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);amd_server@ubuntu: ~/workspace/linux-stable/mm# grep ALLOC_SLOWPATH . -nr./slub.c:2666: stat(s, ALLOC_SLOWPATH);./slub.c:5391:STAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);amd_server@ubuntu: ~/workspace/linux-stable/mm# struct_task 的 context switch 事件计数 12345678910111213struct task_struct {...... unsigned long nvcsw; unsigned long nivcsw;......}amd_server@ubuntu: ~/workspace/linux-stable/kernel/sched# grep &quot;nivcsw&quot; . -nr./debug.c:497: (long long)(p-&gt;nvcsw + p-&gt;nivcsw),./debug.c:934: nr_switches = p-&gt;nvcsw + p-&gt;nivcsw;./debug.c:989: __PS(&quot;nr_involuntary_switches&quot;, p-&gt;nivcsw);./core.c:4988: switch_count = &amp;prev-&gt;nivcsw;amd_server@ubuntu: ~/workspace/linux-stable/kernel/sched# 看到静态追踪点 或者stat统计数据 之后，看看附近的函数，更上层函数等","link":"/2021/01/11/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/%E6%88%91%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E7%9C%8B%E4%BB%A3%E7%A0%81/"},{"title":"记录第一个kernel patch","text":"之前也提过两个patch给社区不过都被否了，很多大佬第一次提patch也是从改改标点开始的。最近在看文件系统相关的代码，看到有个地方定义和注释不一致，可以给社区贡献一下，也顺便记录一下，搭建环境的过程（之前用的ubuntu18虚拟机被我删了…现在用ubuntu20重新搞一下） 修改代码，形成patch其实我这个patch很简单，只是修改一个注释而已： 1234567891011121314151617sh@ubuntu:~/workspace/linux$ git diffdiff --git a/include/linux/jbd2.h b/include/linux/jbd2.hindex 08f904943ab2..a1ef05412acf 100644--- a/include/linux/jbd2.h+++ b/include/linux/jbd2.h@@ -452,8 +452,8 @@ struct jbd2_inode { struct jbd2_revoke_table_s; /**- * struct handle_s - The handle_s type is the concrete type associated with- * handle_t.+ * struct jbd2_journal_handle - The jbd2_journal_handle type is the concrete+ * type associated with handle_t. * @h_transaction: Which compound transaction is this update a part of? * @h_journal: Which journal handle belongs to - used iff h_reserved set. * @h_rsv_handle: Handle reserved for finishing the logical operation.sh@ubuntu:~/workspace/linux$ 12345678910111213141516sh@ubuntu:~/workspace/linux$ git log -1commit f8db1794205285c44d4b0b7d83f0fda1b9adec00 (HEAD -&gt; master)Author: Hui Su &lt;sh_def@163.com&gt;Date: Tue Sep 22 23:28:05 2020 +0800 FIX the comment of struct jbd2_journal_handle the struct name was modified long ago, but the comment still use struct handle_s. Signed-off-by: Hui Su &lt;sh_def@163.com&gt;sh@ubuntu:~/workspace/linux$sh@ubuntu:~/workspace/linux$sh@ubuntu:~/workspace/linux$ git format-patch HEAD^0001-FIX-the-comment-of-struct-jbd2_journal_handle.patchsh@ubuntu:~/workspace/linux$ 如果第一个版本有问题则需要重新提第二个版本patch，与第一个版本差别就是加上 ‘ -v2’ ‘ -v3’ 等， 12sh@ubuntu:~/workspace/linux-stable$ git format-patch HEAD^ -v2v2-0001-tools-time-access-sys-kernel-debug-udelay_test-be.patch 在邮件发出之后会自动变成[PATCH v2] tools/time: access /sys/kernel/debug/udelay_test before test 如果不是一个patch，而是一系列patch怎么办呢？ 123肯定是有办法的，目前我还没这个需求，留白，后续补充lkml中的邮件格式：[PATCH v6 00/25] Add support for Clang LTO git format-patch 找到修改代码部分的maintainer 先用 perl scripts/checkpatch.pl 检查patch有无语法错误 再用 perl scripts/get_maintainer.pl 获取patch修改的代码的 maintainer1234567891011sh@ubuntu:~/workspace/linux$ perl scripts/checkpatch.pl 0001-FIX-the-comment-of-struct-jbd2_journal_handle.patch total: 0 errors, 0 warnings, 10 lines checked0001-FIX-the-comment-of-struct-jbd2_journal_handle.patch has no obvious style problems and is ready for submission.sh@ubuntu:~/workspace/linux$ sh@ubuntu:~/workspace/linux$ perl scripts/get_maintainer.pl 0001-FIX-the-comment-of-struct-jbd2_journal_handle.patch &quot;Theodore Ts'o&quot; &lt;tytso@mit.edu&gt; (maintainer:JOURNALLING LAYER FOR BLOCK DEVICES (JBD2))Jan Kara &lt;jack@suse.com&gt; (maintainer:JOURNALLING LAYER FOR BLOCK DEVICES (JBD2))linux-ext4@vger.kernel.org (open list:JOURNALLING LAYER FOR BLOCK DEVICES (JBD2))linux-kernel@vger.kernel.org (open list)sh@ubuntu:~/workspace/linux$ 安装配置邮件客户端安装下面工具 123sudo apt-get install sendmailsudo apt install muttsudo apt install smtp 配置一下 1234567891011121314151617181920sh@ubuntu:~$ cat .muttrc set sendmail=&quot;/usr/bin/msmtp&quot;set envelope_from=yesset realname=&quot;Hui Su&quot;set from=sh_def@163.comset use_from=yesset envelope_from=yessh@ubuntu:~$ sh@ubuntu:~$ sh@ubuntu:~$ sh@ubuntu:~$ cat .msmtprc account defaulthost smtp.163.comport 25from sh_def@163.comauth plaintls offuser sh_def@163.compassword #这里填授权码sh@ubuntu:~$ 测试一下 12sh@ubuntu:~$ echo &quot;test&quot; |mutt -s &quot;my_first_test&quot; xxx@163.comsh@ubuntu:~$ echo &quot;test&quot; |mutt -s &quot;my_first_test&quot; xxxx@qq.com 吐槽一下，如果是第一次配置还是比较难搞的参考参考 git 配置这个应该在 git commit 阶段就已经配置好了，不赘述 推送patch只需要在各个邮箱之间加上,即可，不赘述 回复邮件一般情况下社区大佬都会在几天到一周内回复你的patch邮件，如果你的patch需要修改，然后你需要重新回复他们的邮件，这应该如何做呢 第一步配置好fetchmail 1234567891011121314151617181920212223sh@ubuntu:~$ sudo apt install fetchmail [sudo] password for rlk: Reading package lists... DoneBuilding dependency tree Reading state information... Donefetchmail is already the newest version (6.4.2-2).0 upgraded, 0 newly installed, 0 to remove and 369 not upgraded.sh@ubuntu:~$ cat ~/.fetchmailrc poll pop3.163.comprotocol POP3user &quot;xxx@163.com&quot; password &quot;客户端授权码&quot;sh@ubuntu:~$ ll | grep rc | grep fetch-rwx------ 1 rlk rlk 83 10月 8 01:38 .fetchmailrc*# 接收邮件sh@ubuntu:~$ fetchmail -vfetchmail: 6.4.2 querying pop3.163.com (protocol POP3) at 2020年10月08日 星期四 01时38分51秒: poll startedTrying to connect to 220.181.12.110/110...connected.fetchmail: POP3&lt; +OK Welcome to coremail Mail Pop3 Server (163coms[10774b260cc7a37d26d71b52404dcf5cs])fetchmail: POP3&gt; CAPAfetchmail: POP3&lt; +OK Capability list follows............... 查看接收的邮件然后直接 mutt 就可以看到接收到的邮件了 123456789101112131415161718192021222324252627282930q:Quit d:Del u:Undel s:Save m:Mail r:Reply g:Group ?:Help 1 O + 9月 09 iovisor-dev@lis ( 378) [iovisor-dev] iovisor-dev@lists.iovisor.org Daily Summary 2 + 9月 22 Mail Delivery S ( 50) Returned mail: see transcript for details 3 + 9月 22 Mail Delivery S ( 50) Returned mail: see transcript for details 4 O F 9月 23 To sh_def@163.c ( 75) [PATCH] mm,page_alloc: fix the count of free pages 5 O T 9月 23 Jan Kara ( 38) ┬─&gt;Re: [PATCH] FIX the comment of struct jbd2_journal_handle 6 O T 10月 03 Theodore Y. Ts' ( 11) └─&gt;Re: [PATCH] FIX the comment of struct jbd2_journal_handle 7 O + 9月 25 Josh Poimboeuf ( 3) ┬─&gt;Re: [PATCH] mm,kmemleak-test.c: move kmemleak-test.c to samples dir 8 O T 10月 05 Catalin Marinas ( 12) └─&gt;Re: [PATCH] mm,kmemleak-test.c: move kmemleak-test.c to samples dir 9 O T 9月 25 akpm@linux-foun ( 313) + mmkmemleak-testc-move-kmemleak-testc-to-samples-dir.patch added to -mm tree 10 O T 9月 25 akpm@linux-foun ( 92) + mm-fix-some-comments-in-page_allocc-and-mempolicyc.patch added to -mm tree 11 O T 9月 28 Ira Weiny ( 57) Re: [PATCH] mm/vmalloc.c: check the addr first 12 O T 9月 28 akpm@linux-foun ( 56) + mm-vmscan-fix-comments-for-isolate_lru_page.patch added to -mm tree 13 O T 9月 28 akpm@linux-foun ( 59) + mm-vmallocc-update-the-comment-in-__vmalloc_area_node.patch added to -mm tree 14 O T 9月 28 akpm@linux-foun ( 66) + mm-vmallocc-fix-the-comment-of-find_vm_area.patch added to -mm tree 15 O T 9月 28 akpm@linux-foun ( 65) + mmz3fold-use-xx_zalloc-instead-xx_alloc-and-memset.patch added to -mm tree 16 O + 9月 29 haifei.wang@car ( 312) 阿里云/今日头条Linux内核研发专家职位推荐 17 O T 9月 29 Dietmar Eggeman ( 59) Re: [PATCH] sched,fair: use list_for_each_entry() in print_cfs_stats() 18 O + 9月 29 iovisor-dev@lis ( 378) [iovisor-dev] iovisor-dev@lists.iovisor.org Daily Summary 19 O T 9月 29 Steven Rostedt ( 61) Re: [PATCH] sched/rt.c remove unnecessary parameter in pick_next_rt_entity 20 O T 9月 30 boris.ostrovsky ( 13) Re: [PATCH] arch/x86: fix some typos in xen_pagetable_p2m_free() 21 O T 10月 01 David Hildenbra ( 57) Re: [PATCH] mm: fix some comments in page_alloc.c and mempolicy.c 22 O T 10月 01 Joe Perches ( 14) └─&gt; 23 O + 10月 01 iovisor-dev@lis ( 378) [iovisor-dev] iovisor-dev@lists.iovisor.org Daily Summary 24 O 10月 02 George Worden ( 1) 25 O T 10月 02 akpm@linux-foun ( 81) [to-be-updated] mm-fix-some-comments-in-page_allocc-and-mempolicyc.patch removed from -mm tree 26 O + 10月 08 流浪人 ( 75) Re:[PATCH] mm/hugetlb.c: just use put_page_testzero() instead of page_count()---Mutt: /var/mail/rlk [Msgs:26 Old:24 Post:8 174K]---(threads/date)---------------------------------------------------------------------------------------------------------------------------------(all)-- 回复邮件我的第一个patch","link":"/2020/09/22/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/%E8%AE%B0%E5%BD%95%E7%AC%AC%E4%B8%80%E4%B8%AAkernel%20patch/"},{"title":"valgrind 定位用户空间内存泄漏","text":"WHY在实际开发中，某个应用程序如果存在内存泄露，且是长时间运行的程序，就会导致比较严重的后果。在一些业务场景的长跑测试中，这些问题往往会充分暴露出来。 一般情况下这种内存泄漏都是以 OOM Kill 而结尾的。 kmsg log 往往只有 这个进程确实消耗了大量进程的证据，但是无法确切知道是哪里的内存泄漏，这里就需要一个工具来帮助检测，如果是发生内存泄漏之后仅仅通过人肉去分析代码，往往很困难。 valgrind 就是这样一款 强大的工具： 检查用户空间内存泄漏 检查 valdrind 原理valdrind 安装，使用直接安装， ubuntu 官方软件源已经包含了，其他平台可以通过源码编译安装 1234567891011121314151617tencent_clould@100ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# sudo apt install valgrindReading package lists... DoneBuilding dependency treeReading state information... DoneThe following packages were automatically installed and are no longer required: openbsd-inetd openjdk-11-jdk-headless tcpd update-inetdUse 'sudo apt autoremove' to remove them.Suggested packages: valgrind-dbg valgrind-mpi kcachegrind alleyoop valkyrieThe following NEW packages will be installed: valgrind0 upgraded, 1 newly installed, 0 to remove and 231 not upgraded.Need to get 20.3 MB of archives.After this operation, 90.0 MB of additional disk space will be used.Get:1 https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-updates/main amd64 valgrind amd64 1:3.15.0-1ubuntu9.1 [20.3 MB]48% [1 valgrind 12.2 MB/20.3 MB 60%]tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# ls 使用如下代码检测 123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;void main(void){ int i = 3; char *p = NULL; p = malloc(1024 * 1024); if (!p) printf(&quot;malloc failed,just wait!!\\n&quot;); printf(&quot;malloc sucess,just wait!!\\n&quot;); while(i--) { usleep(1000 * 1000); }} 对于 valgrind 不好的地方在于： 对于想使用 valgrind来检查内存泄漏的业务来说，必须从开始 用valgrind 启动，意味着对于业务需要重启。还有比如很难复现的内存泄漏，等你重启业务用 valgrind来启动，说不定又不复现了。。 对于 valgrind 启动的业务来说，会比直接启动有一些性能损失。 userspace memleak demo使用上面代码 直接用 valgrind 启动 12345678910111213141516171819202122tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# valgrind ./a.out==2813281== Memcheck, a memory error detector==2813281== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.==2813281== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==2813281== Command: ./a.out==2813281==malloc sucess,just wait!!==2813281====2813281== HEAP SUMMARY:==2813281== in use at exit: 1,048,576 bytes in 1 blocks==2813281== total heap usage: 2 allocs, 1 frees, 1,049,600 bytes allocated==2813281====2813281== LEAK SUMMARY:==2813281== definitely lost: 1,048,576 bytes in 1 blocks==2813281== indirectly lost: 0 bytes in 0 blocks==2813281== possibly lost: 0 bytes in 0 blocks==2813281== still reachable: 0 bytes in 0 blocks==2813281== suppressed: 0 bytes in 0 blocks==2813281== Rerun with --leak-check=full to see details of leaked memory==2813281====2813281== For lists of detected and suppressed errors, rerun with: -s==2813281== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) 可以看到 HEAP SUMMARY: 中写了 12==2813281== in use at exit: 1,048,576 bytes in 1 blocks==2813281== total heap usage: 2 allocs, 1 frees, 1,049,600 bytes allocated 在进程退出时，仍然有 1048576 bytes 内存在使用中，这部分就是泄漏的内存。但是我们仍然不能确定到底是哪里泄漏的内存，按照他的建议 加上 -leak-check=full 1234567891011121314151617181920212223242526tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# valgrind --leak-check=full ./a.out==2813434== Memcheck, a memory error detector==2813434== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.==2813434== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==2813434== Command: ./a.out==2813434==malloc sucess,just wait!!==2813434====2813434== HEAP SUMMARY:==2813434== in use at exit: 1,048,576 bytes in 1 blocks==2813434== total heap usage: 2 allocs, 1 frees, 1,049,600 bytes allocated==2813434====2813434= 1,048,576 bytes in 1 blocks are definitely lost in loss record 1 of 1==2813434== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2813434== by 0x1091AD: main (in /home/ubuntu/workspace/test_modules/resource_leak/user_space_memleak/a.out)==2813434====2813434== LEAK SUMMARY:==2813434== definitely lost: 1,048,576 bytes in 1 blocks==2813434== indirectly lost: 0 bytes in 0 blocks==2813434== possibly lost: 0 bytes in 0 blocks==2813434== still reachable: 0 bytes in 0 blocks==2813434== suppressed: 0 bytes in 0 blocks==2813434====2813434== For lists of detected and suppressed errors, rerun with: -s==2813434== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# 可以看到这次， valgrind 已经将泄漏的具体位置打印了出来 123==2813434= 1,048,576 bytes in 1 blocks are definitely lost in loss record 1 of 1==2813434== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2813434== by 0x1091AD: main (in /home/ubuntu/workspace/test_modules/resource_leak/user_space_memleak/a.out) 可以用 gcc -g 重新编译一下带上符号表信息，再次用 valgrind 定位，可以得到更详细信息。 12==2822695== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2822695== by 0x1091AD: main (user_space_memleak.c:9) 这次直接将 在 哪个文件 哪一行都直接打印出来了。 out of bounds access demo代码： 12345678910111213141516171819202122#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;void main(void){ int i = 3; char *p = NULL; p = malloc(1024); if (!p) printf(&quot;malloc failed,just wait!!\\n&quot;); printf(&quot;malloc sucess,just wait!!*(p + 10) = %d\\n&quot;, *(p + 10)); *(p + 1023) = *(p + 1024); *(p + 1024) = 1; while(i--) { usleep(1000 * 1000); } free(p); free(p + 1);} gcc -g 编译之偶 用 valgrind ./a.out 跑一下 123456789101112131415161718192021222324252627282930313233343536373839404142tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/valgrind# valgrind ./a.out==2826928== Memcheck, a memory error detector==2826928== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.==2826928== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==2826928== Command: ./a.out==2826928====2833797== Use of uninitialised value of size 8==2833797== at 0x48B681B: _itoa_word (_itoa.c:179)==2833797== by 0x48D26F4: __vfprintf_internal (vfprintf-internal.c:1687)==2833797== by 0x48BCEBE: printf (printf.c:33)==2833797== by 0x109225: main (out_of_bounds_access.c:13)malloc sucess,just wait!!==2826928== Invalid read of size 1==2826928== at 0x109200: main (out_of_bounds_access.c:14)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9)==2826928====2826928== Invalid write of size 1==2826928== at 0x109213: main (out_of_bounds_access.c:15)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9)==2826928====2845980== Invalid free() / delete / delete[] / realloc()==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10927F: main (out_of_bounds_access.c:21)==2845980== Address 0x4a4d041 is 1 bytes inside a block of size 1,024 free'd==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10926F: main (out_of_bounds_access.c:20)==2845980== Block was alloc'd at==2845980== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x1091ED: main (out_of_bounds_access.c:9)==2826928====2826928== HEAP SUMMARY:==2826928== in use at exit: 0 bytes in 0 blocks==2826928== total heap usage: 2 allocs, 3 frees, 2,048 bytes allocated==2826928====2826928== All heap blocks were freed -- no leaks are possible==2826928====2826928== For lists of detected and suppressed errors, rerun with: -s==2826928== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0) 也是直接指出了 一次引用未初始化的内存, 两次越界访问, 一次非法 free： 一次引用为初始化的内存 12345==2833797== Use of uninitialised value of size 8==2833797== at 0x48B681B: _itoa_word (_itoa.c:179)==2833797== by 0x48D26F4: __vfprintf_internal (vfprintf-internal.c:1687)==2833797== by 0x48BCEBE: printf (printf.c:33)==2833797== by 0x109225: main (out_of_bounds_access.c:13) 一次越界读访问： 12345==2826928== Invalid read of size 1==2826928== at 0x109200: main (out_of_bounds_access.c:14)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9) 一次越界写访问 12345==2826928== Invalid write of size 1==2826928== at 0x109213: main (out_of_bounds_access.c:15)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9) 一次 invaild free 123456789==2845980== Invalid free() / delete / delete[] / realloc()==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10927F: main (out_of_bounds_access.c:21)==2845980== Address 0x4a4d041 is 1 bytes inside a block of size 1,024 free'd==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10926F: main (out_of_bounds_access.c:20)==2845980== Block was alloc'd at==2845980== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x1091ED: main (out_of_bounds_access.c:9) 总结实际项目中应用的内存泄漏往往比这要复杂很多，可能代码考虑了正常情况下的 free，异常情况下没有free等， valgrind 是很强大，有很多参数，但也只能帮助我们在出现问题之后定位，还是需要养成良好的编码习惯，减少杜绝这类问题 参考文章1参考文章2","link":"/2021/01/17/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4/valgrind%20%E5%AE%9A%E4%BD%8D%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"title":"KASAN 定位越界访问","text":"WHYKASAN 原理KASAN 使用KASAN 代码KASAN overhead当然假设我们场景允许 使能KASAN之后，重新编译 kernel，也要关系 KASAN 所带来的overhead是否允许，如果你的场景本来就是一个高负载的已经80%多的CPU使用率了，然后开启KASAN,是很可能出问题的。","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/KASAN%20%E4%BD%BF%E7%94%A8/"},{"title":"kmemleak 定位内存泄露","text":"WHYkmemleak 原理kmemleak 使用kmemleak 代码kmemleak overhead当然假设我们场景允许 使能kmemleak之后，重新编译 kernel，也要关系 kmemleak 所带来的overhead是否允许，如果你的场景本来就是一个高负载的已经80%多的CPU使用率了，然后开启kmemleak,是很可能出问题的。 在 公司 一款三核心的 Cortex-A7 的产品中测试结果： disabled enabled Per Core增加% 换算成单核CPU%User: 3.54% 7.63% 4.09% 12.27%Sys: 10.68% 23.76% 13.08% 38.7%Idle: 84.6% 67.68% 17.02% -50.76% 在我自己 qemu 虚拟机中测试结果是： disabled enabled Per Core增加% 换算成单核CPU% User: 3.54% 7.63% 12.27%Sys: 10.68% 23.76% 38.7%Idle: 84.6% 67.68% -50.76%","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/kmemleak%20%E5%AE%9A%E4%BD%8D%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"title":"page_owner 定位 buddy 内存泄漏","text":"WHYpage_owner 原理page_owner 使用从mm/makefile 看，需要配置 CONFIG_PAGE_OWNER=y 1obj-$(CONFIG_PAGE_OWNER) += page_owner.o 重新编译之后，启动qemu, 需要加上 page_owner=on: 123456789101112sudo qemu-system-x86_64 \\ -kernel /tmp/bzImage \\ -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\ -append &quot;root=/dev/sda5 console=ttyS0 crashkernel=256M page_owner=on&quot; \\ -smp 4 \\ -m 2048 \\ --enable-kvm \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ --nographic \\ -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\ -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share page_owner 代码","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/page_owner%20%E5%AE%9A%E4%BD%8D%20buddy%20%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"title":"slub_debug 定位内存越界","text":"WHYslub_debug 原理slub_debug 使用slub_debug 代码slub_debug overhead当然假设我们场景允许 使能slub_debug之后，重新编译 kernel，也要关系 slub_debug 所带来的overhead是否允许，如果你的场景本来就是一个高负载的已经80%多的CPU使用率了，然后开启slub_debug,是很可能出问题的。 在 公司 一款三核心的 Cortex-A7 的产品中测试结果： disabled enabled Per Core增加% 换算成单核CPU%User: 3.54% 7.63% 4.09% 12.27%Sys: 10.68% 23.76% 13.08% 38.7%Idle: 84.6% 67.68% 17.02% -50.76% 在我自己 qemu 虚拟机中测试结果是： disabled enabled Per Core增加% 换算成单核CPU% User: 3.54% 7.63% 12.27%Sys: 10.68% 23.76% 38.7%Idle: 84.6% 67.68% -50.76%","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/slub%20debug%20%E5%AE%9A%E4%BD%8D%E5%86%85%E5%AD%98%E8%B6%8A%E7%95%8C/"},{"title":"使能lock_dep解决死锁问题","text":"lockdep 就是 lock dependencies 缩写，翻译是 锁依赖。 如何使能 lockdep在 make menuconfig 使能 lockdep 之后，会自动增加如下配置 12345678echo &quot;CONFIG_LOCKUP_DETECTOR=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_SOFTLOCKUP_DETECTOR=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_HARDLOCKUP_DETECTOR_PERF=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_HARDLOCKUP_DETECTOR=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HARDLOCKUP_PANIC=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.config 配置之后重新编译运行，会发现在 /proc/lockdep 目录下多出几个文件 12345678/proc/sys/kernel/lock_stat--------置位则可以查看/proc/lock_stat统计信息，清楚则关闭lockdep统计信息。/proc/sys/kernel/max_lock_depth---/proc/sys/kernel/prove_locking/proc/locks/proc/lock_stat-------------------关于锁的使用统计信息/proc/lockdep---------------------存在依赖关系的锁/proc/lockdep_stats---------------存在依赖关系锁的统计信息/proc/lockdep_chains--------------依赖关系锁链表 lockdep 原理常见的死锁有如下两种： 递归死锁：中断等延迟操作中使用了锁，和外面的锁构成了递归死锁。 AB-BA死锁：多个锁因处理不当而引发死锁，多个内核路径上的所处理顺序不一致也会导致死锁。 Linux内核提供死锁调试模块Lockdep，跟踪每个锁的自身状态和各个锁之间的依赖关系，经过一系列的验证规则来确保锁之间依赖关系是正确的。 先看代码注释 123456789101112131415/* * this code maps all the lock dependencies as they occur in a live kernel * and will warn about the following classes of locking bugs: * * - lock inversion scenarios * - circular lock dependencies * - hardirq/softirq safe/unsafe locking bugs * * Bugs are reported even if the current locking scenario does not cause * any deadlock at this point. * * I.e. if anytime in the past two locks were taken in a different order, * even if it happened for another task, even if those were different * locks (but of the same class as this lock), this code will detect it. */ 翻译就是可以解决如下的问题 锁定反转方案 – ABBA 循环锁依赖性 – 递归锁 hardirq / softirq安全/不安全的锁定错误 lockdep 案例写了一个ABBA型 基于 spinlock的 deadlock demo。参考 代码其实这个demo 会触发多个问题 deadlock 被检测出来。 由于是基于 spinlock 的，所以在等待lock的时候一直处于spin，导致 rcu stall 等待 20s 之后，由于基于 spinlock的，所以两个cpu一直未调度，发生 soft lockup，然后 panic. 编译安装之后，kmsg 显示如下问题： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[ 147.475517] lockdep_test: loading out-of-tree module taints kernel.[ 157.769995][ 157.770640] ======================================================[ 157.770683] WARNING: possible circular locking dependency detected[ 157.770683] 5.11.0-rc4+ #5 Tainted: G O[ 157.770683] ------------------------------------------------------[ 157.770683] krace_0/3755 is trying to acquire lock:[ 157.770683] ffffffffc0527468 (&amp;g_lockdep_test.lock_A){+.+.}-{2:2}, at: klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] but task is already holding lock:[ 157.770683] ffffffffc05274a8 (&amp;g_lockdep_test.lock_B){+.+.}-{2:2}, at: klockdep_test_BA+0x18/0x80 [lockdep_test][ 157.770683] which lock already depends on the new lock.[ 157.770683] the existing dependency chain (in reverse order) is:[ 157.770683] -&gt; #1 (&amp;g_lockdep_test.lock_B){+.+.}-{2:2}:[ 157.770683] _raw_spin_lock+0x27/0x40[ 157.770683] klockdep_test_AB+0x2e/0x80 [lockdep_test][ 157.770683] kthread+0x10a/0x140[ 157.770683] ret_from_fork+0x22/0x30[ 157.770683] -&gt; #0 (&amp;g_lockdep_test.lock_A){+.+.}-{2:2}:[ 157.770683] __lock_acquire+0x139e/0x28a0[ 157.770683] lock_acquire+0xbd/0x360[ 157.770683] _raw_spin_lock+0x27/0x40[ 157.770683] klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] kthread+0x10a/0x140[ 157.770683] ret_from_fork+0x22/0x30[ 157.770683] other info that might help us debug this:[ 157.770683] Possible unsafe locking scenario:[ 157.770683] CPU0 CPU1[ 157.770683] ---- ----[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A);[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A);[ 157.770683] *** DEADLOCK ***[ 157.770683] 1 lock held by krace_0/3755:[ 157.770683] #0: ffffffffc05274a8 (&amp;g_lockdep_test.lock_B){+.+.}-{2:2}, at: klockdep_test_BA+0x18/0x80 [lockdep_test][ 157.770683] stack backtrace:[ 157.770683] CPU: 1 PID: 3755 Comm: krace_0 Kdump: loaded Tainted: G O 5.11.0-rc4+ #5[ 157.770683] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1 04/01/2014[ 157.770683] Call Trace:[ 157.770683] dump_stack+0x77/0x97[ 157.770683] check_noncircular+0xfe/0x110[ 157.770683] ? find_held_lock+0x2b/0x80[ 157.770683] __lock_acquire+0x139e/0x28a0[ 157.770683] lock_acquire+0xbd/0x360[ 157.770683] ? klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] ? klockdep_test_AB+0x80/0x80 [lockdep_test][ 157.770683] _raw_spin_lock+0x27/0x40[ 157.770683] ? klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] kthread+0x10a/0x140[ 157.770683] ? kthread_park+0x80/0x80[ 157.770683] ret_from_fork+0x22/0x30 其中可以很明显看到如下提示 123456[ 157.770683] CPU0 CPU1[ 157.770683] ---- ----[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A);[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A); 这个提示简直无敌，完美显示了 deadlock 的原因 写了一个ABBA型 基于 mutex 的 deadlock demo。参考 代码其实这个demo 会触发多个问题 deadlock 被检测出来。 等待 120s 之后，由于基于 mutex的，所以发生死锁的俩线程都是 D状态，所以检测到发生 hung_task，然后 panic. 为啥 mutex 的waiter会 处于 D状态 ？看看代码 123456789101112static noinline void __sched__mutex_lock_slowpath(struct mutex *lock){ __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);}void __sched mutex_lock(struct mutex *lock){ if (!__mutex_trylock_fast(lock)) __mutex_lock_slowpath(lock);}EXPORT_SYMBOL(mutex_lock); 发现等待 mutex的线程都会被设置成 TASK_UNINTERRUPTIBLE，也就是 D状态。 也可以通过 hung_task 之后panic 的现场看出来 1234567891011121314151617181920212223crash&gt; ps | grep UN 3193 2 2 ffff9127bb82b240 UN 0.0 0 0 [krace_0] 3194 2 0 ffff9127c3458040 UN 0.0 0 0 [krace_1]crash&gt; bt 3193PID: 3193 TASK: ffff9127bb82b240 CPU: 2 COMMAND: &quot;krace_0&quot; #0 [ffffb557007efd78] __schedule at ffffffff8c319af2 #1 [ffffb557007efe08] schedule at ffffffff8c31a1e6 #2 [ffffb557007efe20] schedule_preempt_disabled at ffffffff8c31a53c #3 [ffffb557007efe28] __mutex_lock at ffffffff8c31bcd5 #4 [ffffb557007eff08] klockdep_test_mutex_BA at ffffffffc02990b2 [lockdep_test_mutex] #5 [ffffb557007eff10] kthread at ffffffff8b6930da #6 [ffffb557007eff50] ret_from_fork at ffffffff8b601ae2crash&gt;crash&gt; bt 3194PID: 3194 TASK: ffff9127c3458040 CPU: 0 COMMAND: &quot;krace_1&quot; #0 [ffffb557004c3d78] __schedule at ffffffff8c319af2 #1 [ffffb557004c3e08] schedule at ffffffff8c31a1e6 #2 [ffffb557004c3e20] schedule_preempt_disabled at ffffffff8c31a53c #3 [ffffb557004c3e28] __mutex_lock at ffffffff8c31bcd5 #4 [ffffb557004c3f08] klockdep_test_mutex_AB at ffffffffc0299032 [lockdep_test_mutex] #5 [ffffb557004c3f10] kthread at ffffffff8b6930da #6 [ffffb557004c3f50] ret_from_fork at ffffffff8b601ae2crash&gt; 可以看到 krace_0 krace_1 线程都是处于 UN 状态，这也导致此时系统负载有2 1234567crash&gt; sys KERNEL: vmlinux DUMPFILE: dump.202101221553 [PARTIAL DUMP] CPUS: 4 DATE: Fri Jan 22 15:53:16 CST 2021 UPTIME: 00:04:06LOAD AVERAGE: 1.96, 1.10, 0.46 再看一下 检测到的死锁日志： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970[ 60.151205] lockdep_test_mutex: loading out-of-tree module taints kernel.[ 70.216063][ 70.216623] ======================================================[ 70.216866] WARNING: possible circular locking dependency detected[ 70.216866] 5.11.0-rc4+ #5 Tainted: G O[ 70.216866] ------------------------------------------------------[ 70.216866] krace_0/3193 is trying to acquire lock:[ 70.216866] ffffffffc029b4b8 (&amp;g_lockdep_test_mutex.lock_A){+.+.}-{3:3}, at: klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] but task is already holding lock:[ 70.216866] ffffffffc029b548 (&amp;g_lockdep_test_mutex.lock_B){+.+.}-{3:3}, at: klockdep_test_mutex_BA+0x1a/0x80 [lockdep_test_mutex][ 70.216866] which lock already depends on the new lock.[ 70.216866] the existing dependency chain (in reverse order) is:[ 70.216866] -&gt; #1 (&amp;g_lockdep_test_mutex.lock_B){+.+.}-{3:3}:[ 70.216866] __mutex_lock+0x8d/0x920[ 70.216866] klockdep_test_mutex_AB+0x32/0x80 [lockdep_test_mutex][ 70.216866] kthread+0x10a/0x140[ 70.216866] ret_from_fork+0x22/0x30[ 70.216866] -&gt; #0 (&amp;g_lockdep_test_mutex.lock_A){+.+.}-{3:3}:[ 70.216866] __lock_acquire+0x139e/0x28a0[ 70.216866] lock_acquire+0xbd/0x360[ 70.216866] __mutex_lock+0x8d/0x920[ 70.216866] klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] kthread+0x10a/0x140[ 70.216866] ret_from_fork+0x22/0x30[ 70.216866] other info that might help us debug this:[ 70.216866] Possible unsafe locking scenario:[ 70.216866] CPU0 CPU1[ 70.216866] ---- ----[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A);[ 70.216866] *** DEADLOCK ***[ 70.216866] 1 lock held by krace_0/3193:[ 70.216866] #0: ffffffffc029b548 (&amp;g_lockdep_test_mutex.lock_B){+.+.}-{3:3}, at: klockdep_test_mutex_BA+0x1a/0x80 [lockdep_test_mutex][ 70.216866] stack backtrace:[ 70.216866] CPU: 2 PID: 3193 Comm: krace_0 Kdump: loaded Tainted: G O 5.11.0-rc4+ #5[ 70.216866] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1 04/01/2014[ 70.216866] Call Trace:[ 70.216866] dump_stack+0x77/0x97[ 70.216866] check_noncircular+0xfe/0x110[ 70.216866] __lock_acquire+0x139e/0x28a0[ 70.216866] lock_acquire+0xbd/0x360[ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? lockdep_hardirqs_on_prepare+0xd4/0x170[ 70.216866] ? _raw_spin_unlock_irqrestore+0x34/0x40[ 70.216866] __mutex_lock+0x8d/0x920[ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? find_held_lock+0x2b/0x80[ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? __next_timer_interrupt+0x100/0x100[ 70.216866] ? klockdep_test_mutex_AB+0x80/0x80 [lockdep_test_mutex][ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? klockdep_test_mutex_AB+0x80/0x80 [lockdep_test_mutex][ 70.216866] klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] kthread+0x10a/0x140[ 70.216866] ? kthread_park+0x80/0x80[ 70.216866] ret_from_fork+0x22/0x30 同样也是 给出了很详细出错位置，对于找出问题代码一如反掌。 123456[ 70.216866] CPU0 CPU1[ 70.216866] ---- ----[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A); lockdep 代码之后填坑 死锁带来的影响对于线程自身 spinlock 的死锁可以带来 线程一直在spin，占用cpu且无法恢复. mutex semaphore rwsem 的死锁会在slowpath 中让线程进入了TASK_UNINTERRUPTIBLE 状态，导致不响应外部信号，也无法使用 kill 去杀死 对于系统 spinlock 的死锁可以带来 rcu stall soft lockup hard lockup问题，如果对应项设置了 panic 选项，就会导致 kernel panic. mutex semaphore rwsem 的死锁会带来系统的 load升高，因为都在 slowpath 中让线程进入了TASK_UNINTERRUPTIBLE 状态，会被统计为系统负载，最后会导致 hung_task 发生，如果对应项设置了 panic 选项，就会导致 kernel panic. 不同锁进入slowpath的行为 123456789101112131415161718192021222324mutex:static noinline void __sched__mutex_lock_slowpath(struct mutex *lock){ __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);}semaphore:static noinline void __sched __down(struct semaphore *sem){ __down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);}semaphore:static noinline void __sched __down(struct semaphore *sem){ __down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);}rwsem:static inline void __down_write(struct rw_semaphore *sem){ __down_write_common(sem, TASK_UNINTERRUPTIBLE);} 可以参考：stack overflow提问博客园文章魅族内核团队文章内核文档","link":"/2021/01/23/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/%E4%BD%BF%E8%83%BDlock_dep%E8%A7%A3%E5%86%B3%E6%AD%BB%E9%94%81%E9%97%AE%E9%A2%98/"},{"title":"文件资源泄漏问题","text":"资源泄漏资源泄漏在实际编程中是一不小心就会遇到的一个问题，最常见的就是 内存泄漏。内存泄漏：对于短时间存在的进程，线程即使存在内存泄漏，往往也不会变现出来，开发人员也很难感知到。但是一旦是长时间运行的进程，线程存在内存泄漏的问题，那将是一个灾难，要么过一会被OOM KILL，要么自己重启，更严重的就是重启机器。 内存泄漏是指内存被分配出来，但是后续一直未使用，且失去了这个内存的引用，一直无法释放的问题。 但是除了最常见内存泄漏之外，还有其他各种资源泄漏的问题，比如这次的 进程文件资源泄漏。 场景虽然是嵌入式平台，也有一些业务需要做压力测试连续几天到一个月不关机长时间跑测试。 某个业务在运行两天之后就会出现进程挂掉的问题。 代码复现如果能用简单代码复现的问题，都不大 代码 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;dirent.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#define DIR_NAME &quot;/tmp/asdf&quot;int sync_dir(char *dir){ if(!dir) return -1; int fd = open(dir, O_ASYNC); if (-1 == fd) return -1; return 0;}void main(void){ int i = 0; int ret = -1; for (i = 0; i &lt; 1024 * 1024 * 64; i++) { ret = sync_dir(DIR_NAME); if (ret) printf(&quot;sync_dir failed. i = %d, ret = %d\\n&quot;, i, ret); else printf(&quot;sync_dir sucess. i = %d\\n&quot;, i); usleep(1000 * 10); }} 嵌入式设备上的资源限制 123root@device_gls:/proc/sys/fs # cat file-nr800 0 21378root@device_gls:/proc/sys/fs # 意思是当前系统允许打开 21378 个文件，已经打开了 800个文件了 我选择用 ubuntu 系统去复现，先将 系统 fd上限制调制与设备一样的水平 12345678tencent_clould@ubuntu: /proc/sys/fs# cat file-max9223372036854775807tencent_clould@1ubuntu: /proc/sys/fs# sudo suVM-0-11-ubuntu# echo 21378 &gt; file-maxVM-0-11-ubuntu# exittencent_clould@ubuntu: /proc/sys/fs# cat file-max21378tencent_clould@ubuntu: /proc/sys/fs# 尝试复现 123456789tencent_clould@130ubuntu: ~/workspace/hexo_blog/source/_posts/资源管理# ./a.outsync_dir sucess. i = 0sync_dir sucess. i = 1....sync_dir sucess. i = 5sync_dir sucess. i = 18194sync_dir sucess. i = 18195...sync_dir failed. i = 18196, ret = -1 同时观察 /proc/sys/fs/file-nr 12345678910111213141516tencent_clould@ubuntu: /proc/sys/fs# cat file-nr4704 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr9824 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr17760 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr20224 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr21024 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr21216 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nrzsh: pipe failed: too many open files in systemtencent_clould@ubuntu: /proc/sys/fs# cat file-nrzsh: pipe failed: too many open files in system 可以看到 a.out 最后无法 通过 open 打开文件了，且同时 zsh shell 都无法打开file-nr了。 与此同时，看到 dmesg 的信息 12[57615.006011] VFS: file-max limit 21378 reached[57616.671617] VFS: file-max limit 21378 reached 也可以比较容易定位出来。 这个问题困扰了当时开发同事很久，最后发现原来是 系统 fd 资源全部泄漏殆尽导致的。 其他限制资源使用的方式 其实还有其他方式限制资源 如 ulimit 123456789101112131415161718tencent_clould@ubuntu: /proc/sys/fs# ulimit -a-t: cpu time (seconds) unlimited-f: file size (blocks) unlimited-d: data seg size (kbytes) unlimited-s: stack size (kbytes) 8192-c: core file size (blocks) 0-m: resident set size (kbytes) unlimited-u: processes 7582-n: file descriptors 1024-l: locked-in-memory size (kbytes) 65536-v: address space (kbytes) unlimited-x: file locks unlimited-i: pending signals 7582-q: bytes in POSIX msg queues 819200-e: max nice 0-r: max rt priority 0-N 15: unlimitedtencent_clould@ubuntu: /proc/sys/fs# /proc/pid/limits12345678910111213141516171819tencent_clould@ubuntu: /proc/sys/fs# cat /proc/4833/limitsLimit Soft Limit Hard Limit UnitsMax cpu time unlimited unlimited secondsMax file size unlimited unlimited bytesMax data size unlimited unlimited bytesMax stack size 8388608 unlimited bytesMax core file size 0 unlimited bytesMax resident set unlimited unlimited bytesMax processes 7582 7582 processesMax open files 1024 1048576 filesMax locked memory 67108864 67108864 bytesMax address space unlimited unlimited bytesMax file locks unlimited unlimited locksMax pending signals 7582 7582 signalsMax msgqueue size 819200 819200 bytesMax nice priority 0 0Max realtime priority 0 0Max realtime timeout unlimited unlimited ustencent_clould@ubuntu: /proc/sys/fs# /etc/security/limits.conf 这里可以预先配置。","link":"/2021/01/15/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/%E6%96%87%E4%BB%B6%E8%B5%84%E6%BA%90%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98/"},{"title":"进程虚拟地址空间泄漏问题","text":"虚拟内存泄漏一般情况我们说 内存泄漏，都是指的是物理内存泄漏，毕竟物理内存是实实在在的，一个进程泄漏了，那么整个系统中的可用内存就会变少。 但是linux的虚拟内存空间是各个进程之间相互隔离的，在arm64系统中 有 T,在 arm32系统中也有3G多。但是否意味着这种 虚拟内存我们就不需要关心呢？ 尝试复现代码： 12345678910111213141516171819#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void main(void){ int i = 0; char *p = NULL; for (i = 0; i &lt; 1024 * 1024 * 64; i++) { p = malloc(1024 * 1024); if (!p) {printf(&quot;malloc failed,p = %p, i = %d\\n&quot;, p, i); break;} else {printf(&quot;malloc sucess.p = %p, i = %d\\n&quot;, p, i);} usleep(1000 * 10); } printf(&quot;malloc failed,just wait!!\\n&quot;); while(1) { usleep(1000 * 10); }} 每次尝试需分配 1G的虚拟内存，但是不会去使用，也就是不会去申请物理内存 x86_64 平台我一开始尝试在腾讯云 服务器上复现： 1234567tencent_clould@130ubuntu: ~/workspace/test_modules/resource_leak/process_virtual_address_memleak# ./a.outmalloc sucess. p = 0x7f8e3f5c6010, i = 0malloc sucess. p = 0x7f8e3f4c5010, i = 1......malloc sucess. p = 0x56771e4ba6e0, i = 461827malloc sucess. p = 0x56771e5ba6f0, i = 461828[1] 2721866 killed ./a.out 直到最后分配到 461828 MB 内存的时候，a.out 被OOM kill了。 到底是为啥呢？？？毕竟我也没有去往 malloc 的虚拟地址中写数据，也不应该会分配物理页面。从 top 命令看 123456789101112top - 15:47:46 up 3 days, 15:06, 2 users, load average: 0.24, 0.16, 0.10Tasks: 157 total, 1 running, 156 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.3 us, 3.6 sy, 0.0 ni, 92.7 id, 0.3 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 1987.9 total, 70.5 free, 1660.6 used, 256.8 buff/cacheMiB Swap: 2048.0 total, 1499.9 free, 548.1 used. 162.0 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2783846 ubuntu 20 0 173.4g 683692 1380 S 0.3 33.6 0:06.86 a.out 1378 jenkins 20 0 2510448 176168 2924 S 0.7 8.7 7:05.12 java2779920 ubuntu 20 0 925684 107776 15196 S 1.0 5.3 0:21.10 node2703822 ubuntu 20 0 1611828 41268 11624 S 0.3 2.0 0:24.40 node2773158 ubuntu 20 0 702408 39436 12496 S 0.7 1.9 0:15.87 已经分配了 173G 虚拟内存，但是同时可以发现 驻留在物理内存中的页面也达到了 683692kb.这到底是什么？ 通过对比 运行 mytest 前后的 /proc/meminfo 文件，发现差异主要在 1234567系统正常Active(anon): 243372 kBInactive(anon): 267836 kBmytest运行之后Active(anon): 550888 kBInactive(anon): 576840 kB 说明确实是 mytest 消耗了大量的内存 用作匿名页面接下来就需要查看具体是哪种页面消耗了物理内存了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354tencent_clould@ubuntu: /proc/2798128# cat smaps | grep heap -A 407fbca3ff9000-7fc69e600000 rw-p 00000000 00:00 0 [heap]Size: 41850908 kBKernelPageSize: 4 kBMMUPageSize: 4 kBRss: 162844 kBPss: 162844 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 0 kBPrivate_Dirty: 162844 kBReferenced: 162844 kBAnonymous: 162844 kBLazyFree: 0 kB此时top 输出top - 16:05:47 up 3 days, 15:24, 4 users, load average: 0.14, 0.09, 0.10Tasks: 157 total, 1 running, 156 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.0 us, 2.7 sy, 0.0 ni, 94.0 id, 0.0 wa, 0.0 hi, 0.3 si, 0.0 stMiB Mem : 1987.9 total, 786.6 free, 917.0 used, 284.2 buff/cacheMiB Swap: 2048.0 total, 1512.2 free, 535.8 used. 912.0 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2798128 ubuntu 20 0 45.1g 185412 1408 S 0.7 9.1 0:01.66 a.out......等待3min......tencent_clould@ubuntu: /proc/2798128# cat smaps | grep heap -A 407fb68e600000-7fc69e600000 rw-p 00000000 00:00 0 [heap]Size: 67371008 kBKernelPageSize: 4 kBMMUPageSize: 4 kBRss: 262144 kBPss: 262144 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 0 kBPrivate_Dirty: 262144 kBReferenced: 262144 kBAnonymous: 262144 kBLazyFree: 0 kBAnonHugePages: 0 kB此时top 输出top - 16:09:43 up 3 days, 15:27, 4 users, load average: 0.24, 0.12, 0.10Tasks: 159 total, 1 running, 158 sleeping, 0 stopped, 0 zombie%Cpu(s): 2.7 us, 3.4 sy, 0.0 ni, 90.3 id, 3.7 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 1987.9 total, 585.4 free, 1072.9 used, 329.5 buff/cacheMiB Swap: 2048.0 total, 1523.9 free, 524.1 used. 754.9 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2798128 ubuntu 20 0 67.9g 278604 1408 S 0.3 13.7 0:02.55 a.out 可以看到虽然 没有往堆内存中写入数据，但是还是消耗了较多的物理内存？这是怎么回事？ 猜想一， 可能是一直连续分配虚拟内存，导致 vma 使用很多？通过查看 /proc/slabinfo 和 /proc/pid/maps 看堆内存就一块，基本没有分块的，kernel中也有vma_merge 的操作，所以应该不是vma 占用的内存 ??？我还没有其他头绪，为什么会占用物理内存 arm32 平台然后想了想，x86_64平台可能是虚拟内存太大，复现时间成本有点高就使用了公司的板子，重新编译了一遍，在一个 arm32 板子上运行 12345678910111213141516171819202122232425262728293031323334353637root@device_gls:/ # mytestmalloc sucess. p = 0xb6b00000, i = 0malloc sucess. p = 0xb6a00000, i = 1malloc sucess. p = 0xb6900000, i = 2malloc sucess. p = 0xb6800000, i = 3malloc sucess. p = 0xb6700000, i = 4malloc sucess. p = 0xb6600000, i = 5malloc sucess. p = 0xb6500000, i = 6......malloc sucess. p = 0x10380000, i = 2663malloc sucess. p = 0x10280000, i = 2664malloc sucess. p = 0x10180000, i = 2665malloc sucess. p = 0x10080000, i = 2666......malloc sucess. p = 0xff80000, i = 2667malloc sucess. p = 0xfe80000, i = 2668malloc sucess. p = 0xfd80000, i = 2669malloc sucess. p = 0xfc80000, i = 2670......malloc sucess. p = 0x4180000, i = 2857malloc sucess. p = 0x4080000, i = 2858malloc sucess. p = 0x3f80000, i = 2859......malloc sucess. p = 0x1080000, i = 2906malloc sucess. p = 0xf80000, i = 2907malloc sucess. p = 0xe80000, i = 2908......malloc sucess. p = 0x80000, i = 2922malloc sucess. p = 0xb6f80000, i = 2923malloc sucess. p = 0xb7080000, i = 2924......malloc sucess. p = 0xbe880000, i = 3044malloc sucess. p = 0xbe980000, i = 3045malloc sucess. p = 0xbea80000, i = 3046malloc sucess. p = 0xbed80000, i = 3047malloc failed, p = 0x0, i = 3048malloc failed,just wait!! 可以看到，arm32 平台在分配了 3038MB 虚拟内存之后，在分配第3039块1MB的内存时，由于虚拟地址空间用完了，不能再继续分配虚拟内存？ 可以看到此时 mytest 进程的地址空间 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647root@device_gls:/proc/1293 # cat maps00080000-7f580000 rw-p 00000000 00:00 0 [anon:libc_malloc]7f5cd000-7f5d0000 r-xp 00000000 103:0f 1523 /system/bin/mytest7f5d0000-7f5d1000 r--p 00002000 103:0f 1523 /system/bin/mytest7f5d1000-7f5d2000 rw-p 00000000 00:00 0 [heap]7f600000-b6d00000 rw-p 00000000 00:00 0 [anon:libc_malloc]b6d64000-b6d68000 r-xp 00000000 103:0f 1428 /system/lib/libnetd_client.sob6d68000-b6d69000 r--p 00003000 103:0f 1428 /system/lib/libnetd_client.sob6d69000-b6d6a000 rw-p 00004000 103:0f 1428 /system/lib/libnetd_client.sob6d6a000-b6d8a000 r--s 00000000 00:0f 11859 /dev/__properties__b6d8a000-b6d8b000 rw-p 00000000 00:00 0 [anon:linker_alloc_vector]b6d8c000-b6d8d000 rw-p 00000000 00:00 0 [anon:linker_alloc_vector]b6d8d000-b6dac000 r-xp 00000000 103:0f 1421 /system/lib/libm.sob6dac000-b6dad000 ---p 00000000 00:00 0b6dad000-b6dae000 r--p 0001f000 103:0f 1421 /system/lib/libm.sob6dae000-b6daf000 rw-p 00020000 103:0f 1421 /system/lib/libm.sob6daf000-b6e23000 r-xp 00000000 103:0f 1355 /system/lib/libc.sob6e23000-b6e27000 r--p 00073000 103:0f 1355 /system/lib/libc.sob6e27000-b6e2a000 rw-p 00077000 103:0f 1355 /system/lib/libc.sob6e2a000-b6e34000 rw-p 00000000 00:00 0b6e34000-b6ebc000 r-xp 00000000 103:0f 1354 /system/lib/libc++.sob6ebc000-b6ebd000 ---p 00000000 00:00 0b6ebd000-b6ec1000 r--p 00088000 103:0f 1354 /system/lib/libc++.sob6ec1000-b6ec2000 rw-p 0008c000 103:0f 1354 /system/lib/libc++.sob6ec2000-b6ec3000 rw-p 00000000 00:00 0b6ec3000-b6ec4000 r--p 00000000 00:00 0b6ec4000-b6ec5000 r--p 00000000 00:00 0 [anon:linker_alloc]b6ec5000-b6ec6000 rw-p 00000000 00:00 0 [anon:linker_alloc]b6ec6000-b6ec7000 rw-p 00000000 00:00 0 [anon:linker_alloc_vector]b6ec7000-b6ec8000 rw-p 00000000 00:00 0 [anon:linker_alloc_32]b6ec8000-b6ec9000 r--p 00000000 00:00 0 [anon:linker_alloc]b6ec9000-b6ee9000 r--s 00000000 00:0f 11859 /dev/__properties__b6ee9000-b6eea000 r--p 00000000 00:00 0b6eea000-b6eeb000 ---p 00000000 00:00 0b6eeb000-b6eed000 rw-p 00000000 00:00 0 [anon:thread signal stack]b6eed000-b6f0a000 r-xp 00000000 103:0f 170 /system/bin/linkerb6f0a000-b6f0b000 r--p 0001c000 103:0f 170 /system/bin/linkerb6f0b000-b6f0d000 rw-p 0001d000 103:0f 170 /system/bin/linkerb6f0d000-b6f0f000 rw-p 00000000 00:00 0b6f80000-beb80000 rw-p 00000000 00:00 0 [anon:libc_malloc]bed01000-bed22000 rw-p 00000000 00:00 0 [stack]bed80000-bee80000 rw-p 00000000 00:00 0 [anon:libc_malloc]bef66000-bef67000 r-xp 00000000 00:00 0 [sigpage]bef67000-bef68000 r--p 00000000 00:00 0 [vvar]bef68000-bef69000 r-xp 00000000 00:00 0 [vdso]ffff0000-ffff1000 r-xp 00000000 00:00 0 [vectors]root@device_gls:/proc/1293 # 我们看 libc_malloc 区域的 的大小 123400080000-7f580000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 2037MB7f600000-b6d00000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 887MBb6f80000-beb80000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 124MBbed80000-bee80000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 1MB 总和是 2037 + 887 + 124 + 1 = 3049MB 这是项目中一个实际的bug，场景是 直播盒子 做长跑测试的时候，每到7天左右，直播业务就会因为malloc失败而导致失败，但是由于业务代码写的问题，没有检查 malloc返回值的原因，导致业务应用会crash, 现象是空指针错误。。很难联想到是因为进程虚拟地址空间全部泄漏导致的问题。业务的小伙伴差了一个多月都没有头绪。。 在 arm32 设备上，User 14 + Nice 0 + Sys 49 + Idle 246 + IOW 1 + IRQ 0 + SIRQ 0 = 310 PID PR CPU% S #THR VSS RSS PCY UID Name 3592 0 0% S 1 3124044K 1688K fg root mytest RSS 也占用了一些，但是远没 X86设备上夸张 123456789101112131415161718192021222324252627282930313233343536373839404142434445Inspiron-5548@ubuntu: ~/workspace# cat /tmp/123 | grep RssRss: 0 kBRss: 12 kBRss: 4 kBRss: 0 kBRss: 256 kBRss: 16 kBRss: 4 kBRss: 4 kBRss: 28 kBRss: 4 kBRss: 4 kBRss: 64 kBRss: 0 kBRss: 4 kBRss: 4 kBRss: 460 kBRss: 16 kBRss: 12 kBRss: 28 kBRss: 536 kBRss: 0 kBRss: 16 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 28 kBRss: 4 kBRss: 0 kBRss: 0 kBRss: 116 kBRss: 4 kBRss: 8 kBRss: 8 kBRss: 0 kBRss: 8 kBRss: 0 kBRss: 4 kBRss: 4 kBRss: 0 kB 其中 malloc 的 rss 只占用 256Kb,大部分是 libc, libc++ libm的代码段占用 12345678910117f680000-b6d80000 rw-p 00000000 00:00 0 [anon:libc_malloc]Name: [anon:libc_malloc]Size: 908288 kBRss: 256 kBPss: 256 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 0 kBPrivate_Dirty: 256 kBReferenced: 256 kBAnonymous: 256 kB 由此可见 进程虚拟地址空间泄漏还是有可能会存在的，尽管可能概率非常低，对系统危害也比物理内存泄漏小很多。但是一旦发生，他的危害对于业务本身也是致命的，必须重启业务或者系统来恢复。 编程习惯养好，一定需要检查函数返回值。","link":"/2021/01/15/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/%E8%BF%9B%E7%A8%8B%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E6%B3%84%E6%BC%8F/"},{"title":"linux hungtask问题","text":"hungtask 定义我们先看一段实际hungtask的 dmesg打印 123456789101112131415[45312.818392] INFO: task ftpd:17682 blocked for more than 120 seconds.[45312.818470] Tainted: G OE 5.4.44 #1[45312.818472] &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.[45312.818474] ftpd D 0 17682 2 0x80004000[45312.818478] Call Trace:[45312.818698] __schedule+0x2e3/0x740[45312.818700] schedule+0x42/0xb0[45312.818702] schedule_timeout+0x152/0x2f0[45312.818755] ? __next_timer_interrupt+0xe0/0xe0[45312.818756] msleep+0x2e/0x40[45312.818760] ftpd+0xaa/0x170 [ftpd][45312.818804] kthread+0x104/0x140[45312.818806] ? 0xffffffffc05bb000[45312.818807] ? kthread_park+0x90/0x90[45312.818808] ret_from_fork+0x35/0x40 第一行就写了 pid为 17682的 task被 block阻塞了超过120s得不到执行，这种往往只是打印hungtask的线程 和 backtrace而已，实际中一般也不会导致panic，但是这种问题确是我们不能忽略的点，有可能这就是后面会出事故的点 hungtask是内核的一种自我保护行为，在检测到一个线程长时间（可设置）处于D状态之后，会打印出线程相关信息和backtrace. 12sh@ubuntu:/var/crash$ ps -aux |grep ftpdroot 18007 0.0 0.0 0 0 ? D 01:01 0:00 [ftpd] D 状态也是进程的一种状态，对应内核中的 TASK_UNINTERRUPTIBLE 状态，一般等待磁盘IO的线程会设置为 TASK_UNINTERRUPTIBLE 状态，此状态无法wakeup，不管是 wake_up_process，还是kill -9去尝试杀死他 都不能将 TASK_UNINTERRUPTIBLE 状态线程唤醒，只有等他自己wakeup。 同时linux内核在统计系统load的时候也将 TASK_UNINTERRUPTIBLE 状态的现场统计了进去，这样如果系统中出现D状态线程之后，整体系统的load就会较高，但是CPU loading却很小，几乎为0。这种统计方式也使得linux系统中loadavg这个参考指标的意义不是那么的大了。 12345678910sh@ubuntu:~$ toptop - 01:04:54 up 13:09, 3 users, load average: 2.05, 1.06, 0.49Tasks: 341 total, 1 running, 340 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.1 us, 0.1 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 3716.5 total, 249.5 free, 2034.2 used, 1432.7 buff/cacheMiB Swap: 2048.0 total, 1557.2 free, 490.8 used. 1398.5 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2253 rlk 20 0 299752 13200 10788 S 0.3 0.3 0:36.83 vmtoolsd 18184 rlk 20 0 20700 4292 3384 R 0.3 0.1 0:00.08 top 什么样的线程会处于D状态，即 TASK_UNINTERRUPTIBLE 状态呢? 123456789101. 磁盘IO线程，在等待磁盘读入数据的过程中会将自己设置为 TASK_UNINTERRUPTIBLE 状态2. 还有一个 msleep()，这个API会先将线程状态设置为 TASK_UNINTERRUPTIBLE,再调用 schedule_timeouot()void msleep(unsigned int msecs){ unsigned long timeout = msecs_to_jiffies(msecs) + 1; while (timeout) timeout = schedule_timeout_uninterruptible(timeout);} config配置 hung_task需要开启如下配置 1234echo &quot;CONFIG_DETECT_HUNG_TASK=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HUNG_TASK_PANIC=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.config hungtask 检测的实现上面讲了hungtask的原理，下面通过代码走读的方式来分析一下hungtask实现细节 首先是init, hungtask机制在初始化的时候使用kthread_run 运行了一个 [khungtaskd]线程，hungtask 主要工作就是在 hungtaskd 线程中完成的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//这是主要干活的static void check_hung_task(struct task_struct *t, unsigned long timeout){ unsigned long switch_count = t-&gt;nvcsw + t-&gt;nivcsw; if (switch_count != t-&gt;last_switch_count) { //此task 没有hungtask t-&gt;last_switch_count = switch_count; t-&gt;last_switch_time = jiffies; return; } if (time_is_after_jiffies(t-&gt;last_switch_time + timeout * HZ)) return; // 此task虽然hung了，但是还未到 120s == 120 * HZ if (sysctl_hung_task_warnings) { pr_err(&quot;INFO: task %s:%d blocked for more than %ld seconds.\\n&quot;, t-&gt;comm, t-&gt;pid, (jiffies - t-&gt;last_switch_time) / HZ); xxxx //提示信息 sched_show_task(t); }}/* * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for * a really long time (120 seconds). If that happens, print out a warning. */static void check_hung_uninterruptible_tasks(unsigned long timeout){ for_each_process_thread(g, t) { //遍历所有线程 if (t-&gt;state == TASK_UNINTERRUPTIBLE) // check当前线程是否处于D状态 check_hung_task(t, timeout); }}/* * kthread which checks for tasks stuck in D state */static int watchdog(void *dummy){ unsigned long hung_last_checked = jiffies; set_user_nice(current, 0); for ( ; ; ) { // 是内核守护线程，所以直接一个for循环 unsigned long timeout = sysctl_hung_task_timeout_secs; unsigned long interval = sysctl_hung_task_check_interval_secs; long t; if (interval == 0) interval = timeout; interval = min_t(unsigned long, interval, timeout); t = hung_timeout_jiffies(hung_last_checked, interval); if (t &lt;= 0) { // 因为最后调用的是 schedule_timout_interruptible()，可以被唤醒，所以需要判断一下 if (!atomic_xchg(&amp;reset_hung_task, 0) &amp;&amp; !hung_detector_suspended) check_hung_uninterruptible_tasks(timeout); // 给所有线程检查是否处于D状态 hung_last_checked = jiffies; continue; } schedule_timeout_interruptible(t); } return 0;}static int __init hung_task_init(void) // 初始化 khungtaskd，主体是 watchdog 函数{ atomic_notifier_chain_register(&amp;panic_notifier_list, &amp;panic_block); /* Disable hung task detector on suspend */ pm_notifier(hungtask_pm_notify, 0); watchdog_task = kthread_run(watchdog, NULL, &quot;khungtaskd&quot;); return 0;}subsys_initcall(hung_task_init); hungtask机制 如何判断这个线程一直没有进行切换得到运行呢？主要用的是 task_struct的几个成员， nvcsw nivcsw last_switch_count分别对应 非自愿上下文切换次数，自愿上下文切换次数，上次检查时上下文总切换次数。通过在扫描时，比较这几个成员的值就可以清晰的看到在两次扫描之间有没有进行过上下文切换 hungtask机制 如何判断这个线程超过120s都没有得到运行呢？主要用的是 task_struct 的 last_switch_time 成员，利用 last_switch_time + timeout * HZ 和 jiffies 比较就可以轻易得出是否超过120s没有被调度过 hungtask 行为控制一般系统中出现hungtask 可能都是短暂的异常，一般就打印一下 info 和 backtrace 即可，但是kernel也提供了相关的选择，比如可以选择hungtask的时候直接panic 1234echo 1 &gt; /proc/sys/kernel/hung_task_panic # hungtask的时候直接 panicecho 120 &gt; /proc/sys/kernel/hung_task_timeout_secs # 一个任务处于D状态多久我们认为他是hungtask了echo 3 &gt; /proc/sys/kernel/hung_task_check_interval_secs # 内核多久进行一次hungtask扫描echo 7 &gt; /proc/sys/kernel/hung_task_warnings # 设置hungtask的warning的等级 其实这些控制参数都对应在上面代码中，由于篇幅原因，精简了一下代码 总结一般hungtask出现，肯定会伴随着 D状态 的线程产生，且超过设定时间一直处于D状态，没有被调度过 D状态，即 TASK_UNINTERRUPTIBLE 线程，一般产生于 IO磁盘等待上 msleep 调用接口上 一些锁机制在等待锁的持有者的时候也会将线程状态设为 TASK_UNINTERRUPTIBLE，那样如果出现某些类型锁的死锁的时候也会出现 hungtask 贴一个其他博客：进程D状态死锁检测","link":"/2020/09/08/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/hung_task%E9%97%AE%E9%A2%98/"},{"title":"linux lockup问题","text":"lockup定义lockup是指是指某段内核代码占着CPU不放。 lockup分两种：hard lockup 与 soft lockup，区别是hard lockup是屏蔽系统中断情况下发生的。贴一段原文： 1234Short answer：A ‘soft lockup’ is defined as a bug that causes the kernel to loop in kernel mode for more than 20 seconds […], without giving other tasks a chance to run.A ‘hard lockup’ is defined as a bug that causes the CPU to loop in kernel mode for more than 10 seconds […], without letting other interrupts have a chance to run. 有几个概念需要澄清一下： 发生lockup的一定是内核代码，因为用户态代码都是可以抢占的。 在内核态代码中发生lockup时，一般伴随着 preempt_disable()或者local_irq_disable()，产生lockup 的基本条件是禁止抢占，当然禁止中断这种就更猛了 Soft lockupSoft lockup是指CPU被内核代码一直占用着，系统某个CPU上超过20s其他进程无法得到运行。系统是如何检测 Soft lockup的呢？ 实现机制设立涉及到几个概念：一般优先级的普通进程，最高优先级的watchdog/?内核线程，Hr-timer时钟中断，其中Hr-timer可以打断watchdog/?执行，watchdog/?可以打断普通优先级的进程执行。 Soft lockup机制在初始化的时候会init一个 Hr-timer，定时执行在执行过程中 获取 percpu变量 watchdog_touch_ts 的值 唤醒 watchdog/? 内核线程 比较 当前timestamp 和 watchdog_touch_ts值 内核线程会用当前的time_stamp 更新 watchdog_touch_ts 贴一个其他博客：进程R状态死锁检测 lockup 与 hung_task 区别soft lockup：一定是 RU 状态进程触发的， RU 状态进程一直运行，占用CPU超过20s之后，还未有过进程切换，就会出现这个问题。 最好复现方式是: preempt_disable()之后 一直循环等待，不去preempt_enable()，这时候就会触发这个问题。a. spin_lock() / preempt_disable() 12345spin_lock(&amp;lock);while(1) { i = i + 1;}spin_unlock(&amp;lock); b. spinlock的 dead_lock 也会触发这种问题 123456 CPU0 CPU1 spin_lock(&amp;lock_B);spin_lock(&amp;lock_A); spin_lock(&amp;lock_A);spin_lock(&amp;lock_B);xxx 此时 CPU0 CPU1 都会检测到 soft lockup hard lockup：也一定是 RU 状态进程触发的， RU 状态进程一直运行，同时禁止了本地中断。占用CPU超过10s之后，还未有过进程切换且本地中断还未打开，就会出现这个问题。 与 soft lockup 相比，只是加了一个条件是中断关闭。最好复现方式是: local_irq_disable() 之后 一直循环等待，不去 local_irq_enable()，这时候就会触发 hard lockup 这个问题。 1 hung task：hung task 一定是 UN 状态进程触发的，hung_task从 注释可以看到 12345/* * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for * a really long time (120 seconds). If that happens, print out * a warning. */ UN 是 TASK_UNINTERRUPTIBLE 缩写，也就是D状态的线程。","link":"/2020/09/12/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/lockup%E9%97%AE%E9%A2%98/"},{"title":"关于preempt_count的思考","text":"preempt_conut 本质就是一个int型的数，是每个 task_struct 的 thread_info 的一个成员变量，但是他和系统的调度密切相关，当然也十分重要。 12345struct thread_info { unsigned long flags; /* low level flags */ int preempt_count; /* 0 =&gt; preemptable, &lt;0 =&gt; bug */ .......}; 在 inlcude/linux/preempt.h 文件看相关定义 12345678910111213141516171819202122232425262728293031323334static __always_inline int preempt_count(void){ return READ_ONCE(current_thread_info()-&gt;preempt_count);}/* * PREEMPT_MASK: 0x000000ff * SOFTIRQ_MASK: 0x0000ff00 * HARDIRQ_MASK: 0x000f0000 * NMI_MASK: 0x00100000 * PREEMPT_NEED_RESCHED: 0x80000000 */#define PREEMPT_BITS 8#define SOFTIRQ_BITS 8#define HARDIRQ_BITS 4#define NMI_BITS 1#define PREEMPT_SHIFT 0#define SOFTIRQ_SHIFT (PREEMPT_SHIFT + PREEMPT_BITS)#define HARDIRQ_SHIFT (SOFTIRQ_SHIFT + SOFTIRQ_BITS)#define NMI_SHIFT (HARDIRQ_SHIFT + HARDIRQ_BITS)#define hardirq_count() (preempt_count() &amp; HARDIRQ_MASK)#define softirq_count() (preempt_count() &amp; SOFTIRQ_MASK)#define irq_count() (preempt_count() &amp; (HARDIRQ_MASK | SOFTIRQ_MASK \\ | NMI_MASK))#define in_irq() (hardirq_count())#define in_softirq() (softirq_count())#define in_interrupt() (irq _count())#define in_serving_softirq() (softirq_count() &amp; SOFTIRQ_OFFSET)#define in_nmi() (preempt_count() &amp; NMI_MASK)#define in_task() (!(preempt_count() &amp; \\ (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET))) 可以看出 preempt_count 0-7 bit被用来抢占计数，8-15 bit被用来软中断计数，16-19 bit被用来硬件中断计数，20bit 被用来 NMI中断计数，31 bit被用来记录是否需要立即sched. in_interrupt() 这些宏本质也是根据 preempt_count()来判断的。 hard irq在进入irq的时候通过 irq_enter 将preempt_count 的 16-19bit ++，在退出irq的时候通过 irq_exit 将preempt_count 的 16-19bit –，但是由于目前linux中的中断往往是不可嵌套的，所以一般 hardirq 只会用到 16 bit，为什么linux给 hardirq 保留了4bit呢，這是歷史原因造成的，早期hardirq还是可以嵌套的。 irq_enter() 123456789101112131415161718192021222324#define preempt_count_add(val) __preempt_count_add(val)#define __irq_enter() \\ do { \\ account_irq_enter_time(current); \\ preempt_count_add(HARDIRQ_OFFSET); \\ // preempt_count 和 hardirq相关++ trace_hardirq_enter(); \\ } while (0)void irq_enter(void) //进入中断上下文{ rcu_irq_enter(); if (is_idle_task(current) &amp;&amp; !in_interrupt()) { /* * Prevent raise_softirq from needlessly waking up ksoftirqd * here, as softirq will be serviced on return from interrupt. */ local_bh_disable(); tick_irq_enter(); _local_bh_enable(); } __irq_enter();} irq_exit() 123456789101112131415161718#define preempt_count_sub(val) __preempt_count_sub(val)void irq_exit(void) //退出中断上下文{#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED local_irq_disable();#else lockdep_assert_irqs_disabled();#endif account_irq_exit_time(current); preempt_count_sub(HARDIRQ_OFFSET); // preempt_count 和 hardirq相关-- if (!in_interrupt() &amp;&amp; local_softirq_pending()) invoke_softirq(); tick_irq_exit(); rcu_irq_exit(); trace_hardirq_exit(); /* must be last! */} soft irqpreempt_count中的第8到15个bit表示softirq count，它记录了进入softirq的嵌套次数，如果softirq count的值为正数，说明现在正处于softirq上下文中。由于softirq在单个CPU上是不会嵌套执行的，因此和hardirq count一样，实际只需要一个bit(bit 8)就可以了。还有一种情况，softirq count 会用到不止 bit8，在禁用中断下半部的情况下，每禁用一次softirq count 就会增加1，理论上最多可以嵌套16次。 进入退出软中断的case 1234567891011121314asmlinkage __visible void __softirq_entry __do_softirq(void){ ...... __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET); ...... while ((softirq_bit = ffs(pending))) { ...... h-&gt;action(h); ...... } ...... __local_bh_enable(SOFTIRQ_OFFSET); ......} 禁止、开启中断下半部的case 123456789101112131415161718192021222324252627282930313233343536void __local_bh_enable_ip(unsigned long ip, unsigned int cnt){ ...xxx}static inline void local_bh_enable_ip(unsigned long ip){ __local_bh_enable_ip(ip, SOFTIRQ_DISABLE_OFFSET);// 开启中断下半部的preempt_count 和 softirq相关++}//开启下半部---------------//禁止下半部static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt){ preempt_count_add(cnt); barrier();}static inline void __raw_spin_lock_bh(raw_spinlock_t *lock){ __local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET); // 禁止中断下半部的preempt_count 和 softirq相关++ spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);}void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock){ __raw_spin_lock_bh(lock);}#define raw_spin_lock_bh(lock) _raw_spin_lock_bh(lock)static __always_inline void spin_lock_bh(spinlock_t *lock){ raw_spin_lock_bh(&amp;lock-&gt;rlock);} 可以看出来执行软中断 和 禁止中断下半部都属于软中断上下文。（我有个疑问，禁止hard irq 属于硬件中断上下文吗？从 in_irq的定义上看不是，但是和软中断不太一样） process context进程上下文，当然不仅仅是进程，只要不是出于NMI、HARD IRQ、SOFT IRQ上下文的都算进程上下文，包括内核线程（包括ksoftirqd、kworker内核线程等） 12#define in_task() (!(preempt_count() &amp; \\ (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET))) 中断上下文不会发生进程切换，是一种隐式的进制调度方法。通常也可以使用 preempt_disable 来显式关闭调度，对 preempt_count ++。 atomic context处于中断上下文中(NMI、hard、soft) 或者 禁止抢占的情况下，都属于原子上下文 123456static __always_inline int preempt_count(void){ return READ_ONCE(current_thread_info()-&gt;preempt_count);}#define in_atomic() (preempt_count() != 0) 参考：Linux中的preempt_count","link":"/2020/09/21/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/%E5%85%B3%E4%BA%8Epreempt_count%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"内核线程退出不正确导致的问题","text":"什么是内核线程？内核线程就是一直运行在内核态的线程，与一般用户态进程的明显区别是 task-&gt;mm == NULL，就是没有自己的地址空间，所有的内核线程都共享一个地址空间 前段时间遇到一个模块卸载之后，crash的问题，我大概抽象了一下源码： 12345678910111213141516171819202122232425262728int run_flag = 0;struct my_struct {int a; char c;};struct my_struct a;static int les_test(void *arg){ while(run_flag){ msleep(1000); func(&amp;a);// 操作较为耗时 } return 0;}static int les_init(void){ run_flag = 1; task = kthread_run(les_test, NULL, &quot;les_test&quot;); return 0;}static void les_exit(void){ run_flag = 0; msleep(100);}module_init(les_init); module_exit(les_exit);MODULE_LICENSE(&quot;GPL&quot;); MODULE_AUTHOR(&quot;XXXX&quot;); 这个模块在insmod 之后work的很好，偶尔在rmmod的时候就会系统crash查看crash现场之后发现是 123456789101112131415161718192021222324252627282930 PANIC: &quot;Oops: 0010 [#1] SMP PTI&quot; (check log for details) PID: 4270 COMMAND: &quot;test&quot; TASK: ffff8e881d3b5f00 [THREAD_INFO: ffff8e881d3b5f00] CPU: 1 STATE: TASK_RUNNING (PANIC)crash&gt; btPID: 4270 TASK: ffff8e881d3b5f00 CPU: 1 COMMAND: &quot;test&quot; #0 [ffffb9c74092bb58] machine_kexec at ffffffff9546fa63 #1 [ffffb9c74092bbb8] __crash_kexec at ffffffff95557502 #2 [ffffb9c74092bc88] crash_kexec at ffffffff95558289 #3 [ffffb9c74092bca0] oops_end at ffffffff954353a9 #4 [ffffb9c74092bcc8] no_context at ffffffff9547efee #5 [ffffb9c74092bd38] __bad_area_nosemaphore at ffffffff9547f200 #6 [ffffb9c74092bd80] bad_area_nosemaphore at ffffffff9547f366 #7 [ffffb9c74092bd90] do_kern_addr_fault at ffffffff9547fd16 #8 [ffffb9c74092bdb8] __do_page_fault at ffffffff9547fdd7 #9 [ffffb9c74092bde0] do_page_fault at ffffffff9547fe0c#10 [ffffb9c74092be10] page_fault at ffffffff96001284 [exception RIP: unknown or invalid address] RIP: ffffffffc08500aa RSP: ffffb9c74092bec0 RFLAGS: 00010246 RAX: 0000000000000000 RBX: 00004b3f0220450c RCX: 0000000000000000 RDX: 0000000000000000 RSI: 0000000000000246 RDI: 0000000000000000 RBP: ffffb9c74092bf00 R8: 0000000000000000 R9: 0000000000000001 R10: 0000000000100000 R11: 0000000000000000 R12: 00004b3f02204508 R13: 00004b3f02206ac0 R14: 000000000002d020 R15: 0000000000000001 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018#11 [ffffb9c74092bf08] kthread at ffffffff954c7124#12 [ffffb9c74092bf50] ret_from_fork at ffffffff96000215 可以看到crash原因是 “Oops: 0010 [#1] SMP PTI”，出问题的test线程正是 内核模块起的线程，为什么内核模块卸载的时候这个test内核线程就出问题了呢? 其实仔细看一下module_exit的源码就会发现，这里用 run_flag 来同步线程是否运行，且 msleep(100)等待 test线程退出。 这种写法就看起来无比诡异 实验我首先将这个 msleep(100) 去掉之后重新编译安装，在rmmod的时候就几乎是100%必现内核crash的问题，这里msleep(100) 也应该是驱动开发者自己debug的时候加上的。看test线程代码我们发现在func(&amp;a)函数中，会使用一个全局变量a。这个操作也比较长，看调用栈也发现大概是这里crash了，再结合是卸载模块的时候crash，应该是可以想到，模块卸载之后模块的相关资源已经被释放，但是这个内核线程由于刚刚msleep(100)结束，并没有退出，此时变量a的资源已经被module_exit释放了，再去将&amp;a 传给func，后续执行crash也不意外了。 这个去掉module_exit 里面msleep(100)的操作，为什么就会导致几乎必现crash问题呢。加上module_exit 里面msleep(100)的操作，概率就会很低了，这是为什么呢。考虑加上msleep(100)的情况，run_flag 被置位，module_exit 会等待100ms再退出，这100ms内大概率test内核线程已经被wakeup起来，然后已经退出了，但是在某些场景下（系统有RT进程，或者load比较重的情况下），test线程刚刚醒来就又被抢占了，而module_exit会释放模块资源，等test线程在此醒来的时候，a的资源其实已经被释放了后续操作很有可能会导致crash。其中test进程何时会再次得到运行呢？答案是不确定的，取决于系统繁忙程度，但只要test一运行就会导致crash。 模块加载、模块卸载都干了啥 修改其实熟悉内核线程API接口的人看到这抽象的源码基本一眼就能看出端倪，内核线程的框架本来就不是这么用的修改之后源码如下 123456789101112131415161718192021222324252627int run_flag = 0;struct my_struct {int a; char c;};struct my_struct a;static int les_test(void *arg){ while(kthread_should_stop){ msleep(1000); func(&amp;a);// 操作较为耗时 } return 0;}static int les_init(void){ run_flag = 1; task = kthread_run(les_test, NULL, &quot;les_test&quot;); return 0;}static void les_exit(void){ kthread_stop(task);}module_init(les_init); module_exit(les_exit);MODULE_LICENSE(&quot;GPL&quot;); MODULE_AUTHOR(&quot;XXXX&quot;); 其中 kthread_stop会设置 kthread的 flag为 KTHREAD_SHOULD_STOP，然后等待 test 内核线程退出 1234567891011121314int kthread_stop(struct task_struct *k){ struct kthread *kthread; int ret; kthread = to_kthread(k); set_bit(KTHREAD_SHOULD_STOP, &amp;kthread-&gt;flags); kthread_unpark(k); wake_up_process(k); wait_for_completion(&amp;kthread-&gt;exited); ret = k-&gt;exit_code; return ret;} 关于内核线程API的实现后面再讲内核线程API","link":"/2020/09/06/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/%E5%86%85%E6%A0%B8%E7%BA%BF%E7%A8%8B%E9%80%80%E5%87%BA%E4%B8%8D%E6%AD%A3%E7%A1%AE%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"bpftrace","text":"bpf epbf bcc bpftrace 关系bpf 与 ebpf关系BPF 是 Berkeley Packet Filter简称，现在也被称为cBPF(classic BPF)，要用于 tcpdump 和 seccomp，BPF早在1992 年就诞生了。 eBPF 是Enahnced Berkeley Packet Filter简称，诞生于2011年。其实 eBPF 本身其实远没有想象中的复杂，我们可以把内核想象成一个庞大而复杂的电路，如果电路出现了异常，我们可以通过使用万用表测量电路，如果当前电路不能满足需求，但又不能推倒重新设计，我们需要串入其他元件。eBPF 之于电路，既是万用表，也是各种功能元件。再回到内核中的 eBPF，内核预先在各个关键路径埋设了 eBPF 程序入口，用户可以编写不同类型的 eBPF 程序，将 eBPF 程序 attach 在内核中不同路径中执行。 bcc 与 ebpfbcc 是 ebpf compile collection简称，是一个包含丰富的内核跟踪分析的 eBPF 工具集，使得 “编 写 BPF 代码-编译成字节码-注入内核-获取结果-展示” 整个过程更加便捷。 bpftrace 与 ebpfbpftrace 可以动态跟踪分析内核，bpftrace 提供了一种类 awk 和 C 的语言，使用 bpftrace 语言编写各种跟踪和分析脚本，并编译成 eBPF 字节码与内核交互，从而实现动态跟踪 Linux 内核。 总的来说bcc 和 bpftrace都是基于 ebpf特性实现的工具，实际使用中我发现bpftrace尤其好用相对于ftrace、perf等。后面会有相应的对比分析。 bpftrace一行代码告诉你性能问题在哪下面主要是 bpftrace 的一些使用方法 列出所有探测点，并且可以添加搜索项，也可以管道传递给grep 1&quot;bpftrace -l&quot; 观察文件open事件 1bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(&quot;%s %s\\n&quot;, comm, str(args-&gt;filename)); }' 进程系统调用数量统计 1bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }' read系统调用的分布/xxx/: 可以过滤{}中执行的条件，一般可以是进程名，pid等 1bpftrace -e 'tracepoint:syscalls:sys_exit_read /pid == 18644/ { @bytes = hist(args-&gt;ret); }' 分析内核实时函数栈profile:是采样 1bpftrace -e 'profile:hz:99 { @[kstack] = count(); }' 调度器跟踪 1bpftrace -e 'tracepoint:sched:sched_switch { @[kstack] = count(); }' 具体有哪些参数变量可以使用，可以参考 /sys/kernel/debug/tracing/events/ 里面具体某个跟踪项的 format 选项，里面列出了可以使用的变量，结构等 12345678910111213141516171819sh@ubuntu[root]:/sys/kernel/debug/tracing/events/sched/sched_switch# cat format name: sched_switchID: 323format: field:unsigned short common_type; offset:0; size:2; signed:0; field:unsigned char common_flags; offset:2; size:1; signed:0; field:unsigned char common_preempt_count; offset:3; size:1; signed:0; field:int common_pid; offset:4; size:4; signed:1; field:char prev_comm[16]; offset:8; size:16; signed:1; field:pid_t prev_pid; offset:24; size:4; signed:1; field:int prev_prio; offset:28; size:4; signed:1; field:long prev_state; offset:32; size:8; signed:1; field:char next_comm[16]; offset:40; size:16; signed:1; field:pid_t next_pid; offset:56; size:4; signed:1; field:int next_prio; offset:60; size:4; signed:1;print fmt: &quot;prev_comm=%s prev_pid=%d prev_prio=%d prev_state=%s%s ==&gt; next_comm=%s next_pid=%d next_prio=%d&quot;, REC-&gt;prev_comm, REC-&gt;prev_pid, REC-&gt;prev_prio, (REC-&gt;prev_state &amp; ((((0x0000 | 0x0001 | 0x0002 | 0x0004 | 0x0008 | 0x0010 | 0x0020 | 0x0040) + 1) &lt;&lt; 1) - 1)) ? __print_flags(REC-&gt;prev_state &amp; ((((0x0000 | 0x0001 | 0x0002 | 0x0004 | 0x0008 | 0x0010 | 0x0020 | 0x0040) + 1) &lt;&lt; 1) - 1), &quot;|&quot;, { 0x0001, &quot;S&quot; }, { 0x0002, &quot;D&quot; }, { 0x0004, &quot;T&quot; }, { 0x0008, &quot;t&quot; }, { 0x0010, &quot;X&quot; }, { 0x0020, &quot;Z&quot; }, { 0x0040, &quot;P&quot; }, { 0x0080, &quot;I&quot; }) : &quot;R&quot;, REC-&gt;prev_state &amp; (((0x0000 | 0x0001 | 0x0002 | 0x0004 | 0x0008 | 0x0010 | 0x0020 | 0x0040) + 1) &lt;&lt; 1) ? &quot;+&quot; : &quot;&quot;, REC-&gt;next_comm, REC-&gt;next_pid, REC-&gt;next_priosh@ubuntu[root]:/sys/kernel/debug/tracing/events/sched/sched_switch# 更多的需要去动手实践，在项目中灵活运用 bpftrace 这样的神器更多资料可以参考：bpftrace的一行代码中文指引brendan大神的博客bpftrace官方guide宋宝华大师的总结","link":"/2020/09/12/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/bpf%E7%9B%B8%E5%85%B3/bpftrace/"},{"title":"文件异常删除、修改","text":"实际工作中经常会遇到一些某个业务的重要的配置文件被删除了，或者某个重要的配置文件被串改了，而这个文件目录 往往又隐藏很深，不太可能被别的业务删除或者篡改，那么如何找到删除文件或者篡改文件的真凶呢。 首先删除一个文件我们一般会使用rm 来做，对应vfs层的就是 unlink方法，可以对unlink跟踪。这里当然可以使用 ftrace工具，但是我已经习惯使用 bpftrace了如下对 unlink的操作进行跟踪 123sh@ubuntu[root]:/home/rlk/workspace/test# bpftrace -e 'kprobe:vfs_unlink {printf(&quot;pid: [%d], comm: [%s]\\n&quot;, pid, comm)}'Attaching 1 probe...pid: [95301], comm: [rm] 这样的打印信息会很多，如果是在生产环境上，有用的信息往往来不及筛选就会被淹没掉，这时可以加上对删除文件名的限制过滤 123456789101112131415161718sh@ubuntu[root]:/sys/kernel/debug/tracing# cd events/syscalls/sys_enter_unlinkatsh@ubuntu[root]:/sys/kernel/debug/tracing/events/syscalls/sys_enter_unlinkat# lsenable filter format hist id triggersh@ubuntu[root]:/sys/kernel/debug/tracing/events/syscalls/sys_enter_unlinkat# cat format name: sys_enter_unlinkatID: 722format: field:unsigned short common_type; offset:0; size:2; signed:0; field:unsigned char common_flags; offset:2; size:1; signed:0; field:unsigned char common_preempt_count; offset:3; size:1; signed:0; field:int common_pid; offset:4; size:4; signed:1; field:int __syscall_nr; offset:8; size:4; signed:1; field:int dfd; offset:16; size:8; signed:0; field:const char * pathname; offset:24; size:8; signed:0; field:int flag; offset:32; size:8; signed:0;print fmt: &quot;dfd: 0x%08lx, pathname: 0x%08lx, flag: 0x%08lx&quot;, ((unsigned long)(REC-&gt;dfd)), ((unsigned long)(REC-&gt;pathname)), ((unsigned long)(REC-&gt;flag)) 可以看出 pathname 就是文件名，下面我们添加对文件名是asdf文件删除的监控，可以清晰的看到pid, comm, 删除的文件名 123sh@ubuntu[root]:/home/rlk/workspace/test# bpftrace -e 'tracepoint:syscalls:sys_enter_unlinkat /str(args-&gt;pathname) == &quot;asdf&quot;/ {printf(&quot;pid: [%d], comm: [%s], rm [%s]\\n&quot;, pid, comm, str(args-&gt;pathname))}'Attaching 1 probe...pid: [95566], comm: [rm], rm [asdf] 这里需要注意 str(args-&gt;pathname) 的用法 这里我们使用的是tracepoint来跟踪的，如果你的机器上支持BTF，使用kprobe来跟踪也是可以的，过滤相关条件的时候比较复杂。 同理记录篡改文件也很好办，可以跟踪有哪些进程写入了这个文件或者打开了这个文件，都是有嫌疑的。","link":"/2020/09/14/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/bpf%E7%9B%B8%E5%85%B3/%E6%96%87%E4%BB%B6%E5%BC%82%E5%B8%B8%E5%88%A0%E9%99%A4%E3%80%81%E4%BF%AE%E6%94%B9/"},{"title":"ftrace可以看的几个参数","text":"Ftrace是一个内部跟踪器，旨在帮助系统的开发人员和设计人员查找内核内部发生的情况。 它可以用于调试或分析在用户空间之外发生的延迟和性能问题。 尽管通常将ftrace视为函数跟踪器，但实际上它是多个分类跟踪实用程序的框架。 可以进行延迟跟踪，以检查禁用和启用中断之间的情况以及抢占以及从唤醒任务到计划任务的时间。 ftrace的最常见用途之一是事件跟踪。 整个内核中有数百个静态事件点，可以通过tracefs文件系统启用这些事件点，以查看内核某些部分的情况。 irqsoff我们都知道linux执行中断的时候都是关中断的，也不存在什么中断嵌套，中断被禁时CPU无法响应任何其他外部事件（除了NMI和SMI）。万一某个驱动开发者代码没有注意将临界区设置的比较大，或者中断处理函数中执行了较多内容，就会直接导致调度延迟增大，直接表现为业务抖动较大，我们有没有什么手段观测这个关中断的时间长短呢?答案是肯定的： 12sh@ubuntu[root]:/sys/kernel/debug/tracing# cat /sys/kernel/debug/tracing/available_tracers hwlat blk mmiotrace function_graph wakeup_dl wakeup_rt wakeup irqsoff function nop irqsoff 这个 tracer 就是来完成这个使命的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647sh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/debug/tracing/tracing_on sh@ubuntu[root]:~# echo irqsoff &gt; /sys/kernel/debug/tracing/current_tracersh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/debug/tracing/tracing_max_latency sh@ubuntu[root]:~# echo 1 &gt; /sys/kernel/debug/tracing/tracing_on sh@ubuntu[root]:~# sleep 5sh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/debug/tracing/tracing_on sh@ubuntu[root]:~# cat /sys/kernel/debug/tracing/trace | head -n 30# tracer: irqsoff## irqsoff latency trace v1.1.5 on 5.4.44# --------------------------------------------------------------------# latency: 769 us, #15/15, CPU#2 | (M:desktop VP:0, KP:0, SP:0 HP:0 #P:4)# -----------------# | task: kworker/2:0-5528 (uid:0 nice:0 policy:0 rt_prio:0)# -----------------# =&gt; started at: e1000_update_stats# =&gt; ended at: e1000_update_stats### _------=&gt; CPU# # / _-----=&gt; irqs-off # | / _----=&gt; need-resched # || / _---=&gt; hardirq/softirq # ||| / _--=&gt; preempt-depth # |||| / delay # cmd pid ||||| time | caller # \\ / ||||| \\ | / kworker/-5528 2d... 0us!: _raw_spin_lock_irqsave &lt;-e1000_update_statskworker/-5528 2d... 609us : e1000_read_phy_reg &lt;-e1000_update_statskworker/-5528 2d... 610us : _raw_spin_lock_irqsave &lt;-e1000_read_phy_regkworker/-5528 2d... 620us : __const_udelay &lt;-e1000_read_phy_regkworker/-5528 2d... 620us+: delay_tsc &lt;-__const_udelaykworker/-5528 2d... 678us : _raw_spin_unlock_irqrestore &lt;-e1000_read_phy_regkworker/-5528 2d... 679us : e1000_read_phy_reg &lt;-e1000_update_statskworker/-5528 2d... 679us : _raw_spin_lock_irqsave &lt;-e1000_read_phy_regkworker/-5528 2d... 687us : __const_udelay &lt;-e1000_read_phy_regkworker/-5528 2d... 688us+: delay_tsc &lt;-__const_udelaykworker/-5528 2d... 770us : tracer_hardirqs_on &lt;-e1000_update_statskworker/-5528 2d... 774us : &lt;stack trace&gt; =&gt; e1000_update_stats =&gt; e1000_watchdog =&gt; process_one_work =&gt; worker_thread =&gt; kthread =&gt; ret_from_forksh@ubuntu[root]:~# cat /sys/kernel/debug/tracing/tracing_max_latency 769 可以看到 tracing_max_latency 是 769，意味着系统最长关中断时间就是 769us.(要重置最大值，需要将0回显到tracing_max_latency中) 这是未设置函数跟踪的结果 可以 echo 1 &gt; /sys/kernel/debug/tracing/options/function-trace 获取更详细输出 preemptoff禁用抢占功能后，我们可能会收到中断，但无法抢占该任务，优先级较高的任务必须等待再次启用抢占功能，才能抢占优先级较低的任务。 preemptoff跟踪程序将跟踪禁用抢占的位置。 与irqsoff跟踪器一样，它记录禁用了抢占的最大延迟。 preemptoff跟踪器的控制与irqsoff跟踪器非常相似。 1234567891011121314151617181920212223242526272829303132333435# echo preemptoff &gt; current_tracer# echo 1 &gt; tracing_on# echo 0 &gt; tracing_max_latency# ls -ltr[...]# echo 0 &gt; tracing_on# cat trace# tracer: preemptoff## preemptoff latency trace v1.1.5 on 3.8.0-test+# --------------------------------------------------------------------# latency: 46 us, #4/4, CPU#1 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:4)# -----------------# | task: sshd-1991 (uid:0 nice:0 policy:0 rt_prio:0)# -----------------# =&gt; started at: do_IRQ# =&gt; ended at: do_IRQ### _------=&gt; CPU## / _-----=&gt; irqs-off# | / _----=&gt; need-resched# || / _---=&gt; hardirq/softirq# ||| / _--=&gt; preempt-depth# |||| / delay# cmd pid ||||| time | caller# \\ / ||||| \\ | / sshd-1991 1d.h. 0us+: irq_enter &lt;-do_IRQ sshd-1991 1d..1 46us : irq_exit &lt;-do_IRQ sshd-1991 1d..1 47us+: trace_preempt_on &lt;-do_IRQ sshd-1991 1d..1 52us : &lt;stack trace&gt; =&gt; sub_preempt_count =&gt; irq_exit =&gt; do_IRQ =&gt; ret_from_intr preemptirqsoffpreemptirqsoff 是以上两者的和，会显示禁止 抢占或者中断的最长时间比如 1234567local_irq_disable();call_function_with_irqs_off();preempt_disable();call_function_with_irqs_and_preemption_off();local_irq_enable();call_function_with_preemption_off();preempt_enable(); wakeup、wakeup_rtwakeup、wakeup_rt 是 trace一个进程从进入CPU的就绪队列到真正被执行的时间的一个tracer，即跟踪调度器的延迟。 参考ftrace 内核文档","link":"/2020/09/19/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/ftrace/ftrace%E5%8F%AF%E4%BB%A5%E7%9C%8B%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%82%E6%95%B0/"},{"title":"vmtouch 观测文件page cache","text":"vmtouch 介绍便携式文件系统缓存诊断和控制 是 vmtouch 作者对于vmtouch的的定义。首先他可以很方便的知道某一个文件当前有多少在 kernel memory 里面作为pagecache 存在。 123456789101112131415ubuntu@ubuntu-Inspiron-5548:~$ vmtouch haha Files: 1 Directories: 0 Resident Pages: 11584/1024000 45M/3G 1.13% Elapsed: 0.031335 secondsubuntu@ubuntu-Inspiron-5548:~$ubuntu@ubuntu-Inspiron-5548:~$ubuntu@ubuntu-Inspiron-5548:~$ cat haha^Cubuntu@ubuntu-Inspiron-5548:~$ vmtouch haha Files: 1 Directories: 0 Resident Pages: 36800/1024000 143M/3G 3.59% Elapsed: 0.033472 secondsubuntu@ubuntu-Inspiron-5548:~$ 明显可以看到在经过 cat 访问之后 文件更多部分被读入 memory，作为 pagecache。 作者对于他的功能介绍： 12345671. 发现你的操作系统正在缓存哪些文件2. 告诉操作系统缓存或清除某些文件或文件区域3. 将文件锁定在内存中，这样操作系统就不会删除它们4. 在服务器故障转移时保留虚拟内存配置文件5. 保持“热备”文件服务器6. 绘制文件系统缓存随时间的使用情况7. 维护缓存使用的“软配额” vmtouch 使用控制增加 pagecache可以将整个文件读入内存，其实我们通过访问这个文件（从头到尾）也可以做到 12345678910111213141516171819202122232425262728ubuntu@ubuntu-Inspiron-5548:~$ vmtouch -vt hahahaha[OOo ] 44737/1024000[OOOOOo ] 90849/1024000[OOOOOOOOo ] 150209/1024000[OOOOOOOOOOOOo ] 215937/1024000[OOOOOOOOOOOOOOOo ] 271489/1024000[OOOOOOOOOOOOOOOOOOOo ] 326305/1024000[OOOOOOOOOOOOOOOOOOOOOo ] 370721/1024000[OOOOOOOOOOOOOOOOOOOOOOOOo ] 414305/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 469697/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 524289/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 577153/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 652481/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 705953/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 757281/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 810593/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 876769/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 920545/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 973473/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 1024000/1024000 Files: 1 Directories: 0 Touched Pages: 1024000 (3G) Elapsed: 9.4376 secondsubuntu@ubuntu-Inspiron-5548:~$ 控制减少 pagecacheevict a file from memory将一个文件的pagecache 从内存中移除 1234567ubuntu@ubuntu-Inspiron-5548:~$ vmtouch -ve hahaEvicting haha Files: 1 Directories: 0 Evicted Pages: 1024000 (3G) Elapsed: 0.085208 seconds 与drop cache不同，vmtouch做到了精准控制单个文件page_cache的效果，而 drop cache不行 12root@ubuntu-Inspiron-5548:/home/ubuntu# echo 1 &gt; /proc/sys/vm/drop_cachesroot@ubuntu-Inspiron-5548:/home/ubuntu# echo 3 &gt; /proc/sys/vm/drop_caches 保持文件在pagecache中1234ubuntu@ubuntu-Inspiron-5548:~$ vmtouch -dl hahaubuntu@ubuntu-Inspiron-5548:~$ vmtouch: FATAL: mlock: haha (Cannot allocate memory)ubuntu@ubuntu-Inspiron-5548:~$ (内存4G，文件4G 是没办法将文件常驻在 内存中的)从此报错信息可以看出 vmtouch 也是通过 mlock 系统调用来实现 文件内容 或者 文件目录内容锁定在 内存中的 vmtouch 作者的文章","link":"/2021/01/07/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/tools/vmtouch%20%E8%A7%82%E6%B5%8B%E6%96%87%E4%BB%B6page%20cache/"},{"title":"60s定位内核性能瓶颈","text":"这篇主要是翻译Brendan的文章perf in 60s和自己见解。主要是在linux性能分析前60s，你应该做什么。 uptime通过 uptime 可以看到系统的最近1min,5min,15min的平均负载load情况，可以知道此时系统的繁忙情况，也可以看出一段时间内系统变得 busier 或者 quiter. 12sh@ubuntu[root]:~# uptime 02:29:52 up 6 days, 14:34, 2 users, load average: 0.00, 0.01, 0.00 dmesg | tail通过 dmesg | tail 可以看到系统的最近有没有有用的错误日志信息 1234567891011sh@ubuntu[root]:~# dmesg | tail[527247.600091] e1000: ens33 NIC Link is Down[527259.696854] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534840.083000] e1000: ens33 NIC Link is Down[534844.115753] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534846.131190] e1000: ens33 NIC Link is Down[534850.162855] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534906.610317] e1000: ens33 NIC Link is Down[534910.643119] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534912.657953] e1000: ens33 NIC Link is Down[534916.690646] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None vmstat通过 vmstat 可以看到系统一个overall的视图，包括 CPU MEM IO Inturrupts 123456sh@ubuntu[root]:~# vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 590592 150108 202604 766576 0 0 6 3 13 15 0 0 100 0 0 0 0 590592 150108 202604 766576 0 0 0 0 289 523 0 0 100 0 0 0 0 590592 150068 202604 766616 0 0 0 0 320 581 0 0 100 0 0 mpstat 通过 mpstat -P ALL 1 可以看到系统的每个 CPU的状态，看看CPU使用是否balance，有时候会有线程绑核出现一些问题 12345678910111213141516sh@ubuntu[root]:~# mpstat -P ALL 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时36分15秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle02时36分16秒 all 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.00 99.7502时36分16秒 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分16秒 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分16秒 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分16秒 3 0.00 0.00 0.00 0.00 0.00 0.99 0.00 0.00 0.00 99.0102时36分16秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle02时36分17秒 all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 pidstat通过 pidstat 1 可以看到系统中最繁忙的几个进程 12345678910111213141516sh@ubuntu[root]:~# pidstat 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时38分37秒 UID PID %usr %system %guest %wait %CPU CPU Command02时38分38秒 0 943 0.93 0.93 0.00 0.00 1.87 0 containerd02时38分38秒 1000 3282 0.00 0.93 0.00 0.00 0.93 3 sshd02时38分38秒 0 78276 0.00 1.87 0.00 0.00 1.87 2 pidstat02时38分38秒 UID PID %usr %system %guest %wait %CPU CPU Command02时38分39秒 1000 15201 0.00 1.00 0.00 0.00 1.00 0 node02时38分39秒 0 77927 0.00 1.00 0.00 0.00 1.00 2 kworker/2:0-events02时38分39秒 0 78276 0.00 1.00 0.00 0.00 1.00 2 pidstat02时38分39秒 UID PID %usr %system %guest %wait %CPU CPU Command02时38分40秒 1000 3347 0.00 1.00 0.00 0.00 1.00 0 node02时38分40秒 0 78276 1.00 0.00 0.00 0.00 1.00 2 pidstat iostat通过 iostat -xz 1 可以看到系统中disk io的状态 12345678910111213141516171819202122232425sh@ubuntu[root]:~# iostat -xz 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.05 0.01 0.09 0.01 0.00 99.84Device r/s rkB/s rrqm/s %rrqm r_await rareq-sz w/s wkB/s wrqm/s %wrqm w_await wareq-sz d/s dkB/s drqm/s %drqm d_await dareq-sz aqu-sz %utilloop0 0.00 0.00 0.00 0.00 0.12 1.48 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop1 0.00 0.00 0.00 0.00 0.11 2.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop2 0.04 0.04 0.00 0.00 0.45 1.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop3 0.01 0.01 0.00 0.00 0.24 1.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop4 0.00 0.00 0.00 0.00 0.15 7.37 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop5 0.02 0.02 0.00 0.00 0.49 1.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop6 0.00 0.00 0.00 0.00 0.13 1.89 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop7 0.05 0.05 0.00 0.00 0.41 1.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop8 0.05 0.05 0.00 0.00 0.67 1.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop9 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sda 1.00 22.93 0.36 26.65 0.56 23.03 0.65 10.07 0.60 47.67 0.36 15.39 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.06avg-cpu: %user %nice %system %iowait %steal %idle 0.00 0.00 0.25 0.00 0.00 99.75Device r/s rkB/s rrqm/s %rrqm r_await rareq-sz w/s wkB/s wrqm/s %wrqm w_await wareq-sz d/s dkB/s drqm/s %drqm d_await dareq-sz aqu-sz %utilsda 2.00 12.00 0.00 0.00 0.50 6.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 free通过 free 可以看到系统中内存的使用情况1234sh@ubuntu[root]:~# free total used free shared buff/cache availableMem: 3805660 2729828 158612 3108 917220 784960Swap: 2097148 595968 1501180 sar通过 sar -n DEV 1 可以看出系统中网络收发是否有问题通过 sar -n TCP,ETCP 1 可以看出系统中TCP连接是否有问题 1234567891011121314sh@ubuntu[root]:~# sar -n DEV 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时44分23秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil02时44分24秒 ens33 4.00 5.00 0.49 0.58 0.00 0.00 0.00 0.0002时44分24秒 lo 11.00 11.00 0.91 0.91 0.00 0.00 0.00 0.0002时44分24秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil02时44分25秒 ens33 7.00 9.00 0.68 1.56 0.00 0.00 0.00 0.0002时44分25秒 lo 18.00 18.00 1.93 1.93 0.00 0.00 0.00 0.0002时44分25秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil02时44分26秒 ens33 2.00 3.00 0.23 0.96 0.00 0.00 0.00 0.0002时44分26秒 lo 6.00 6.00 1.10 1.10 0.00 0.00 0.00 0.00 1234567891011121314sh@ubuntu[root]:~# sar -n TCP,ETCP 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时45分45秒 active/s passive/s iseg/s oseg/s02时45分46秒 0.00 0.00 16.00 16.0002时45分45秒 atmptf/s estres/s retrans/s isegerr/s orsts/s02时45分46秒 0.00 0.00 0.00 0.00 0.0002时45分46秒 active/s passive/s iseg/s oseg/s02时45分47秒 0.00 0.00 16.00 18.0002时45分46秒 atmptf/s estres/s retrans/s isegerr/s orsts/s02时45分47秒 0.00 0.00 0.00 0.00 0.00 top如果一般没啥问题，一般会以top结尾1234567891011121314top - 02:47:39 up 6 days, 14:52, 2 users, load average: 0.16, 0.07, 0.01Tasks: 341 total, 1 running, 340 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.0 us, 1.5 sy, 0.0 ni, 98.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 3716.5 total, 149.4 free, 2670.2 used, 896.9 buff/cacheMiB Swap: 2048.0 total, 1466.0 free, 582.0 used. 762.2 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 168136 8196 5688 S 0.0 0.2 0:17.47 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.61 kthreadd 3 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_gp 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_par_gp 6 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H-kblockd 9 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 mm_percpu_wq 10 root 20 0 0 0 0 S 0.0 0.0 0:00.42 ksoftirqd/0","link":"/2020/09/13/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/60s%E5%AE%9A%E4%BD%8D%E5%86%85%E6%A0%B8%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/"},{"title":"快速消耗完物理内存","text":"题目之前遇到一个面试，是写一个快速消耗完物理内存的代码 需要注意的是是物理内存，很多不了解内核的同学往往会忽视这一点主要考察的是linux 内存分配的延迟分配策略 内存分配的延迟分配主要是说linux 应用程序在通过brk系统调用向linux内核申请内存时，都是申请的虚拟内存，只有在这个内存真正被使用（访问）的时候，kernel会发生缺页中断，进而去分配物理内存。 解答那这个问题就变成了，分配很多内存，且分配之后去访问一下，让内核产生缺页中断，给申请的虚拟内存分配物理内存 12345678910111213141516#include &lt;stdio.h&gt;int main(int argc, char **argv){ int *p = NULL; int i = 0, j = 0; for (i = 0; i &lt; (1 &lt;&lt; 30); i++) { for (j = 0; i &lt; (1 &lt;&lt; 30); j++) { p = (int *)malloc(4096); p = 1; } } return 0;} 我在ubuntu20.04上运行这个代码之后，几秒钟后这个a.out就被oomkill干掉了dmesg 最后打印的消息是 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[992416.124437] node invoked oom-killer: gfp_mask=0x100cca(GFP_HIGHUSER_MOVABLE), order=0, oom_score_adj=0[992416.128825] CPU: 1 PID: 44301 Comm: node Kdump: loaded Not tainted 5.4.44 #1[992416.128825] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[992416.128826] Call Trace:[992416.144917] dump_stack+0x6d/0x9a[992416.145738] dump_header+0x4f/0x1eb[992416.145742] oom_kill_process.cold+0xb/0x10[992416.145743] out_of_memory+0x1bc/0x490[992416.145777] __alloc_pages_slowpath+0xd5e/0xe50[992416.145793] ? __switch_to_asm+0x40/0x70[992416.145795] __alloc_pages_nodemask+0x2d0/0x320[992416.145811] alloc_pages_current+0x87/0xe0[992416.146058] __page_cache_alloc+0x72/0x90[992416.146061] pagecache_get_page+0xbf/0x300[992416.146062] filemap_fault+0x69a/0xa40[992416.146143] ? unlock_page_memcg+0x12/0x20[992416.146166] ? page_add_file_rmap+0xff/0x1a0[992416.146181] ? xas_load+0xd/0x80[992416.146182] ? xas_find+0x17f/0x1c0[992416.146184] ? filemap_map_pages+0x24c/0x380[992416.146458] ext4_filemap_fault+0x32/0x46[992416.146475] __do_fault+0x3c/0x130[992416.146477] __handle_mm_fault+0xff5/0x1700[992416.146723] ? hrtimer_init_sleeper+0x90/0x90[992416.146726] handle_mm_fault+0xca/0x200[992416.147382] do_user_addr_fault+0x1f9/0x450[992416.147385] __do_page_fault+0x58/0x90[992416.147386] do_page_fault+0x2c/0xe0[992416.147387] page_fault+0x34/0x40[992416.147623] RIP: 0033:0x7f9a73ac1210[992416.147628] Code: Bad RIP value.[992416.147629] RSP: 002b:00007ffc77fb2448 EFLAGS: 00010213[992416.147646] RAX: 00007f9a739de698 RBX: 000000000000000b RCX: 00007f9a73b03266[992416.147646] RDX: 0000000000000400 RSI: 00007ffc77fb24a0 RDI: 0000000000000001[992416.147974] RBP: 00007ffc77fb5560 R08: 0000000000000000 R09: 0000000000000008[992416.147975] R10: 0000000000001388 R11: 0000000000000000 R12: 0000000000001388[992416.147976] R13: 0000000000000000 R14: 0000000002887f98 R15: 0000000002887f40[992416.148030] Mem-Info:[992416.148188] active_anon:478453 inactive_anon:255010 isolated_anon:0 active_file:161 inactive_file:27 isolated_file:0 unevictable:16 dirty:0 writeback:0 unstable:0 slab_reclaimable:26060 slab_unreclaimable:48649 mapped:35873 shmem:35877 pagetables:8285 bounce:0 free:21419 free_pcp:0 free_cma:0[992416.148190] Node 0 active_anon:1913812kB inactive_anon:1020040kB active_file:644kB inactive_file:108kB unevictable:64kB isolated(anon):0kB isolated(file):0kB mapped:143492kB dirty:0kB writeback:0kB shmem:143508kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB writeback_tmp:0kB unstable:0kB all_unreclaimable? no[992416.148191] Node 0 DMA free:14928kB min:284kB low:352kB high:420kB active_anon:748kB inactive_anon:104kB active_file:0kB inactive_file:0kB unevictable:0kB writepending:0kB present:15988kB managed:15904kB mlocked:0kB kernel_stack:0kB pagetables:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[992416.148193] lowmem_reserve[]: 0 2772 3665 3665 3665[992416.148194] Node 0 DMA32 free:54416kB min:50896kB low:63620kB high:76344kB active_anon:1478944kB inactive_anon:856680kB active_file:20kB inactive_file:416kB unevictable:0kB writepending:0kB present:3129152kB managed:2867008kB mlocked:0kB kernel_stack:10032kB pagetables:26552kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[992416.148196] lowmem_reserve[]: 0 0 893 893 893[992416.148196] Node 0 Normal free:16332kB min:16400kB low:20500kB high:24600kB active_anon:434120kB inactive_anon:163256kB active_file:476kB inactive_file:412kB unevictable:64kB writepending:0kB present:1048576kB managed:922756kB mlocked:64kB kernel_stack:7056kB pagetables:6588kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[992416.148198] lowmem_reserve[]: 0 0 0 0 0[992416.148199] Node 0 DMA: 8*4kB (UM) 6*8kB (UM) 6*16kB (U) 7*32kB (UM) 1*64kB (U) 1*128kB (M) 0*256kB 0*512kB 2*1024kB (UM) 0*2048kB 3*4096kB (ME) = 14928kB[992416.148202] Node 0 DMA32: 2340*4kB (UME) 1077*8kB (UME) 606*16kB (UME) 419*32kB (UME) 180*64kB (UME) 18*128kB (UME) 1*256kB (M) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 55160kB[992416.148205] Node 0 Normal: 1046*4kB (UME) 629*8kB (UME) 246*16kB (UME) 110*32kB (UME) 2*64kB (M) 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 16800kB[992416.148413] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB[992416.148413] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB[992416.148414] 43441 total pagecache pages[992416.148415] 7332 pages in swap cache[992416.148415] Swap cache stats: add 540973, delete 533621, find 171870/177923[992416.148416] Free swap = 0kB[992416.148416] Total swap = 2097148kB[992416.148416] 1048429 pages RAM[992416.148417] 0 pages HighMem/MovableOnly[992416.148417] 97012 pages reserved[992416.148417] 0 pages cma reserved[992416.148417] 0 pages hwpoisoned[992416.148418] Tasks state (memory values in pages):[992416.148418] [ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name[992416.148431] [ 348] 0 348 13050 18 122880 480 -250 systemd-journal[992416.148448] [ 381] 0 381 5900 117 65536 707 -1000 systemd-udevd#省略了很多进程内存信息的打印..............[992416.148679] [ 140627] 1000 140627 5180 168 73728 68 0 top[992416.148680] [ 140709] 1000 140709 4177 0 61440 22 0 sleep[992416.148681] [ 140711] 1000 140711 687213 440843 5550080 243091 0 a.out[992416.148682] oom-kill:constraint=CONSTRAINT_NONE,nodemask=(null),cpuset=/,mems_allowed=0,global_oom,task_memcg=/user.slice/user-1000.slice/session-4.scope,task=a.out,pid=140711,uid=1000[992416.148857] Out of memory: Killed process 140711 (a.out) total-vm:2748852kB, anon-rss:1763368kB, file-rss:4kB, shmem-rss:0kB, UID:1000 pgtables:5420kB oom_score_adj:0[992416.268209] oom_reaper: reaped process 140711 (a.out), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB 在OOM的时候，kernel将内核栈都print出来了，原因是 __alloc_pages_slowpath 之后没有成功，然后就 out_of_memory 启动了大杀器 OOMkill process. Oom Killer 杀进程策略在这个例子中，看到最后杀死的进程是 a.out 1[992416.148857] Out of memory: Killed process 140711 (a.out) 为什么是a.out被杀呢？可以看一下 out_of_memory 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344unsigned long oom_badness(struct task_struct *p, unsigned long totalpages){ long points; long adj; if (oom_unkillable_task(p)) // init 进程 和内核线程不能被选中 return 0; adj = (long)p-&gt;signal-&gt;oom_score_adj; /* * The baseline for the badness score is the proportion of RAM that each * task's rss, pagetable and swap space use. */ points = get_mm_rss(p-&gt;mm) + get_mm_counter(p-&gt;mm, MM_SWAPENTS) + mm_pgtables_bytes(p-&gt;mm) / PAGE_SIZE; // 考虑进程的 FILE ANON SHMEM SWAP 分区占用 和 页表PTE 占用的内存}static int oom_evaluate_task(struct task_struct *task, void *arg){ points = oom_badness(task, oc-&gt;totalpages);}static void select_bad_process(struct oom_control *oc){ struct task_struct *p; rcu_read_lock(); for_each_process(p) if (oom_evaluate_task(p, oc)) break; rcu_read_unlock();}bool out_of_memory(struct oom_control *oc){ check_panic_on_oom(oc); select_bad_process(oc); if (oc-&gt;chosen &amp;&amp; oc-&gt;chosen != (void *)-1UL) oom_kill_process(oc, !is_memcg_oom(oc) ? &quot;Out of memory&quot; : &quot;Memory cgroup out of memory&quot;); return !!oc-&gt;chosen;} select_bad_process 会选择一个最坏的一个 process，这个坏怎么定义呢，可以看到 oom_badness 主要是针对用户进程的 FILE ANON SHMEM SWAP 分区占用 和 页表PTE 占用的内存 做了一个评估 和 事先设置的 p-&gt;signal-&gt;oom_score_adj 值所以一般不建议 给一般进程设置 很小的 p-&gt;signal-&gt;oom_score_adj值，这样会导致 进程在内存泄露时 无法被OOM选中，从而误杀其他进程。 一般触发OOM有两种： 121. pagefault_out_of_memory --&gt; out_of_memory2. __alloc_pages_slowpath --&gt; __alloc_pages_may_oom --&gt; out_of_memory 我们刚刚遇到的这次就是 读入page cache 需要分配内存，内存不够导致的一次OOM 123456789101112131415[992416.146062] filemap_fault+0x69a/0xa40[992416.146143] ? unlock_page_memcg+0x12/0x20[992416.146166] ? page_add_file_rmap+0xff/0x1a0[992416.146181] ? xas_load+0xd/0x80[992416.146182] ? xas_find+0x17f/0x1c0[992416.146184] ? filemap_map_pages+0x24c/0x380[992416.146458] ext4_filemap_fault+0x32/0x46[992416.146475] __do_fault+0x3c/0x130[992416.146477] __handle_mm_fault+0xff5/0x1700[992416.146723] ? hrtimer_init_sleeper+0x90/0x90[992416.146726] handle_mm_fault+0xca/0x200[992416.147382] do_user_addr_fault+0x1f9/0x450[992416.147385] __do_page_fault+0x58/0x90[992416.147386] do_page_fault+0x2c/0xe0[992416.147387] page_fault+0x34/0x40 关于OOM 还有很多需要注意的，就不在这里细说了","link":"/2020/09/05/memory/%E9%9D%A2%E8%AF%95/%E5%86%99%E4%B8%80%E4%B8%AA%E6%B6%88%E8%80%97%E5%AE%8C%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%9A%84%E7%A8%8B%E5%BA%8F/%E6%B6%88%E8%80%97%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98/"},{"title":"SystemTap examples","text":"SystemTap脚本集锦本章列举了若干可用于监控和调查内核子系统的SystemTap脚本。所有这些示例都能在 centos的 /usr/share/systemtap/testsuite/systemtap.examples/下找到。 SystemTap脚本集锦SystemTap脚本集锦 解读错误信息解读错误信息 参考SystemTap 内核文档 参考 RedHat systemTap 文档 参考 文档","link":"/2021/01/07/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/systemTap/examples/SystemTap%20examples/"},{"title":"How to Use system Tap","text":"SystemTap允许使用者监控Linux系统当前的运行情况，以便进一步分析。这将有助于运维或开发人员缉查bug或性能问题的罪魁祸首。SystemTap提供了一门领域特定语言，使得用户可以编写自定义脚本，调查和监控各种内核函数、系统调用，和其它发生在内核空间的事件。 就此而言，SystemTap不仅仅是个工具，它是一个让你能够自定义内核取证和监控工具的生态系统。 当前版本的SystemTap提供的探测内核空间事件的众多选项，可以在不同版本的内核下使用。然而，SystemTap对探测用户空间事件的支持依赖于内核的支持（需要uprobe机制），而多数内核缺乏这一支持。结果是，仅有部分内核上的SystemTap版本支持用户空间探测。 安装SystemTap安装 systemtap 软件 12Inspiron-5548@ubuntu: /var/crash# sudo apt install systemtapInspiron-5548@ubuntu: /var/crash# sudo apt install systemtap-runtime 安装 kernel debug info 12Inspiron-5548@ubuntu: /var/crash# sudo apt install systemtapInspiron-5548@ubuntu: /var/crash# sudo apt install systemtap-runtime SystemTap 可以用来干什么SystemTap允许用户仅需编写和重用简单的脚本即可获取Linux繁多的运行数据。通过SystemTap脚本，你可以又好又快地提取数据、过滤数据、汇总数据。诊断复杂的性能问题（或功能问题）再也不是难事。整个SystemTap脚本所做的，无非就是声明感兴趣的事件，然后添加对应的处理程序。当SystemTap脚本运行时，SystemTap会监控声明的事件；一旦事件发生，Linux内核会临时切换到对应的处理程序，完成后再重拾原先的工作。 12可供监控的事件种类繁多：进入/退出某个函数，定时器到期，会话终止，等等。处理程序由一组SystemTap语句构成，指明事件发生后要做的工作。其中包括从事件上下文中提取数据，存储到内部变量中，输出结果。 SystemTap 使用最简单的一行代码123456789Inspiron-5548@127ubuntu: ~/workspace# echo &quot;probe timer.s(1) {exit()}&quot; | sudo stap -v -Pass 1: parsed user script and 476 library scripts using 108312virt/90968res/7440shr/83392data kb, in 260usr/30sys/315real ms.Pass 2: analyzed script: 1 probe, 1 function, 0 embeds, 0 globals using 109896virt/92776res/7688shr/84976data kb, in 10usr/0sys/9real ms.Pass 3: translated to C into &quot;/tmp/stappbkhvF/stap_629b1ee8abda600005ad17f270124c66_947_src.c&quot; using 110032virt/92776res/7688shr/85112data kb, in 0usr/0sys/1real ms.Pass 4: compiled C into &quot;stap_629b1ee8abda600005ad17f270124c66_947.ko&quot; in 15530usr/2160sys/17963real ms.Pass 5: starting run.Pass 5: run completed in 20usr/30sys/1459real ms.Inspiron-5548@ubuntu: ~/workspace# 可以看出 SystemTap 脚本运行需要结果5个步骤，在加载 SystemTap脚本过程(生成ko)的时候，SystemTap 耗时较多，尤其是CPU资源。 SystemTap脚本运行时，会启动一个对应的SystemTap会话。整个会话大致流程如下： 首先，SystemTap会检查脚本中用到的tapset，确保它们都存在于tapset库中（通常是/usr/share/systemtap/tapset/）。然后SystemTap会把找到的tapset替换成在tapset库中对应的定义。tapset是tap（听诊器）的集合，指一些预定义的SystemTap事件或函数。完整的tapset列表 SystemTap接着会把脚本转化成C代码，运行系统的C编译器编译出一个内核模块。完成这一步的工具包含在systemtap包中SystemTap随即加载该模块，并启用脚本中所有的探针（包括事件和对应的处理程序）。这一步由system-runtime包的staprun完成。每当被监控的事件发生，对应的处理程序就会被执行。一旦SystemTap会话终止，探针会被禁用，内核模块也会被卸载。这一串流程皆始于一个简单的命令行程序：stap。这个程序包揽了SystemTap主要的功能。 脚本如何编写在大多数情况下，SystemTap脚本是每个SystemTap会话的基石。SystemTap脚本决定了需要收集的信息类型，也决定了对收集到的信息的处理方式。SystemTap脚本由两部分组成：事件和处理程序。一旦SystemTap会话准备就绪，SystemTap会监控操作系统中特定的事件，并在事件发生的时候触发对应的处理程序。 1一个事件和它对应的处理程序合称探针。一个SystemTap脚本可以有多个探针。 一个探针的处理程序部分通常称之为探针主体（probe body） 与应用开发的方式类比，使用事件和处理程序就像在程序的特定位置插入打日志的语句。每当程序运行时，这些日志会帮助你查看程序执行的流程。但是SystemTasp脚本允许你在无需重新编译代码，即可插入检测指令，而且处理程序也不限于单纯地打印数据。事件会触发对应的处理程序；对应的处理程序记录下感兴趣的数据，并以你指定的格式输出。 SystemTap脚本的后缀是.stp，并以这样的语句表示一个探针： 1probe event {statements} SystemTap支持给一个探针指定多个事件；每个事件以逗号隔开。如果给某一个探针指定了多个事件，只要其中一个事件发生，SystemTap就会执行对应的处理程序。每个探针有自己对应的语句块。语句块由花括号 {} 括住，包含事件发生时需要执行的所有语句。SystemTap会顺序执行这些语句；语句间通常不需要特殊的分隔符或终止符。 SystemTap还允许你编写函数来提取探针间公共的逻辑。所以，与其在多个探针间复制粘贴重复的语句，你不如把它们放入函数中，就像函数调用一样： 123function function_name(arguments) {statements}probe event {function_name(arguments)} 当探针被触发时，function_name中的语句会被执行。arguments是传递给函数的可选的入参。 SystemTap 事件我们还需要了解SystemTap 事件，这里主要分为 同步事件 和 异步事件。 同步事件同步事件会在任意进程执行到内核特定位置时触发。你可以用它来作为其它事件的参照点，毕竟同步事件有着清晰的上下文信息。包括： syscall.system_call 进入名为system_call的系统调用。如果想要监控的是退出某个系统调用的事件，在后面添加.return。举个例子，要想监控进入和退出系统调用close的事件，应该使用syscall.close和syscall.close.return。 vfs.file_operation 进入虚拟文件系统（VFS）名为file_operation的文件操作。跟系统调用事件一样，在后面添加.return可以监控对应的退出事件。 译注：file_operation取值的范畴，取决于当前内核中struct file_operations的定义的操作. kernel.function(&quot;function&quot;) 进入名为function的内核函数。举个例子，kernel.function(“sys_open”)即内核函数sys_open被调用时所触发的事件。同样，kernel.function(“sys_open”).return会在sys_open函数调用返回时被触发。 在定义探测事件时，可以使用像*这样的通配符。你也可以用内核源码文件名限定要跟踪的函数。看下面的例子： 12probe kernel.function(&quot;*@net/socket.c&quot;) { }probe kernel.function(&quot;*@net/socket.c&quot;).return { } kernel.trace(&quot;tracepoint&quot;)到达名为tracepoint的静态内核探测点（tracepoint）。较新的内核（&gt;= 2.6.30）包含了特定事件的检测代码。这些事件一般会被标记成静态内核探测点。一个例子是，kernel.trace(“kfree_skb”)表示内核释放了一个网络缓冲区的事件。（译注：想知道当前内核设置了哪些静态内核探测点吗？你需要运行sudo perf list。） module(&quot;module&quot;).function(&quot;function&quot;) 进入指定模块module的函数function。举个例子： 12probe module(&quot;ext3&quot;).function(&quot;*&quot;) { }probe module(&quot;ext3&quot;).function(&quot;*&quot;).return { } 异步事件异步事件跟特定的指令或代码的位置无关。 这部分事件主要包含计数器、定时器和其它类似的东西。 begin SystemTap会话的启动事件，会在脚本开始时触发。 end SystemTap会话的结束事件，会在脚本结束时触发。 timer events 用于周期性执行某段处理程序。举个例子： 1probe timer.s(4) { printf(&quot;hello world\\n&quot;) } 上面的例子中，每隔4秒就会输出hello world。还可以使用其它规格的定时器： 12345timer.ms(milliseconds)timer.us(microseconds)timer.ns(nanoseconds)timer.hz(hertz)timer.jiffies(jiffies) 定时事件总是跟其它事件搭配使用。其它事件负责收集信息，而定时事件定期输出当前状况，让你看到数据随时间的变化情况。 SystemTap 处理程序有了事件之后，我们还需要在事件发生之后进行处理。 example1: 12345probe begin{ printf (&quot;hello world\\n&quot;) exit ()} SystemTap脚本会一直运行，直到执行了exit()函数。如果你想中途退出一个脚本，可以用Ctrl+c中断。这是一个异步事件 begin 之后开始打印 一个字符串. printf 是一个标准输出函数 1printf (&quot;format string\\n&quot;, arguments) example2: 123456probe syscall.open{ printf (&quot;%s(%d) open\\n&quot;, execname(), pid())}echo ' probe syscall.open { printf (&quot;%s(%d) open\\n&quot;, execname(), pid()) }' | sudo stap -v - example2中 SystemTap会在每次open被调用时，输出调用程序的名字和PID。 该探针输出的结果看上去会是这样(not in zsh, in bash)： 123456789101112ubuntu@ubuntu-Inspiron-5548:~/workspace$ echo ' probe syscall.open { printf (&quot;%s(%d) open\\n&quot;, execname(), pid()) }' | sudo stap -v -Pass 1: parsed user script and 476 library scripts using 108308virt/90696res/7172shr/83388data kb, in 290usr/40sys/364real ms.Pass 2: analyzed script: 4 probes, 5 functions, 97 embeds, 4 globals using 110292virt/93060res/7724shr/85372data kb, in 230usr/330sys/575real ms.Pass 3: using cached /root/.systemtap/cache/4c/stap_4cd2c557b5457e5f955c640275432033_64778.cPass 4: using cached /root/.systemtap/cache/4c/stap_4cd2c557b5457e5f955c640275432033_64778.koPass 5: starting run.rg(25671) openrg(25671) openrg(25671) openrg(25671) openrg(25671) open 下面是常用的 SystemTap 内建函数： tid() 当前的tid（thread id）。 uid() 当前的uid。 cpu() 当前的CPU号 gettimeofday_s() 自epoch以来的秒数 ctime() 将上一个函数返回的秒数转化成时间字符串 pp() 返回描述当前处理的探测点的字符串 thread_indent() name 返回系统调用的名字。这个变量只能在syscall.system_call触发的处理程序中使用。 target() 当你通过stap script -x PID或stap script -c command来执行某个脚本script时，target()会返回你指定的PID或命令名。举个例子： 123456probe syscall.* { if (pid() == target()) printf(&quot;%s\\n&quot;, name)}echo ' probe syscall.* { if (pid() == target()) printf(&quot;%s\\n&quot;, name) }' | sudo stap -v - 这个 SystemTap 脚本使用了通配符 probe了所以系统调用，在对脚本解析，编译成为 kernel module ko的时候尤其耗时，在我 I5 5200U的机器上居然准备工作做了进1min… 12345678910111213141516top - 12:38:07 up 12:53, 2 users, load average: 2.56, 1.77, 1.14任务: 277 total, 5 running, 271 sleeping, 0 stopped, 1 zombie%Cpu(s): 10.0 us, 17.6 sy, 0.0 ni, 66.9 id, 0.0 wa, 0.0 hi, 5.5 si, 0.0 stMiB Mem : 3658.2 total, 196.4 free, 2043.1 used, 1418.8 buff/cacheMiB Swap: 2048.0 total, 1572.1 free, 475.9 used. 986.8 avail Mem 进程号 USER PR NI VIRT RES SHR %CPU %MEM TIME+ COMMAND 54775 root 20 0 171592 154320 18248 R 99.7 4.1 1:05.72 stap 932 ubuntu 20 0 1750472 40364 14900 S 3.7 1.1 11:16.15 Xorg 1279 ubuntu 20 0 4777288 218332 57988 S 3.0 5.8 12:57.76 gnome-shell 2458 ubuntu 20 0 4633228 103048 65556 R 3.0 2.8 0:10.23 chrome 1747 ubuntu 20 0 987308 43280 31604 S 1.7 1.2 0:46.33 gnome-terminal- 2371 ubuntu 20 0 1342876 232676 99800 S 0.7 6.2 8:23.72 chrome 18 root 20 0 0 0 0 S 0.3 0.0 0:01.14 ksoftirqd/1 30 root 20 0 0 0 0 S 0.3 0.0 0:00.87 ksoftirqd/3Inspiron-5548@ubuntu: ~/workspace/linux-stable/drivers# 输出是 1234567891011121314151617181920212223242526272829303132333435363738394041ubuntu@ubuntu-Inspiron-5548:~/workspace$ echo ' probe syscall.* { if (pid() == target()) printf(&quot;%s\\n&quot;, name) }' | sudo stap -v -x 2371 -Pass 1: parsed user script and 476 library scripts using 108312virt/90872res/7344shr/83392data kb, in 510usr/40sys/559real ms.qWARNING: cross-file global variable reference to identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/syscalls_cfg_trunc.stp:3:8 from: identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/sysc_add_key.stp:19:59 source: user_buffer_quoted(payload_uaddr, plen, syscall_string_trunc), ^ in expansion of macro: operator '@_SYSCALL_ADD_KEY_ARGSTR' at /usr/share/systemtap/tapset/linux/sysc_add_key.stp:72:2 source: @_SYSCALL_ADD_KEY_ARGSTR ^WARNING: cross-file global variable reference to identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/syscalls_cfg_trunc.stp:3:8 from: identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/sysc_mount.stp:31:46 source: data = user_string_n_quoted(pointer_arg(5), syscall_string_trunc) ^WARNING: cross-file global variable reference to identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/syscalls_cfg_trunc.stp:3:8 from: identifier 'syscall_string_trunc' at :23:49 source: buf_str = user_buffer_quoted(buf_uaddr, count, syscall_string_trunc) ^ in expansion of macro: operator '@_SYSCALL_WRITE_REGARGS' at /usr/share/systemtap/tapset/linux/sysc_write.stp:100:2 source: @_SYSCALL_WRITE_REGARGS ^Pass 2: analyzed script: 853 probes, 29 functions, 100 embeds, 5 globals using 129252virt/113636res/9172shr/104332data kb, in 35560usr/79740sys/116322real ms.Pass 3: using cached /root/.systemtap/cache/e2/stap_e2b35608bd8c0499c68f451dc8b09a85_432536.cPass 4: using cached /root/.systemtap/cache/e2/stap_e2b35608bd8c0499c68f451dc8b09a85_432536.koPass 5: starting run.recvmsgwritewriteepoll_waitepoll_waitepoll_waitrecvmsgreadsendtofutexrecvmsgrecvmsgrecvmsgpollrecvmsgrecvmsg SystemTap 处理程序的基本结构SystemTap 在处理程序中 它们的语法基本上类似于C或awk。 变量处理程序里面当然可以使用变量，你所需的不过是给它取个好名字，把函数或表达式的值赋给它，然后就可以使用它了。SystemTap可以自动判定变量的类型。举个例子，如果你用gettimeofday_s()给变量foo赋值，那么foo就是数值类型的，可以在printf()中通过%d输出。 变量默认只能在其所定义的探针内可用。这意味着变量的生命周期仅仅是处理程序的某次运行。不过你也可以在探针外定义变量，并使用global修饰它们，这样就能在探针间共享变量了。 ⁠ example1: 12345678910global count_jiffies, count_msprobe timer.jiffies(100) { count_jiffies ++ }probe timer.ms(100) { count_ms ++ }probe timer.ms(12345){ hz=(1000*count_jiffies) / count_ms printf (&quot;jiffies:ms ratio %d:%d =&gt; CONFIG_HZ=%d\\n&quot;, count_jiffies, count_ms, hz) exit ()} example1 中 timer-jiffies.stp 通过累加jiffies和milliseconds，来求出内核的CONFIG_HZ配置。global语句使得count_jiffies和count_ms在每个探针中可用。 目标变量(Target Variables)跟内核代码相关的事件，如kernel.function(“function”)和kernel.statement(“statement”)，允许使用目标变量获取这部分代码中可访问到的变量的值。你可以使用-L选项来列出特定探测点下可用的目标变量。如果已经安装了内核调试信息，你可以通过这个命令获取vfs_read中可用的目标变量 由于 我当前笔记本环境问题，暂时没法使用这个，会报错，后续补充 123456789101112131415ubuntu@ubuntu-Inspiron-5548:/etc/apt$ sudo stap -L 'kernel.function(&quot;vfs_read&quot;)'Tip: /usr/share/doc/systemtap/README.Debian should help you get started.ubuntu@ubuntu-Inspiron-5548:/etc/apt$ sudo stap -e 'probe kernel.function(&quot;vfs_read&quot;) {&gt; printf (&quot;current files_stat max_files: %d\\n&quot;,&gt; @var(&quot;[email protected]/file_table.c&quot;)-&gt;max_files);&gt; exit(); }'semantic error: while resolving probe point: identifier 'kernel' at &lt;input&gt;:1:7 source: probe kernel.function(&quot;vfs_read&quot;) { ^semantic error: missing x86_64 kernel/module debuginfo [man warning::debuginfo] under '/lib/modules/5.4.0-58-generic/build'Pass 2: analysis failed. [man error::pass2]Tip: /usr/share/doc/systemtap/README.Debian should help you get started.ubuntu@ubuntu-Inspiron-5548:/etc/apt$ 123456789Inspiron-5548@ubuntu: ~/workspace/linux-stable/drivers# stap -l 'syscall.*'syscall.acceptsyscall.accept4syscall.accessosyscall.acctsyscall.add_keysyscall.adjtimexsyscall.alarmsyscall.arch_prctl 整齐打印目标变量（Pretty Printing Target Variables）某些场景中，我们可能需要输出当前可访问的各种变量，以便于记录底层的变化。SystemTap提供了一些操作，可以生成描述特定目标变量的字符串： $$vars 输出作用域内每个变量的值。等价于sprintf(“parm1=%x … parmN=%x var1=%x … varN=%x”, parm1, …, parmN, var1, …, varN)。如果变量的值在运行时找不到，输出=?。 $$locals 同$$vars，只输出本地变量。 $$parms 同$$vars，只输出函数入参。 $$return 仅在带return的探针中可用。如果被监控的函数有返回值，它等价于sprintf(“return=%x”, $return)，否则为空字符串。 条件语句有些时候，你写的SystemTap脚本较为复杂，可能需要用上条件语句。SystemTap支持C风格的条件语句，另外还支持foreach (VAR in ARRAY) {}形式的遍历。 命令行参数通过$或@加个数字的形式可以访问对应位置的命令行参数。用$会把用户输入当作整数，用@会把用户输入当作字符串。 probe kernel.function(@1) { }probe kernel.function(@1).return { }上面的脚本期望用户把要监控的函数作为命令行参数传递进来。你可以让脚本接受多个命令行参数，分别命名为@1，@2等等，按用户输入的次序逐个对应。 Tapsetstapsets是一些包含常用的探针和函数的内置脚本，你可以在SystemTap脚本中复用它们。 当用户运行一个SystemTap脚本时，SystemTap会检测脚本中的事件和处理程序，并在翻译脚本成C代码之前，加载用到的tapset。就像SystemTap脚本一样，tapset的拓展名也是.stp。默认情况下tapset位于/usr/share/systemtap/tapset/。 跟SystemTap脚本不同的是，tapset不能被直接运行；它只能作为库使用。tapset库让用户能够在更高的抽象层次上定义事件和函数。tapset提供了一些常用的内核函数的别名，这样用户就不需要记住完整的内核函数名了（尤其是有些函数名可能会因内核版本的不同而不同）。另外tapset也提供了常用的辅助函数，比如之前我们见过的thread_indent()。 总结SystemTap 使用过程中发现 他解析编译极其慢，很耗费 CPU资源，最好做到一次解析编译模块，可以到处部署，实际SystemTap 也已经支持了这样的做法。 参考SystemTap 内核文档 参考 RedHat systemTap 文档 参考 文档","link":"/2021/01/09/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/systemTap/How%20to%20Use%20system%20Tap/"},{"title":"定位slub内存泄露的几种方法","text":"总结一下最近最近遇到的内存泄露的一个bug，其实内核态的内存泄露还是比较难搞的，严重情况下基本只能重启设备解决，对于互联网公司来说大规模重启服务器来说代价有点大，有了一些工具我们还是比较容易观测到的，我用写了一个内核模块来模拟内核态的内存泄露，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/fs.h&gt;#include &lt;linux/version.h&gt;#include &lt;linux/kthread.h&gt;#include &lt;linux/delay.h&gt;#include &lt;linux/slab.h&gt;#define LIULANGRENAAA_CACHE_SIZE (5678)static struct kmem_cache *liulangrenaaa_cache;struct task_struct *task = NULL;static int memleak_task_test(void *arg){ int m = 1000 * 10; int n = 1000 * 20; int i = 0; gfp_t gfp = GFP_KERNEL; while(!kthread_should_stop()){ i++; kmem_cache_alloc(liulangrenaaa_cache, gfp); if (i &lt; m) { msleep_interruptible(50); } else if ((i &gt; m) &amp;&amp; (i &lt; n)) { msleep_interruptible(500); } else if (i &gt; n) { msleep_interruptible(5000); } if (i % 50 == 1) { printk(KERN_ERR &quot;i = %d\\n&quot;, i); } } return 0;}static int memleak_init(void){ liulangrenaaa_cache = kmem_cache_create(&quot;liulangrenaaa_cache&quot;, LIULANGRENAAA_CACHE_SIZE, LIULANGRENAAA_CACHE_SIZE, SLAB_PANIC, NULL); if (liulangrenaaa_cache == NULL) { return 1; } task = kthread_run(memleak_task_test, NULL, &quot;memleak_task_test&quot;); if (IS_ERR(task)) { printk(KERN_ERR &quot;Ecard: unable to memleak_task_test kernel thread: %ld\\n&quot;, PTR_ERR(task)); return PTR_ERR(task); } return 0;}static void memleak_exit(void){ if (NULL != task) { kthread_stop(task); } kmem_cache_destroy(liulangrenaaa_cache);}module_init(memleak_init);module_exit(memleak_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;liulangrenaaa&quot;); 通过下面命令之后，安装内核模块 12makesudo insmod memleak.ko 过一段时间之后，就会发现内核内存不足，其中 /proc/meminfo 的 SUnreclaim 较大，且存在持续增加现象，这说明slab内存存在泄漏现象，属于典型的slab内存泄露 123sh@ubuntu[root]:/sys/kernel/slab# cat /proc/meminfoSReclaimable: 256132 kBSUnreclaim: 897648 kB 123sh@ubuntu[root]:/sys/kernel/slab# cat /proc/meminfoSReclaimable: 256132 kBSUnreclaim: 997652 kB 定位到slab内存泄露之后还不够，还需要了解到底是哪种slab内存泄露，linux内核提供了相关工具 slabtop slabinfo，可以观察内核中slab使用情况 123456789101112131415161718192021222324252627sh@ubuntu[root]:/sys/kernel/slab# slabtop Active / Total Objects (% used) : 1158388 / 1252082 (92.5%) Active / Total Slabs (% used) : 27004 / 27004 (100.0%) Active / Total Caches (% used) : 103 / 147 (70.1%) Active / Total Size (% used) : 409641.83K / 438329.62K (93.5%) Minimum / Average / Maximum Object : 0.01K / 0.35K / 10.08K OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME 204498 172768 84% 0.19K 4869 42 38952K dentry134580 129007 95% 0.13K 2243 60 17944K kernfs_node_cache122061 112693 92% 1.07K 4209 29 134688K ext4_inode_cache 75348 61356 81% 0.10K 1932 39 7728K buffer_head 61952 61952 100% 0.50K 968 64 30976K kmalloc-512 61659 60140 97% 0.20K 1581 39 12648K vm_area_struct 54144 53470 98% 0.03K 423 128 1692K kmalloc-32 53184 51912 97% 0.06K 831 64 3324K anon_vma_chain 45888 42616 92% 0.25K 717 64 11472K filp 43232 35798 82% 0.57K 772 56 24704K radix_tree_node 38743 35893 92% 0.59K 731 53 23392K inode_cache 34960 34556 98% 0.09K 760 46 3040K anon_vma 31872 26373 82% 0.06K 498 64 1992K kmalloc-64 21248 21248 100% 0.02K 83 256 332K kmalloc-16 19968 19968 100% 0.01K 39 512 156K kmalloc-8 16320 12472 76% 0.04K 160 102 640K ext4_extent_status 13200 12367 93% 0.66K 275 48 8800K proc_inode_cache 13018 12877 98% 0.69K 283 46 9056K squashfs_inode_cache 12070 12070 100% 0.05K 142 85 568K ftrace_event_field 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657sh@ubuntu[root]:/sys/kernel/slab# slabinfoName Objects Objsize Space Slabs/Part/Cpu O/S O %Fr %Ef Flg........dax_cache 42 752 32.7K 0/0/1 42 3 0 96 Aadentry 25537 192 6.1M 674/195/74 42 1 26 80 adma-kmalloc-512 64 512 32.7K 0/0/1 64 3 0 100 ddmaengine-unmap-128 30 1056 32.7K 0/0/1 30 3 0 96 Admaengine-unmap-256 15 2080 32.7K 0/0/1 15 3 0 95 Aext4_extent_status 12472 40 655.3K 151/61/9 102 0 38 76 aext4_groupinfo_4k 8008 144 1.1M 142/0/1 56 1 0 98 aext4_inode_cache 1131 1088 1.2M 32/0/7 29 3 0 96 aext4_system_zone 612 40 24.5K 2/0/4 102 0 0 99 fat_inode_cache 44 736 32.7K 0/0/1 44 3 0 98 afile_lock_cache 148 216 32.7K 0/0/4 37 1 0 97 files_cache 276 704 196.6K 2/0/4 46 3 0 98 Afilp 2213 256 720.8K 10/10/34 64 2 22 78 Afuse_inode 78 784 65.5K 0/0/2 39 3 0 93 Aafuse_request 112 144 16.3K 0/0/2 56 1 0 98 hugetlbfs_inode_cache 102 624 65.5K 0/0/2 51 3 0 97 inode_cache 22871 600 15.4M 442/47/30 53 3 9 88 ajbd2_journal_head 2244 112 303.1K 4/4/33 68 1 10 82 ajbd2_revoke_table_s 256 16 4.0K 0/0/1 256 0 0 100 akernfs_node_cache 129007 128 18.3M 2226/283/17 60 1 12 89 kmalloc-128 1536 128 196.6K 10/0/14 64 1 0 100 kmalloc-16 16128 16 258.0K 39/0/24 256 0 0 100 kmalloc-192 2184 192 425.9K 10/0/42 42 1 0 98 kmalloc-1k 2498 1024 2.7M 67/30/16 32 3 36 94 kmalloc-256 4224 256 1.0M 39/0/27 64 2 0 100 kmalloc-2k 2023 2048 4.2M 114/7/15 16 3 5 98 kmalloc-32 41694 32 1.3M 296/35/35 128 0 10 98 kmalloc-4k 1738 4096 7.4M 216/30/11 8 3 13 95 kmalloc-512 54016 512 27.6M 823/0/21 64 3 0 100 kmalloc-64 14637 64 1.2M 248/176/58 64 0 57 74 kmalloc-8 12800 8 102.4K 12/0/13 512 0 0 100 kmalloc-8k 398 8192 3.4M 97/6/7 4 3 5 95 kmalloc-96 3738 96 364.5K 35/0/54 42 0 0 98 kmalloc-rcl-128 192 128 24.5K 0/0/3 64 1 0 100 akmalloc-rcl-64 384 64 24.5K 0/0/6 64 0 0 100 akmalloc-rcl-96 168 96 16.3K 0/0/4 42 0 0 98 akmem_cache 2149 408 1.0M 43/15/19 36 2 24 86 Akmem_cache_node 2496 64 159.7K 9/0/30 64 0 0 100 Aliulangrenaaa_cache 26080 5678 228.5M 870/1/0 3 3 0 51 *mm_struct 120 1048 131.0K 0/0/4 30 3 0 95 Amqueue_inode_cache 34 920 32.7K 0/0/1 34 3 0 95 Anames_cache 72 4096 294.9K 0/0/9 8 3 0 100 Anet_namespace 18 4928 98.3K 0/0/3 6 3 0 90 numa_policy 186 264 49.1K 1/0/2 62 2 0 99 pde_opener 408 40 16.3K 0/0/4 102 0 0 99 proc_dir_entry 1176 192 229.3K 20/0/8 42 1 0 98 proc_inode_cache 1789 672 1.3M 5/5/37 48 3 11 87 aradix_tree_node 6181 576 6.5M 177/149/22 56 3 74 54 aRAW 641 976 819.2K 13/5/12 32 3 20 76 ARAWv6 494 1176 622.5K 5/0/14 26 3 0 93 Arequest_queue 60 2104 131.0K 0/0/4 15 3 0 96 request_sock_TCP 212 296 65.5K 0/0/4 53 2 0 95 scsi_sense_cache 1536 96 196.6K 21/0/3 64 1 0 75 A........ 在 slabinfo 的输出中明显看到 liulangrenaaa 的slab使用有异常，size为 5678，使用的个数高达26080，总内存占用是 228.5M，看来我们是找到了内存泄露的真凶了。 找到 liulangrenaaa 这个slab内存泄露之后，如何从代码上找到内存泄露的源头呢。我的一般做法是 直接在代码中搜索 liulangrenaaa 看slab的创建，根据创建的名字再去搜索 alloc 和 free的代码，对于我抽象的这个例子来说直接就可以看出端倪，有时候在复杂的代码逻辑想找出分配但没有释放的点是比较困难的 根据 liulangrenaaa slab 的 size，直接使用 bpftrace 来跟踪 kmem_cache_alloc 和 kmem_cache_free 函数且参数 size和我们相符的点，将comm 和 pid打印出来，基本是一把就能确定是谁在泄露这个slab，需要依赖bpftrace ftrace 来跟踪所有的 kmem_cache_alloc 和 kmem_cache_free点，然后将trace保存下来一一去对比，这个方法类似于方法二，但是工作量很大，因为ftrace不能过滤参数 使用kmemleak工具检查内存泄露，但是这种方法需要内核支持，如果不支持可能需要重新编译内核，大服务器上工作量有点大，嵌入式系统中还好 使用slub_debug 这个特性，但是也需要内核支持，同方法4 使用bpftrace来过滤首先要知道slab的分配和释放接口是 kmem_cache_alloc kmem_cache_free,然后去寻找 内核有没有提供 tracepoint点或者可以krpobe的点 1234567891011121314sh@ubuntu[root]:/sys/kernel/slab# bpftrace -l | grep trace |grep mem | grep alloctracepoint:kmem:kmalloctracepoint:kmem:kmem_cache_alloctracepoint:kmem:kmalloc_nodetracepoint:kmem:kmem_cache_alloc_nodetracepoint:kmem:mm_page_alloctracepoint:kmem:mm_page_alloc_zone_lockedtracepoint:kmem:mm_page_alloc_extfragsh@ubuntu[root]:/sys/kernel/slab# bpftrace -l | grep trace |grep mem | grep freetracepoint:kmem:kfreetracepoint:kmem:kmem_cache_freetracepoint:kmem:mm_page_freetracepoint:kmem:mm_page_free_batched 幸运的是发现内核居然都提供了这两个tracepoint点，然后需要去观察一下这两个tracepoint点可以给我们带来什么信息，直接到 /sys/kernel/debug/tracing/events/ 目录下看一下 1234567891011121314sh@ubuntu[root]:/sys/kernel/debug/tracing/events# cat kmem/kmem_cache_alloc/format name: kmem_cache_allocID: 539format: field:unsigned short common_type; offset:0; size:2; signed:0; field:unsigned char common_flags; offset:2; size:1; signed:0; field:unsigned char common_preempt_count; offset:3; size:1; signed:0; field:int common_pid; offset:4; size:4; signed:1; field:unsigned long call_site; offset:8; size:8; signed:0; field:const void * ptr; offset:16; size:8; signed:0; field:size_t bytes_req; offset:24; size:8; signed:0; field:size_t bytes_alloc; offset:32; size:8; signed:0; field:gfp_t gfp_flags; offset:40; size:4; signed:0; 可以看到 bytes_req 就是申请的slab大小，下面我们使用bpftrace来跟踪这个tp点，加上条件限制 12345678910111213141516171819sh@ubuntu[root]:/sys/kernel/slab# bpftrace -e 'tracepoint:kmem:kmem_cache_alloc / args-&gt;bytes_req == 5678 / { printf(&quot;pid:%d, comm:%s, kstack:[%s]\\n&quot;, pid, comm, kstack);}'Attaching 1 probe...pid:40953, comm:memleak_task_te, kstack:[ kmem_cache_alloc+340 kmem_cache_alloc+340 memleak_task_test+112 kthread+260 ret_from_fork+53]pid:40953, comm:memleak_task_te, kstack:[ kmem_cache_alloc+340 kmem_cache_alloc+340 memleak_task_test+112 kthread+260 ret_from_fork+53]sh@ubuntu[root]:/sys/kernel/debug/tracing/events# ps -aux |grep 40953root 40953 0.0 0.0 0 0 ? S 14:17 0:00 [memleak_task_te]root 40958 0.0 0.0 17668 736 pts/4 S+ 14:18 0:00 grep --color=auto 40953 根据内核线程名 memleak_task_te 就可以直接在代码中搜索，再结合 kstack 基本就可以直接找到bug 使用ftrace来跟踪其实和上面bpftrace 使用的原理是一样的，但是bpftrace 对内核版本要求较高，很多centos服务器的版本都比较老，甚至还在2.6.32左右，这样bpftrace基本不能用，所以ftrace一直很热门同样也需要找到分配释放slab的接口，也需要知道有没有提供相关tracepoint点，上面bpftrace方法已经说了，不赘述。 12345678910111213141516171819sh@ubuntu[root]:/sys/kernel/debug/tracing# echo 1 &gt; events/kmem/kmem_cache_alloc/enable sh@ubuntu[root]:/sys/kernel/debug/tracing# cat trace# tracer: nop## entries-in-buffer/entries-written: 348/348 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | | &lt;idle&gt;-0 [003] ..s. 312444.111637: kmem_cache_alloc: call_site=__build_skb+0x24/0x60 ptr=0000000003218205 bytes_req=224 bytes_alloc=256 gfp_flags=GFP_ATOMIC &lt;...&gt;-39771 [000] .... 312444.141944: kmem_cache_alloc: call_site=getname_flags+0x4f/0x1f0 ptr=00000000873301d3 bytes_req=4096 bytes_alloc=4096 gfp_flags=GFP_KERNEL &lt;...&gt;-39771 [000] .... 312444.141956: kmem_cache_alloc: call_site=__alloc_file+0x28/0x110 ptr=000000003ce0135a bytes_req=256 bytes_alloc=256 gfp_flags=GFP_KERNEL|__GFP_ZERO &lt;...&gt;-39771 [000] .... 312444.141957: kmem_cache_alloc: call_site=security_file_alloc+0x29/0x90 ptr=00000000142c82a3 bytes_req=24 bytes_alloc=24 gfp_flags=GFP_KERNEL|__GFP_ZERO &lt;...&gt;-39771 [000] .... 312444.342794: kmem_cache_alloc: call_site=getname_flags+0x4f/0x1f0 ptr=00000000873301d3 bytes_req=4096 bytes_alloc=4096 gfp_flags=GFP_KERNEL &lt;...&gt;-39771 [000] .... 312444.342800: kmem_cache_alloc: call_site=__alloc_file+0x28/0x110 ptr=000000003ce0135a bytes_req=256 bytes_alloc=256 gfp_flags=GFP_KERNEL|__GFP_ZERO 这样的信息对我们来说实在是太多了，我们知道trace保存这些信息的其实是一个 ringbuffer，短时间这么多的信息肯定会一遍一遍的填充这个 ringbufer，往往其中也没几条我们想要的信息，这就轮到filter上场了，可以根据规则想 filter 中写入相应规则，过滤我们想要trace的事件 12345678910111213141516171819202122232425262728293031323334353637383940414243sh@ubuntu[root]:/sys/kernel/debug/tracing/events/kmem/kmem_cache_alloc# lsenable filter format hist id triggersh@ubuntu[root]:/sys/kernel/debug/tracing# echo &quot;bytes_req == 5678&quot; &gt; events/kmem/kmem_cache_alloc/filter sh@ubuntu[root]:/sys/kernel/debug/tracing# cat events/kmem/kmem_cache_alloc/filter bytes_req == 5678sh@ubuntu[root]:/sys/kernel/debug/tracing# echo 0 &gt; tracesh@ubuntu[root]:/sys/kernel/debug/tracing# cat trace# tracer: nop## entries-in-buffer/entries-written: 0/0 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | |sh@ubuntu[root]:/sys/kernel/debug/tracing# echo 1 &gt; events/kmem/kmem_cache_alloc/enable sh@ubuntu[root]:/sys/kernel/debug/tracing# cat trace# tracer: nop## entries-in-buffer/entries-written: 55/55 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | | &lt;...&gt;-41112 [002] .... 313039.418842: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000844671fe bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.477171: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000157b6ef2 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.537008: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000a69f35fe bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.597138: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000b724443c bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.656971: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=000000003b39979a bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.716942: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=000000009a7e8ccf bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.777130: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=0000000088be3316 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.837584: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000b53a68b4 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.896236: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=000000004173b1a7 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNELsh@ubuntu[root]:/sys/kernel/debug/tracing# ps -aux | grep 41112root 41112 0.0 0.0 0 0 ? S 14:36 0:00 [memleak_task_te]root 41119 0.0 0.0 17668 724 pts/4 S+ 14:37 0:00 grep --color=auto 41112 ftrace的 event tracing 功能也可以实现类似和 bpftrace的功能，但是不能看到 kstack。看到泄露 slab的内核线程后面的工作也比较好做了，可以通过 proc接口查看stack，虽然这种方法看到的stack不是问题线程的stack，但是也有一些帮助 12345sh@ubuntu[root]:/proc/41112# cat stack [&lt;0&gt;] msleep_interruptible+0x30/0x60[&lt;0&gt;] memleak_task_test+0x82/0xb0 [memleak][&lt;0&gt;] kthread+0x104/0x140[&lt;0&gt;] ret_from_fork+0x35/0x40 相关 event trace的其他方法详见Event Trace Doc 使用memleak来定位如果发现内核中有 memleak 现象直接 cat /sys/kernel/debug/kmemleak这是内核每过一段时间救过自动扫描内核空间然后检查出来的memleak，当然也可以手动触发kmemleak检查 echo scan /sys/kernel/debug/kmemleak 1234567891011121314151617181920212223242526272829303132333435363738394041sh@ubuntu[root]:/sys/kernel/debug# cat kmemleak unreferenced object 0xffff8ea54df1a990 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117333 (age 199.916s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50unreferenced object 0xffff8ea54df1d310 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117349 (age 199.852s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50unreferenced object 0xffff8ea54df18010 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117364 (age 199.792s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50unreferenced object 0xffff8ea548de5310 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117379 (age 199.732s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50 这是我内核模块卸载之后打印的kstack，可以根据comm线程名字来找出 kmemleak 的内鬼真凶。 kmemleak的原理可以参考内核文档 kmemleak 实现原理 使用slub_debug来定位在打开 slub_debug的情况下，定位到 liulangrenaaa_cache slab 泄露之后，可以trace这个slab的分配行为。需要打开内核编译选项，只有slub有这个trace功能，slab slob没有 12+CONFIG_SLUB_DEBUG=y+CONFIG_SLUB_DEBUG_ON=y 重新编译内核之后，使能slub trace 1234sh@ubuntu[root]:~# echo 1 &gt; /sys/kernel/slab/liulangrenaaa_cache/trace sh@ubuntu[root]:~# sleep 1sh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/slab/liulangrenaaa_cache/trace sh@ubuntu[root]:~# dmesg | tail 观察kmesg输出，每次分配，释放都会被trace下来，最后destory也会，如果在没有完全释放所有分配的slub之前destory slub 就会产生一个bug警告 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[ 2109.158214] TRACE liulangrenaaa_cache alloc 0x00000000459b1c3f inuse=3 fp=0x0000000000000000[ 2109.158217] CPU: 0 PID: 4561 Comm: memleak_task_te Kdump: loaded Tainted: G OE 5.4.44 #2[ 2109.158217] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[ 2109.158218] Call Trace:[ 2109.158221] dump_stack+0x98/0xda[ 2109.158224] alloc_debug_processing.cold+0x53/0x72[ 2109.158225] ___slab_alloc+0x506/0x590[ 2109.158226] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158227] ? kmem_cache_alloc+0x23e/0x260[ 2109.158228] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158229] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158230] __slab_alloc+0x51/0x90[ 2109.158231] kmem_cache_alloc+0x23e/0x260[ 2109.158232] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158233] memleak_task_test+0x70/0xb0 [memleak][ 2109.158234] kthread+0x104/0x140[ 2109.158235] ? 0xffffffffc08ea000[ 2109.158236] ? kthread_park+0x90/0x90[ 2109.158237] ret_from_fork+0x3a/0x50[ 2109.238159] =============================================================================[ 2109.238162] BUG liulangrenaaa_cache (Tainted: G OE ): Objects remaining in liulangrenaaa_cache on __kmem_cache_shutdown()[ 2109.238171] -----------------------------------------------------------------------------[ 2109.238172] Disabling lock debugging due to kernel taint[ 2109.238173] INFO: Slab 0x000000001be073de objects=3 used=2 fp=0x00000000fd86908a flags=0xfffffc0010200[ 2109.238176] CPU: 1 PID: 4580 Comm: rmmod Kdump: loaded Tainted: G B OE 5.4.44 #2[ 2109.238176] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[ 2109.238177] Call Trace:[ 2109.238180] dump_stack+0x98/0xda[ 2109.238183] slab_err+0xb7/0xdc[ 2109.238184] __kmem_cache_shutdown.cold+0x1b/0x121[ 2109.238186] shutdown_cache+0x16/0x160[ 2109.238187] kmem_cache_destroy+0x21c/0x240[ 2109.238189] memleak_exit+0x26/0x28 [memleak][ 2109.238191] __x64_sys_delete_module+0x147/0x2b0[ 2109.238192] ? entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238193] ? trace_hardirqs_on+0x38/0xf0[ 2109.238195] do_syscall_64+0x5f/0x1a0[ 2109.238196] entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238197] RIP: 0033:0x7f78838e7a3b[ 2109.238199] Code: 73 01 c3 48 8b 0d 55 84 0c 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa b8 b0 00 00 00 0f 05 &lt;48&gt; 3d 01 f0 ff ff 73 01 c3 48 8b 0d 25 84 0c 00 f7 d8 64 89 01 48[ 2109.238199] RSP: 002b:00007ffda198cbf8 EFLAGS: 00000206 ORIG_RAX: 00000000000000b0[ 2109.238200] RAX: ffffffffffffffda RBX: 000055a871197790 RCX: 00007f78838e7a3b[ 2109.238201] RDX: 000000000000000a RSI: 0000000000000800 RDI: 000055a8711977f8[ 2109.238201] RBP: 00007ffda198cc58 R08: 0000000000000000 R09: 0000000000000000[ 2109.238201] R10: 00007f7883963ac0 R11: 0000000000000206 R12: 00007ffda198ce30[ 2109.238202] R13: 00007ffda198e752 R14: 000055a8711972a0 R15: 000055a871197790[ 2109.238204] INFO: Object 0x00000000459b1c3f @offset=16[ 2109.238205] INFO: Allocated in memleak_task_test+0x70/0xb0 [memleak] age=20 cpu=0 pid=4561[ 2109.238206] __slab_alloc+0x51/0x90[ 2109.238207] kmem_cache_alloc+0x23e/0x260[ 2109.238208] memleak_task_test+0x70/0xb0 [memleak][ 2109.238209] kthread+0x104/0x140[ 2109.238210] ret_from_fork+0x3a/0x50[ 2109.238211] INFO: Object 0x00000000f693e050 @offset=10640[ 2109.238212] INFO: Allocated in memleak_task_test+0x70/0xb0 [memleak] age=35 cpu=0 pid=4561[ 2109.238213] __slab_alloc+0x51/0x90[ 2109.238214] kmem_cache_alloc+0x23e/0x260[ 2109.238215] memleak_task_test+0x70/0xb0 [memleak][ 2109.238215] kthread+0x104/0x140[ 2109.238216] ret_from_fork+0x3a/0x50[ 2109.238219] kmem_cache_destroy liulangrenaaa_cache: Slab cache still has objects[ 2109.238221] CPU: 1 PID: 4580 Comm: rmmod Kdump: loaded Tainted: G B OE 5.4.44 #2[ 2109.238221] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[ 2109.238221] Call Trace:[ 2109.238222] dump_stack+0x98/0xda[ 2109.238223] kmem_cache_destroy.cold+0x15/0x1a[ 2109.238224] memleak_exit+0x26/0x28 [memleak][ 2109.238225] __x64_sys_delete_module+0x147/0x2b0[ 2109.238226] ? entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238227] ? trace_hardirqs_on+0x38/0xf0[ 2109.238228] do_syscall_64+0x5f/0x1a0[ 2109.238229] entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238229] RIP: 0033:0x7f78838e7a3b[ 2109.238230] Code: 73 01 c3 48 8b 0d 55 84 0c 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa b8 b0 00 00 00 0f 05 &lt;48&gt; 3d 01 f0 ff ff 73 01 c3 48 8b 0d 25 84 0c 00 f7 d8 64 89 01 48[ 2109.238230] RSP: 002b:00007ffda198cbf8 EFLAGS: 00000206 ORIG_RAX: 00000000000000b0[ 2109.238238] RAX: ffffffffffffffda RBX: 000055a871197790 RCX: 00007f78838e7a3b[ 2109.238239] RDX: 000000000000000a RSI: 0000000000000800 RDI: 000055a8711977f8[ 2109.238239] RBP: 00007ffda198cc58 R08: 0000000000000000 R09: 0000000000000000[ 2109.238240] R10: 00007f7883963ac0 R11: 0000000000000206 R12: 00007ffda198ce30[ 2109.238240] R13: 00007ffda198e752 R14: 000055a8711972a0 R15: 000055a871197790","link":"/2020/09/18/memory/%E9%9D%A2%E8%AF%95/%E5%86%99%E4%B8%80%E4%B8%AA%E6%B6%88%E8%80%97%E5%AE%8C%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%9A%84%E7%A8%8B%E5%BA%8F/%E5%AE%9A%E4%BD%8Dslub%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"cgroup","slug":"cgroup","link":"/tags/cgroup/"},{"name":"x86","slug":"x86","link":"/tags/x86/"},{"name":"通用寄存器","slug":"通用寄存器","link":"/tags/%E9%80%9A%E7%94%A8%E5%AF%84%E5%AD%98%E5%99%A8/"},{"name":"栈","slug":"栈","link":"/tags/%E6%A0%88/"},{"name":"函数调用","slug":"函数调用","link":"/tags/%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/"},{"name":"oops","slug":"oops","link":"/tags/oops/"},{"name":"panic","slug":"panic","link":"/tags/panic/"},{"name":"空指针","slug":"空指针","link":"/tags/%E7%A9%BA%E6%8C%87%E9%92%88/"},{"name":"文件系统","slug":"文件系统","link":"/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"name":"page_cache","slug":"page-cache","link":"/tags/page-cache/"},{"name":"qemu","slug":"qemu","link":"/tags/qemu/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"kvm","slug":"kvm","link":"/tags/kvm/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"内核同步","slug":"内核同步","link":"/tags/%E5%86%85%E6%A0%B8%E5%90%8C%E6%AD%A5/"},{"name":"锁粒度","slug":"锁粒度","link":"/tags/%E9%94%81%E7%B2%92%E5%BA%A6/"},{"name":"per-cpu","slug":"per-cpu","link":"/tags/per-cpu/"},{"name":"软中断","slug":"软中断","link":"/tags/%E8%BD%AF%E4%B8%AD%E6%96%AD/"},{"name":"tasklet","slug":"tasklet","link":"/tags/tasklet/"},{"name":"ksoftirqd","slug":"ksoftirqd","link":"/tags/ksoftirqd/"},{"name":"死锁检测","slug":"死锁检测","link":"/tags/%E6%AD%BB%E9%94%81%E6%A3%80%E6%B5%8B/"},{"name":"kdump","slug":"kdump","link":"/tags/kdump/"},{"name":"crash","slug":"crash","link":"/tags/crash/"},{"name":"makedumpfile","slug":"makedumpfile","link":"/tags/makedumpfile/"},{"name":"开发工具","slug":"开发工具","link":"/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"生活感悟","slug":"生活感悟","link":"/tags/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"},{"name":"杂项","slug":"杂项","link":"/tags/%E6%9D%82%E9%A1%B9/"},{"name":"read code","slug":"read-code","link":"/tags/read-code/"},{"name":"kernel patch","slug":"kernel-patch","link":"/tags/kernel-patch/"},{"name":"用户内存泄漏","slug":"用户内存泄漏","link":"/tags/%E7%94%A8%E6%88%B7%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"内存泄漏","slug":"内存泄漏","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"valgrind","slug":"valgrind","link":"/tags/valgrind/"},{"name":"内核内存越界","slug":"内核内存越界","link":"/tags/%E5%86%85%E6%A0%B8%E5%86%85%E5%AD%98%E8%B6%8A%E7%95%8C/"},{"name":"内存越界","slug":"内存越界","link":"/tags/%E5%86%85%E5%AD%98%E8%B6%8A%E7%95%8C/"},{"name":"KASAN","slug":"KASAN","link":"/tags/KASAN/"},{"name":"内核内存泄漏","slug":"内核内存泄漏","link":"/tags/%E5%86%85%E6%A0%B8%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"kmemleak","slug":"kmemleak","link":"/tags/kmemleak/"},{"name":"page_owner","slug":"page-owner","link":"/tags/page-owner/"},{"name":"slub_debug","slug":"slub-debug","link":"/tags/slub-debug/"},{"name":"deadlock","slug":"deadlock","link":"/tags/deadlock/"},{"name":"lockdep","slug":"lockdep","link":"/tags/lockdep/"},{"name":"资源泄漏","slug":"资源泄漏","link":"/tags/%E8%B5%84%E6%BA%90%E6%B3%84%E6%BC%8F/"},{"name":"文件描述符泄漏","slug":"文件描述符泄漏","link":"/tags/%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%B3%84%E6%BC%8F/"},{"name":"内存泄露","slug":"内存泄露","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/"},{"name":"虚拟地址空间泄漏","slug":"虚拟地址空间泄漏","link":"/tags/%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E6%B3%84%E6%BC%8F/"},{"name":"进程调度","slug":"进程调度","link":"/tags/%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/"},{"name":"hungtask","slug":"hungtask","link":"/tags/hungtask/"},{"name":"hardlockup","slug":"hardlockup","link":"/tags/hardlockup/"},{"name":"softlockup","slug":"softlockup","link":"/tags/softlockup/"},{"name":"preempt_count","slug":"preempt-count","link":"/tags/preempt-count/"},{"name":"内核抢占","slug":"内核抢占","link":"/tags/%E5%86%85%E6%A0%B8%E6%8A%A2%E5%8D%A0/"},{"name":"kthread","slug":"kthread","link":"/tags/kthread/"},{"name":"bpf","slug":"bpf","link":"/tags/bpf/"},{"name":"bpftrace","slug":"bpftrace","link":"/tags/bpftrace/"},{"name":"性能稳定性","slug":"性能稳定性","link":"/tags/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/"},{"name":"问题系统","slug":"问题系统","link":"/tags/%E9%97%AE%E9%A2%98%E7%B3%BB%E7%BB%9F/"},{"name":"irq","slug":"irq","link":"/tags/irq/"},{"name":"ftrace","slug":"ftrace","link":"/tags/ftrace/"},{"name":"vmtouch","slug":"vmtouch","link":"/tags/vmtouch/"},{"name":"pagecache","slug":"pagecache","link":"/tags/pagecache/"},{"name":"内管管理","slug":"内管管理","link":"/tags/%E5%86%85%E7%AE%A1%E7%AE%A1%E7%90%86/"},{"name":"OOM","slug":"OOM","link":"/tags/OOM/"},{"name":"systemTap","slug":"systemTap","link":"/tags/systemTap/"},{"name":"event trace","slug":"event-trace","link":"/tags/event-trace/"},{"name":"memleak","slug":"memleak","link":"/tags/memleak/"},{"name":"slub","slug":"slub","link":"/tags/slub/"}],"categories":[{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"linux内核","slug":"linux内核","link":"/categories/linux%E5%86%85%E6%A0%B8/"},{"name":"server","slug":"server","link":"/categories/server/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"shell脚本","slug":"shell脚本","link":"/categories/shell%E8%84%9A%E6%9C%AC/"},{"name":"生活感悟","slug":"生活感悟","link":"/categories/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"},{"name":"kernel debug","slug":"kernel-debug","link":"/categories/kernel-debug/"}]}